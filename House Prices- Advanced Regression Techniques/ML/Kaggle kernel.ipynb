{
  "cells": [
    {
      "metadata": {
        "_uuid": "e0a351d9be51b1c1d9839a0c13777431bdad4125",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport datetime\n\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\nfrom scipy import stats\nimport math\nimport random\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.metrics import  mean_squared_log_error\n\nfrom xgboost import XGBRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f319576eaf2c5298673868fbe31b5f1128d1232",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_File = '../input/train.csv'\ntest_File = '../input/test.csv'\n#train_File = 'train.csv'\n#test_File = 'test.csv'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51261ef31da4070dcc29efd4c83ede22dbdedc1c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "dd = display.display",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "64b6f65571a681c0fa2fdadf150ec80bbdf3b650"
      },
      "cell_type": "markdown",
      "source": "# 1. Gather"
    },
    {
      "metadata": {
        "_uuid": "2a6076c0664a55d4d6c673ead696040f69210177",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def loadData():\n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    \n    df = pd.concat([df_train, df_test], axis=0,sort=True,ignore_index=True)\n    \n    return df\n\ndf_before_clean = loadData()\ndd(df_before_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "745cc50e8d0ecfc797d4305fd168c2b5189860ca"
      },
      "cell_type": "markdown",
      "source": "# 2. Assess Data : Inspecting Data for Quality and Tidiness Issues\n#### 2.1 Quality Issues : Issues with content - missing, duplicate or incorrect data. a.k.a Dirty data \n* 2.1.a Completeness : *\"Are there any rows, columns or cells missing values?\"*\n  * 35 columns have the missing values: \n  \n  ['Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtUnfSF', 'Electrical', 'Exterior1st', 'Exterior2nd', 'Fence', 'FireplaceQu', 'Functional', 'GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageYrBlt', 'KitchenQual', 'LotFrontage', 'MSZoning', 'MasVnrArea', 'MasVnrType', 'MiscFeature', 'PoolQC', 'SalePrice', 'SaleType', 'TotalBsmtSF', 'Utilities']\n######   \n* 2.1.b Validity : *\"Does the data comply to the data schema like duplicate patient id or zip code being < 5 digits or float data type?\"*\n######   \n   * Following are Categorical Variables but currently are being considered as integer/float:\n\n        * MSSubClass\n        * OverallQual\n        * OverallCond\n        * FireplaceQu\n        * MoSold\n######   \n   * Following variables are supposed to be Integer type but Box-Cox or Scaling will anyway type cast them to float:\n\n      * LotFrontage, LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, YrSold, SalePrice\n   * GarageYrBlt will be NA when garage is not available for the house and hence this variable needs to be dropped as it does not comply to the schema.\n   \n######      \n  \n* 2.1.c Accuracy : *\"Wrong data that is valid. like hieght = 300 inches; it still complies to the standard i.e. inches but data is in accurate.\"*\n######   \n    * MSZoning has 4 missing entries. Also, we do not have data samples to Agricultre (A),  Industrial (I), Residential Low Density Park (RP). Therefore, there exist a probability that missing entries will be replaced with wrong data which are valid for the variables.\n######   \n    * Similarly, Utilites has 2 missing entries. Also, we do not have data samples for NoSewr ((Electricity, Gas, and Water (Septic Tank)) and ELO (Electricity only)\n######   \n    * Exterior1st, has no missing values in training data set. But has no samples for 'Other' and 'PreCast' Material. Testing data has one missing sample for the variable.\n######   \n    * Exterior2nd has no missing values in the training data set. But has no samples for \"PreCast\" material. Testing data has one missing sample for the variable.\n######   \n    * MasVnrType has 8 missing values in training data set. It has total of 5 valid values.But there are no samples for \"CBlock\" (Cylinder block). One in testing set also has a record with missing value for this variable.\n######   \n    * ExterQual has no samples for \"Po\" but fortunately there are no missing values for it in both training or testing data.\n######   \n    * BsmtQual has 37 missing entries. It has no samples for \"Po\" ((Poor (<70 inches)). Testing data set has 46 missing entries.\n######   \n    * BsmtCond has 37 missing entries. It has no samples for \"Ex\" (Excellent). There are 46 missing entries in the testing dataset.\n######   \n    * KitchenQual has no training samples on \"Po\" but testing sample has a missing entry.\n    * Functional has no training samples for \"Po\" but testing sample has a missing entry.\n    * PoolQC has no samples for \"Typ\" but testing sample has missing values.\n    * SaleType has no samples for \"VWD\" but testing sample has a missing value record.\n######   \n######   \n* 2.1.d Consistency : *\"Both valid and accurate but inconsistent. state = california and CA\"*\n######   \n  * BsmtExposure has training samples as NA for both No Basement and also for missing values. There are also 2 testing samples with missing values as NA.\n######   \n  * BsmtFinType2 has training samples as NA for both No Basement and also for missing values.\n######   \n  * TotalBsmtSF has both 0 and NA representing as missing basement.\n######   \n  * BsmtExposure has NA for both missing and no basement houses.\n  * BsmtFinType2 has NA for both missing and no basement for a house.\n  \n"
    },
    {
      "metadata": {
        "_uuid": "2c4dd8490b734cecc9b1183116622b54a31483ac"
      },
      "cell_type": "markdown",
      "source": "#### 2.2 Tidiness Issues: Issues with structure - untidy or messy data\n* 2.2.a Each observation is a row\n  * No Issues: Each observation is a unique house (no duplicate records)\n######   \n* 2.2.b Each variable is a column\n  * No Issues: There are no colummns with multi data or concatenated data.\n######  \n* 2.2.c Each observational unit is a table\n  * No Issues: There are no cross referring keys present in the table. Bsmt* and Garage* variables do form a logical group but there is no unique identities to the group.\n######   "
    },
    {
      "metadata": {
        "_uuid": "bfa5a3201b071e159d6f0ba1ce70ce9a54a1853e"
      },
      "cell_type": "markdown",
      "source": "##### Hypothesis 1: Bsmt__ variables are NA when TotalBsmtSF is 0\n##### Proof:\n* BsmtFinType1 is NA when TotalBsmtSF is 0\n* BsmtUnfSF is 0 whenever TotalBsmtSF is 0; Even in testing set it is NA only when TotalBsmtSF is NA\n* BsmtFullBath is 0 whenever TotalBsmtSF is 0; Even in testin set it is NA only when TotalBsmtSF is NA or 0.\n* BsmtHalfBath is 0 whenever TotalBsmtSF is 0; Even in testin set it is NA only when TotalBsmtSF is NA or 0."
    },
    {
      "metadata": {
        "_uuid": "dee7662b7b69a4d6adde71ccf2d08eb83e61f071"
      },
      "cell_type": "markdown",
      "source": "##### Key Observations:\n* Dataset has House Prices which were sold in between 2006 - 2010.\n* Surprised to see no bathroom and no bedroom but with kitchen Houses !!! Where do they sleep and shit after the heavy meal ?\n* NA value in GarageType can be easily mis interpreted as missing value. However, it is not true. NA in GarageType clearly indicates no garage because in both train and testing dataset GarageArea = 0 in all those cases. Similarly, GarageYrBlt, GarageFinish, GarageQual, GarageCond are also NA when GarageArea = 0. And GarageCars,GarageArea = 0 ==> GarageArea = 0.\n* How do we verify NA in Fence as missing entry or No Fence ??\n* How do we verify NA in MiscFeatures as None or missing value ?? Note that MiscVal is zero for NA, Othr & Shed."
    },
    {
      "metadata": {
        "_uuid": "31b5569b22ccc4a934b9c7a3de528de1be0a9b15"
      },
      "cell_type": "markdown",
      "source": "### 2.1.a Completeness : *\"Are there any rows, columns or cells missing values?\"*"
    },
    {
      "metadata": {
        "_uuid": "20d8fa80fa5b91ff02c5cf998045da909f6cbade",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def missingValueAssessment(df):\n    nan_columns = df.columns[df.isna().any()].tolist()\n    print('NaN columns :', nan_columns, \"\\n# :\", len(nan_columns))\n    \n    print(\"Duplicated rows count: \", df[df.duplicated()].shape)\n    df = df.fillna('NA')\n    print(\"Duplicated rows count: \", df[df.duplicated()].shape)\n    \nmissingValueAssessment(df_before_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c11b14cec3d14276b514b892bee289f9a3a7fe3"
      },
      "cell_type": "markdown",
      "source": "### 2.2.a Each observation is a row"
    },
    {
      "metadata": {
        "_uuid": "b148f64b18f8778d327cfb7efa5512eee1271676",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def checkHouseIsRepeated(df):\n    df_temp = df.groupby(['SalePrice','GrLivArea','YearBuilt','YearRemodAdd']).agg('count').reset_index()[['SalePrice','GrLivArea','YearBuilt','YearRemodAdd','Id']]\n    dd(\"Samples with same 'SalePrice','GrLivArea','YearBuilt','YearRemodAdd' : \",df_temp[df_temp.Id > 1])\n    \n    \ncheckHouseIsRepeated(df_before_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e86b5ac9a7d324ab778ceeb5a92640fc7102173"
      },
      "cell_type": "markdown",
      "source": "* As there are no time series data: as in there is no variable indicating the time of the reading carried out, It is safe to assume the reading was done at one shot and there would not be any duplicate entries of a house.\n* With the above assumption, group by 'SalePrice','GrLivArea','YearBuilt','YearRemodAdd' count indicates that there are no duplicate records."
    },
    {
      "metadata": {
        "_uuid": "efb54e7b72fe82b33cd27f532167bd87e77ed644",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_before_clean.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f93b84bebcd31734e295a6a8103cce6a90deba4"
      },
      "cell_type": "markdown",
      "source": "# 3.0 Cleaning Data"
    },
    {
      "metadata": {
        "_uuid": "3dfbfab28b468242244b5322d32700d5654abd9b"
      },
      "cell_type": "markdown",
      "source": "##### Let us first do the cleaning activities where we have high confidence of imputing the values as listed in the above Assessment summary."
    },
    {
      "metadata": {
        "_uuid": "80a7664ab7d06cfac9f7345740b5cb1b44288949",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def convertOrdinalToNumber(df):\n    \n    #Convert ordinal categorical values to numeric values\n    \n    \n    Ordinal_columns = ['BsmtCond','BsmtQual','ExterCond','ExterQual',\n                       'FireplaceQu','GarageCond','GarageQual','HeatingQC',\n                       'PoolQC','KitchenQual']\n    \n    #Thankfully panda converts the missing values to -1 here\n    for c in Ordinal_columns:\n        \n        df[c] = pd.Categorical(df[c], categories=['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'], \n                               ordered=True).codes\n        dd(c, df[c].unique())\n    \n    \n    df['BsmtExposure'] = pd.Categorical(df['BsmtExposure'],\n                                       categories=['NA', 'No', 'Mn', 'Av', 'Gd'],\n                                       ordered=True).codes\n    \n    df['GarageFinish'] = pd.Categorical(df['GarageFinish'],\n                                       categories=['NA', 'Unf', 'RFn', 'Fin'],\n                                       ordered=True).codes\n    \n    df['PavedDrive'] = pd.Categorical(df['PavedDrive'],\n                                       categories=['N', 'P', 'Y'],\n                                       ordered=True).codes\n    \n    df['Utilities'] = pd.Categorical(df['Utilities'],\n                                       categories=['ELO', 'NoSeWa', 'NoSewr', 'AllPub'],\n                                       ordered=True).codes\n    \n    df['LotShape'] = pd.Categorical(df['LotShape'],\n                                       categories=['Reg', 'IR1', 'IR2', 'IR3'],\n                                       ordered=True).codes\n    \n    df['LandSlope'] = pd.Categorical(df['LandSlope'],\n                                       categories=['Gtl', 'Mod', 'Sev'],\n                                       ordered=True).codes\n    \n    df['Functional'] = pd.Categorical(df['Functional'],\n                                       categories=['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod',\n                                                   'Min2', 'Min1', 'Typ'],\n                                       ordered=True).codes\n    \n    df['BsmtFinType1'] = pd.Categorical(df['BsmtFinType1'],\n                                       categories=['NA', 'Unf', 'LwQ', 'Rec', 'BLQ',\n                                                   'ALQ', 'GLQ'],\n                                       ordered=True).codes\n    \n    df['BsmtFinType2'] = pd.Categorical(df['BsmtFinType2'],\n                                       categories=['NA', 'Unf', 'LwQ', 'Rec', 'BLQ',\n                                                   'ALQ', 'GLQ'],\n                                       ordered=True).codes\n    \n    dd('BldgType',df['BldgType'].unique(),df[df['BldgType'].isna()]) #'1Fam', '2fmCon', 'Duplex', 'TwnhsE', 'Twnhs']\n    df['BldgType'] = pd.Categorical(df['BldgType'],\n                                       categories=['1Fam', '2fmCon', 'Duplex', 'Twnhs', 'TwnhsE'],\n                                       ordered=True).codes\n    dd('BldgType',df['BldgType'].unique())\n    \n    df['HouseStyle'] = pd.Categorical(df['HouseStyle'],\n                                       categories=['1Story', '1.5Unf', '1.5Fin', '2Story', '2.5Unf', '2.5Fin', 'SFoyer', 'SLvl'],\n                                       ordered=True).codes\n    \n    df['LandContour'] = pd.Categorical(df['LandContour'],\n                                       categories=['Low', 'Lvl', 'Bnk', 'HLS'],\n                                       ordered=True).codes\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb0cf59f1497315a4e353b9046fd567b3147dcda",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cleanStage1(df):\n    \n    #convert data type\n    #we are being little lineant to give float64 for YearBuilt, YrSold but those guys are going to be box-coxed \n    #so let them at least enjoy the bigger size for now\n    \n    #Before changing thier data type, let us first convert them to continuous variable wherever applicable. \n    #i.e. YearBuilt, YearRemodAdd, YrSold & MoSold\n    \n    df['AgeInMonths'] = df.YearBuilt.apply(lambda x: (2019 - x)*12)\n    \n    df['AgeWhenSold'] = (df.YrSold - df.YearBuilt) * 12 + df.MoSold\n    \n    df['AgeWhenRemod'] = (df.YearRemodAdd - df.YearBuilt) * 12\n    \n    df= df.drop(['YearBuilt', 'YrSold', 'MoSold', 'YearRemodAdd'],axis=1)\n    \n    float64_variables = ['LotFrontage', 'LotArea', 'AgeInMonths', 'AgeWhenRemod', 'MasVnrArea', \\\n                     'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \\\n                     'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\\\n                     'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n                     'PoolArea', 'MiscVal', 'AgeWhenSold', 'SalePrice', 'OverallQual', 'OverallCond']\n    \n    for c in float64_variables:\n        df[c] = df[c].astype(np.float64)\n    \n    int_to_categorical_variables = ['MSSubClass',]\n    for c in int_to_categorical_variables:\n        df[c] = df[c].astype(str)\n    \n    dd(len(df.columns[df.isna().any()].tolist()), df.columns[df.isna().any()].tolist())\n    df = convertOrdinalToNumber(df)\n    \n    dd(\"Number of na columns : \",len([c for c in list(df) if (-1 in df[c].unique()) or (df[df[c].isna()].shape[0] > 0)]))\n    dd(\"To be imputed columns : \", [c for c in list(df) if (-1 in df[c].unique()) or (df[df[c].isna()].shape[0] > 0)])\n    \n    dd(df.KitchenQual.unique())\n    dd(df.BsmtCond.unique())\n    #return\n    \n    #As per our data analysis TotalBsmtSF = 0 when TotalBsmtSF is missing.\n    df.loc[df.TotalBsmtSF.isna(), 'TotalBsmtSF'] = 0\n\n    #BsmtQual\tBsmtCond\tBsmtExposure\tBsmtFinType1\tBsmtFinType2\tBsmtFullBath\tBsmtHalfBath\n    #BsmtFinSF1\tBsmtFinSF2\tBsmtUnfSF\n    bsmt_cols = [c for c in list(df) if 'Bsmt' in c]\n    df.loc[df.TotalBsmtSF == 0, bsmt_cols] = 0\n\n    #verify almighty pandas half line solution to assignment\n    dd(df[df.TotalBsmtSF== 0][bsmt_cols])\n   \n    dd(df.GarageType.unique())\n\n    #GarageYrBlt\tGarageFinish GarageQual\tGarageCond GarageCars\tGarageArea\n    df['GarageType'] = df.GarageType.fillna(\"NA\")\n    gar_cols = [c for c in list(df) if ('Garage' in c) and ('GarageType' not in c)]\n    df.loc[df.GarageType == \"NA\", gar_cols] = 0\n    \n    dd(df[df.GarageType== \"NA\"][gar_cols+['GarageType']])\n\n    #drop obsolete columns\n    df = df.drop(['Id','GarageYrBlt'], axis=1)\n    df['SalePrice'] = df.SalePrice.fillna(0)\n    \n    \n    dd(df.KitchenQual.unique())\n    dd(df.BsmtCond.unique())\n\n    return df\n\ndf_stage_1 = cleanStage1(df_before_clean.copy())\ndf_stage_1.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ca9707b06ea25db7234ed75abda018ff57e93ef",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_stage_1[df_stage_1.BsmtCond == -1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96a98002b9eeaefa16ecbf1221d944ff7b747691"
      },
      "cell_type": "code",
      "source": "df_stage_1[df_stage_1.GarageArea.isna()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9b61767cb21897686a70ec1b189f52d84b60d26"
      },
      "cell_type": "markdown",
      "source": "##### We had map when TotalBsmtSF =0 , there are couple of entries when it is TotalBsmtSF != 0. We may have to other Bsmt- attributes to impute the values for it but we will do such analysis after the straight forward missing value imputation.\n\n##### Similarly, Garage variables are not completely imputed yet."
    },
    {
      "metadata": {
        "_uuid": "f8610227232a76f9017ad2b20e4a37c4ee30208b"
      },
      "cell_type": "markdown",
      "source": "##### We have now 25 columns to look after for the first round of cleaning."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "181bd61d63321bb99ee5141376ed234263354e04"
      },
      "cell_type": "code",
      "source": "dd(\"Total columns in the input dataset : \",df_stage_1.shape[1])\ndd(\"Number of na columns : \",len([c for c in list(df_stage_1) if (-1 in df_stage_1[c].unique()) or (df_stage_1[df_stage_1[c].isna()].shape[0] > 0)]))\ndd(\"To be imputed columns : \", [c for c in list(df_stage_1) if (-1 in df_stage_1[c].unique()) or (df_stage_1[df_stage_1[c].isna()].shape[0] > 0)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c5fd330508d85541a92b467b32faec725088938"
      },
      "cell_type": "markdown",
      "source": "##### Before Imputing let us find if there are further more ordinal variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a3cf97c455e183f806ef277a1f553118e17b4c5"
      },
      "cell_type": "code",
      "source": "_= [dd(c,df_stage_1[c].unique()) for c in df_stage_1.select_dtypes(include=np.object) ]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22150f580337986230410fba8da72fd6f64b13ff"
      },
      "cell_type": "code",
      "source": "def checkOrdinal(df):\n    df['TotalSqFt'] = df['GrLivArea'] + df['TotalBsmtSF']\n    dd(df.groupby('BldgType').agg('median').reset_index().sort_values('TotRmsAbvGrd')[['BldgType', 'TotalSqFt','TotRmsAbvGrd','BedroomAbvGr']])\n    dd(df.groupby('BldgType').agg('max').reset_index().sort_values('TotRmsAbvGrd')[['BldgType', 'TotalSqFt','TotRmsAbvGrd','BedroomAbvGr']])\n    \n    \n    dd(df.groupby('HouseStyle').agg('median').reset_index().sort_values('TotalSqFt')[['HouseStyle', 'TotalSqFt','TotRmsAbvGrd','BedroomAbvGr']])\n    dd(df.groupby('HouseStyle').agg('max').reset_index().sort_values('TotalSqFt')[['HouseStyle', 'TotalSqFt','TotRmsAbvGrd','BedroomAbvGr']])\n    \ncheckOrdinal(df_stage_1.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0760f99305bdba361daac2d9684677550271490d"
      },
      "cell_type": "markdown",
      "source": "##### We know from the domain knowledge \"type of dwelling\" that 1Fam < 2FmCon < Duplx < Twnhs. Now, between the town houses, I am finalizing the sequence through Total sq ft. \n* final ordinal variable : 1Fam < 2FmCon < Duplx < Twnhs < TwnhsE"
    },
    {
      "metadata": {
        "_uuid": "7c86c1a4a90b8ee5d8ef07123e50ea78dd259b3f"
      },
      "cell_type": "markdown",
      "source": "##### Similarly, for HouseStyle we have ,\n* 1Story < 1.5Unf < 1.5Fin < 2Story < 2.5Unf < 2.5Fin < SFoyer < SLvl\n\n##### I had confusion between Unf and Fin but I relied on again sq. ft, rooms and bedrooms. I could not get enough material to defend Sfoyer and SLvl, so again relied on those 3 parameters."
    },
    {
      "metadata": {
        "_uuid": "f5781fb2d461f48985356add2ad873938e8fa070"
      },
      "cell_type": "markdown",
      "source": "##### With domain knowledge(common sense) For LandContour,\n* Low < Lvl < Bnk < HLS"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b573d46699d3f4374dce0e1a4b7ae552110492e"
      },
      "cell_type": "code",
      "source": "def checkOrdinal1(df):\n    dd(df.groupby('MSZoning').agg('sum').reset_index().sort_values('TotRmsAbvGrd')[['MSZoning','TotRmsAbvGrd']])\ncheckOrdinal1(df_stage_1.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e08eb5d195fd151a39f36429405a75fbe28d382a"
      },
      "cell_type": "markdown",
      "source": "##### Let us first see all the NaN values"
    },
    {
      "metadata": {
        "_uuid": "0bf8f785e6aae4e376b74aec040c78b70ef11b23",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plotNAs(df):\n    nan_columns = df.columns[df.isna().any()].tolist()\n    #nan_columns.remove('SalePrice')\n    for c in df.fillna('NotAvailable')[nan_columns]:\n        df[[c,'SalePrice']].fillna('NotAvailable').\\\n        groupby(by=c).agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+c)\n        plt.xticks(rotation=45)\n        plt.show()\nplotNAs(df_stage_1.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "195f2fc30462689c693559b6a30636ea03925b02"
      },
      "cell_type": "markdown",
      "source": "### 1. Alley"
    },
    {
      "metadata": {
        "_uuid": "2a9bfbf210d58ff43ab6f7f65639e4521c0af1a4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def outerLandScape(df_temp):\n    df = df_temp.copy()\n    print(\"Initial Shape : \", df.shape)\n    \n    beyond_house = ['Neighborhood','Street', 'PavedDrive', 'Alley']\n    df_temp = df.groupby(beyond_house).agg('count').reset_index()[['Neighborhood','Street', 'PavedDrive', 'Alley',\"SalePrice\"]]\n    dd()\n    \n    def getAlley(Street, Neighborhood, PavedDrive):\n        try:\n            alley = df_temp[\n                (df_temp['Street'] == Street ) &\n                (df_temp['Neighborhood'] == Neighborhood ) &\n                (df_temp['PavedDrive'] == PavedDrive) \n            ]['Alley'].tolist()[0]\n        except:\n            alley = 'NA'\n        \n        return alley\n        \n    \n    df['Alley'] = df.Alley.fillna(\"NA\")\n    df[['Alley','SalePrice']].fillna(0).\\\n        groupby(by='Alley').agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+'Alley')\n    plt.xticks(rotation=45)\n    plt.show()\n    \n    na_alley_count = df[df.Alley == \"NA\"].shape[0]\n    gr_alley_count = df[df.Alley == \"Grvl\"].shape[0]\n    pa_alley_count = df[df.Alley == \"Pave\"].shape[0]\n    \n        \n    df['Alley'] = df.apply( lambda x: getAlley (x['Street']\n                                                ,x['Neighborhood']\n                                                ,x['PavedDrive']\n                                               ) if x['Alley'] == \"NA\" else x['Alley']\n        ,axis=1)\n    \n    \n    df[['Alley','SalePrice']].fillna(0).\\\n        groupby(by='Alley').agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+'Alley')\n    plt.xticks(rotation=45)\n    plt.show()\n    \n    \n    print(\"Alley Snapshot Before : NA -\", na_alley_count,\"Grvl - \", gr_alley_count, \"Pave - \", pa_alley_count)\n    print(\"Alley Snapshot After  : NA -\", df[df.Alley == \"NA\"].shape[0],\n          \"Grvl - \", df[df.Alley == \"Grvl\"].shape[0], \"Pave - \", df[df.Alley == \"Pave\"].shape[0]\n         )\n    \n    return df\n\ndf_alley = outerLandScape(df_stage_1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d8e683871512fd91ba2a11afeda76bcb365ab95f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def outerLandScape2():\n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    \n    df = pd.concat([df_train, df_test], axis=0,sort=True,ignore_index=True)\n    print(\"Initial Shape : \", df.shape)\n    \n    '''beyond_house = [ 'MSZoning','Street', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood',\n                      'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'PavedDrive','SaleCondition','Fence', 'Alley']\n    df_temp = df[beyond_house].copy()\n    for c in beyond_house:\n        df_temp = df_temp[df_temp[c].notna()]#.reset_index()\n        print(c, df_temp.shape)'''\n    #display.display(df_temp)\n    \n    #beyond_house = ['Neighborhood','Street', 'PavedDrive', 'Alley']\n    #df = df.fillna('NNN')\n    beyond_house = ['Neighborhood','Street','Alley']\n    #beyond_house = ['Street', 'Alley']\n    \n    #display.display(df.groupby(beyond_house).agg('count').reset_index()[['Neighborhood','Street', 'PavedDrive', 'Alley','Id']])\n    display.display(df.groupby(beyond_house).agg('count').reset_index()[['Neighborhood','Street', 'Alley','Id']])\n    \n    \nouterLandScape2()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "413785dd0f436284aa24422d59302764ab1f8392"
      },
      "cell_type": "markdown",
      "source": "##### We can have the above table mapping missing Alley variable values. I am here assuming that Street and Alley would intersect. Therefore, there is a pattern with respect to location regarding the type of material used.\n\n* However, the variable itself might not have such significance to target variable and hence can we can drop the imputation to it. The model which uses this variable would give lower significance level during training."
    },
    {
      "metadata": {
        "_uuid": "fa0e8728b8a135cd1881c8ff3f02b0cd7b556381"
      },
      "cell_type": "markdown",
      "source": "### 2. Electrical"
    },
    {
      "metadata": {
        "_uuid": "ac4f02ceb71fbedd5049583dddcbed4deb920a55",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def electricalWrangling(df_temp):\n    df = df_temp.copy()\n    df1 = df_temp.copy()\n    print(df[df.Electrical.isna()].shape)\n    df['Electrical'] = df.Electrical.fillna('NA')\n    display.display(df[['Neighborhood','Electrical', 'SalePrice']].groupby(['Neighborhood','Electrical']).agg('count'))\n    \n    df1['Electrical'] = df1.Electrical.fillna('SBrkr')\n    display.display(df1[['Neighborhood','Electrical', 'SalePrice']].groupby(['Neighborhood','Electrical']).agg('count'))\n    return df1\n    \ndf_electrical = electricalWrangling(df_stage_1.drop('Alley',axis=1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dc2535506898ebb499932dc3cda370331decd835"
      },
      "cell_type": "markdown",
      "source": "##### At Timber, most of them have SBrkr as electrical system. So, it is a safe bet to have the missing entry replaced with 'SBrkr'"
    },
    {
      "metadata": {
        "_uuid": "0b279b86709d190a8f71211c6c8803e0fdbdb722"
      },
      "cell_type": "markdown",
      "source": "### 3. Exterior1st & Exterior2nd"
    },
    {
      "metadata": {
        "_uuid": "6f3415f7df07a92d7cadd3b06e3eff9a624fab63",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def exteriorWrangle(df_temp):\n    df = df_temp.copy()\n    dd(df.shape)\n    dd(df[df.Exterior1st == df.Exterior2nd].shape)\n    dd(df[df.Exterior1st.isna()][['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond', 'MSSubClass']])\n    dd(df[df.Exterior2nd.isna()][['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond','MSSubClass']])\n    dt = df.\\\n       groupby(['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond', 'MSSubClass']).\\\n       agg('count').reset_index().\\\n       sort_values(by=['MSSubClass','SalePrice'],ascending=False)[['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond','MSSubClass','SalePrice']]\n    #dd(dt)\n    dd(dt[(dt.MSSubClass == '30') & (dt.Neighborhood == 'Edwards')].head())\n    \n    def bestExt1(Neighborhood,ExterQual, ExterCond, MSSubClass):\n        return dt[(dt.Neighborhood == Neighborhood)&\n                  (dt.ExterQual == ExterQual)&\n                  (dt.ExterCond ==  ExterCond)&\n                  (dt.MSSubClass == MSSubClass)]['Exterior1st'].tolist()[0]\n    \n    def bestExt2(Neighborhood,ExterQual, ExterCond, MSSubClass):\n        return dt[(dt.Neighborhood == Neighborhood)&\n                  (dt.ExterQual == ExterQual)&\n                  (dt.ExterCond ==  ExterCond)&\n                  (dt.MSSubClass == MSSubClass)]['Exterior2nd'].tolist()[0]\n        \n    df['Exterior1st'] = df.Exterior1st.fillna('NA')\n    df['Exterior2nd'] = df.Exterior2nd.fillna('NA')\n    \n    df['Exterior1st'] = df.apply(lambda x: bestExt1(\n                                    x['Neighborhood'], x['ExterQual'], x['ExterCond'], x['MSSubClass']\n                                    ) if x['Exterior1st'] ==\"NA\" else x['Exterior1st'], axis=1)\n    df['Exterior2nd'] = df.apply(lambda x: bestExt2(\n                                    x['Neighborhood'], x['ExterQual'], x['ExterCond'], x['MSSubClass']\n                                    ) if x['Exterior2nd'] ==\"NA\" else x['Exterior2nd'], axis=1)\n    \n    dd(df[df.index == 2151][['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond', 'MSSubClass']])\n    return df\n    \n\n    \ndf_ext = exteriorWrangle(df_electrical)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "430d3c5c1ba416f2ff4f3abcf13025b91574f5cc"
      },
      "cell_type": "markdown",
      "source": "##### it is observed that many of the houses have Exterior1st and Exterior2nd same values per neighborhood. Therefore, we will create a matrix of neighborhood and Exterior1st. We will first populate Exterior1st from neighborhood value and then we will populate Exterior2nd from Exterior1st."
    },
    {
      "metadata": {
        "_uuid": "978c99b238daf55dcfb1dcc3d62435bf563fbbfa"
      },
      "cell_type": "markdown",
      "source": "### 4. Fence"
    },
    {
      "metadata": {
        "_uuid": "5a3b8ea3a1f5dfef6073bb6ec1aa0d3581f97cdb",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def fenceWrangling(df_temp):\n    df = df_temp.copy()\n\n    print(\"Count of missing Fence : \",df[df.Fence.isna()].shape)\n    \n    df['Fence'] = df.Fence.fillna('NA')\n    \n    #dd(df.groupby(['MSSubClass', 'Neighborhood','Fence', ]).agg('count').reset_index()[['MSSubClass', 'Neighborhood','Fence', 'SalePrice']])\n    dd(df.groupby(['Neighborhood','Fence', ]).agg('count').reset_index()[['Neighborhood','Fence', 'SalePrice']])\n    return df\n    \ndf_fence = fenceWrangling(df_ext)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "730cbc18cfd3208b2c6c0e2d5b287e61c1be4836"
      },
      "cell_type": "markdown",
      "source": "##### It is too tedious to decide if missing value indicates \"NA\" - No Fence or the entry was missing. What does actually Fence depend on ?\n* For now i m going to rely on NA for the missing entry. Safest assumption."
    },
    {
      "metadata": {
        "_uuid": "be573f4333b1a21672960e42d56b45061b3e12e7"
      },
      "cell_type": "markdown",
      "source": "##### It is too risky to map from othe rvariables. Though it seems like it depends on LotArea or LotFrontage. It is not very clear if it depends solely on one of the variable or sort of combination of others. Let us keep it NA for missing values, so that it would mean no fence available."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a241dda174d30e265fd8389a03b7486753c20c10"
      },
      "cell_type": "code",
      "source": "def plotMinusOnes(df):\n    minusOneCols = [c for c in list(df) if -1 in df[c].unique()]\n    \n    for c in df[minusOneCols]:\n        df[[c,'SalePrice']].\\\n        groupby(by=c).agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+c)\n        plt.xticks(rotation=45)\n        plt.show()\nplotMinusOnes(df_stage_1.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1bb19a2f274f3982fc6b46d4b3f8790f4deeb3f7"
      },
      "cell_type": "markdown",
      "source": "### 5.0 Functional"
    },
    {
      "metadata": {
        "_uuid": "61976ce4697cd3c7daa84bdd05cba25867ef3288",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def functionalWrangle(df):\n    print(\"Unique values in Functional variable Before: \", df.Functional.unique())\n    print(\"How many missing entries are there in Functional variable : \",df[df.Functional == -1].shape[0])\n    \n    co_qu_columns = [c for c in list(df) if (\"Co\" in c) or (\"Qu\" in c)]\n    co_qu_columns.append('SalePrice')\n    co_qu_columns.append('Functional')\n    print(\"Quality and Condition Related columns : \", co_qu_columns)\n    \n    dd(\"Samples with missing functional feature values : \" , df[df.Functional.isna()][co_qu_columns])\n    \n    dd(\"Let us see the samples which have similar Overall Condition and Quality\")\n    dd(df[(df.OverallCond == 5) & (df.OverallQual == 1)][co_qu_columns])\n    dd(df[(df.OverallCond == 1) & (df.OverallQual == 4)][co_qu_columns])\n    \n    df.loc[df.Functional == -1, \"Functional\"] = 7\n    \n    print(\"Unique values in Functional variable : \", df.Functional.unique())\n    \n    return df\n    \ndf_funct = functionalWrangle(df_fence.copy())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd3e5826dfeb7f45dab9c5c912e41f5995d202ef"
      },
      "cell_type": "markdown",
      "source": "##### the rule itself says, assume typical unless deductions are warranted. However, there is no entry of salvage in our data set. Though it is not mandatory to have all categorical values has to be there in the dataset but it always raises the question why not that variable ? Can *Cond and *Qu variable give us hint of not 'Sal' ?\n\n* I am actually tempted to put 'Sal' but due to lack of samples for Sal, I will be putting it as 'Typ'"
    },
    {
      "metadata": {
        "_uuid": "9efcd37b79e28599c6f8fb0f8ceb9df55f72b7fd"
      },
      "cell_type": "markdown",
      "source": "### 6. LotFrontage"
    },
    {
      "metadata": {
        "_uuid": "c26b9467cc19772ed497c7829ee23cd1ea7bda8f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def LotFrontagecheck(df):\n\n    print(\"Number of samples with missing LotFrontage variable values (Before): \", df[df.LotFrontage.isna()].shape[0])\n    df_LotFrontage = df[['Neighborhood','LotFrontage']].groupby('Neighborhood').agg(lambda x:x.value_counts().index[0]).reset_index()\n    df_dict = dict([tuple(x) for x in df_LotFrontage.values])\n    print(\"\\nLotFrontage median valued per Neighborhood \", df_dict)\n    \n    df['LotFrontage'] =df.LotFrontage.fillna(-1)\n    df['LotFrontage'] =df.apply(lambda x: df_dict[x['Neighborhood']] if x['LotFrontage'] == -1 else x['LotFrontage'],axis=1)\n    print(\"\\nNumber of samples with missing LotFrontage variable values (After): \", df[df.LotFrontage.isna()].shape[0])\n    \n\n    return df\n    \ndf_lot = LotFrontagecheck(df_funct.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd967d023b7a776f2758d74275ce7343ac26271f"
      },
      "cell_type": "markdown",
      "source": "##### LotFrontage: taking neighborhood as reference most occuring distance is used for filling missing values. Inspiration: neighboring house have same distance to road /gate.\n\n* Lot area > 10000 & LotFrontage > 200  seems like outliers"
    },
    {
      "metadata": {
        "_uuid": "171d354f1715e98a01aa1bee87b36d7052608b9c"
      },
      "cell_type": "markdown",
      "source": "### 7. MSZoning"
    },
    {
      "metadata": {
        "_uuid": "3eecd29007907d674605071ecee152c12326ca47",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def msZoningWrangle(df):\n    \n    zone_related = ['LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n                    'Condition2', 'BldgType', 'OverallQual', 'OverallCond', 'MSZoning'\n                   ]\n    \n    temp  = df.groupby(['Neighborhood','MSSubClass','MSZoning']).\\\n    agg('count').reset_index().\\\n    sort_values(by=['SalePrice'],ascending=False)[['Neighborhood','MSSubClass','MSZoning','SalePrice']]\n    dd(temp[(temp.Neighborhood == 'IDOTRR')  |(temp.Neighborhood == 'Mitchel')])\n    \n    def returnmsZone(Neighborhood,MSSubClass):\n        return temp[(temp.Neighborhood == Neighborhood) &\n                   (temp.MSSubClass == MSSubClass)]['MSZoning'].tolist()[0]\n    \n    dd(df[df.MSZoning.isna()][['Neighborhood','MSSubClass','MSZoning']])\n    df['MSZoning'] = df.MSZoning.fillna(\"NA\")\n    df['MSZoning'] = df.apply(lambda x: returnmsZone(x['Neighborhood'], x['MSSubClass']) \n                              if x['MSZoning'] == \"NA\" else x['MSZoning'], axis=1)\n    \n    dd(df[df.index.isin([1915,2216,2250,2904])][['Neighborhood','MSSubClass','MSZoning']])\n\n    return df\n\ndf_ms = msZoningWrangle(df_lot.copy())    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "279247013392f2109f7b362cae054707a1e8520c"
      },
      "cell_type": "markdown",
      "source": "##### MSZoning is general zoning classification. Therefore,  it must be specific to an area and hence 'Neighborhood' is the variable to our rescue. MSZoning = RL  when neighbor is 'Mitchel' and  RM when neighbor is  IDOTRR and they are is missing."
    },
    {
      "metadata": {
        "_uuid": "d61a447937627d4dbc6778f2ff890e359c29a7c9"
      },
      "cell_type": "markdown",
      "source": "### 8.0 MasVnrType & MasVnrArea"
    },
    {
      "metadata": {
        "_uuid": "45d033892a94cf4e22daffdea5521148899a89be",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def MasVnrTypeCheck(df_temp):\n    #df = pd.read_csv('train.csv')\n    df=df_temp.copy()\n    print(df[df.MasVnrType.isna()].shape, )\n    #display.display(df[['Neighborhood','MasVnrType','Id']].groupby(['Neighborhood','MasVnrType']).agg('count').reset_index())\n    \n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrType.isna()].head())\n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrArea.isna()].head())\n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrArea == 0].head())\n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrType == 'None'].head())\n    \n    df['MasVnrType'] = df.MasVnrType.fillna(\"None\")\n    df['MasVnrArea'] = df.MasVnrArea.fillna(0)\n\n    df['MasVnrArea'] = df['MasVnrArea'].astype(np.float64)\n\n    return df\ndf_mas = MasVnrTypeCheck(df_ms)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ce92aaf2df956a7379b59490823b03e0c06d5cc"
      },
      "cell_type": "markdown",
      "source": "* MasVnrArea nan count is equivalent to MasVnrType count.\n* MasVnrArea == 0 is already present \n* whenever MasVnrArea == 0 MasVnrType is also None \n* Therefore, MasVnrArea will be mapped to zero when MasVnrType = None\n\n##### Outlier: area > 1400 is only one sample which has low sale price. Its overall condition and quality is moderate and there are enough sample for those bands."
    },
    {
      "metadata": {
        "_uuid": "0da58a3a3cb01966220c12cdf8ecfa6348a6720f"
      },
      "cell_type": "markdown",
      "source": "### 9. MiscFeature"
    },
    {
      "metadata": {
        "_uuid": "a0ae3cb5602c945d48beb29b3a5f20688b596fee",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def miscFeatureWrangle(df):\n    #df = df_temp.copy()\n    df_df = df.fillna('NA').groupby(['MiscFeature','MiscVal']).agg('count').reset_index()[['MiscFeature','MiscVal','SalePrice']]\n    \n    dd(\"Before :\",df_df[df_df.MiscVal == 0])\n    dd(df_df[df_df.MiscFeature == \"NA\"])\n    #dd(df_df[df_df.MiscVal == \"NA\"])\n    \n    df['MiscFeature'] = df.apply(lambda x: \"NA\" if x['MiscVal'] == 0 else x['MiscFeature'],axis=1)\n    df['MiscFeature'] = df.apply(lambda x: \"Gar2\" if x['MiscVal'] == 17000.0 else x['MiscFeature'],axis=1)\n    \n    df_df = df.fillna('NA').groupby(['MiscFeature','MiscVal']).agg('count').reset_index()[['MiscFeature','MiscVal','SalePrice']]\n    \n    dd(\"After :\",df_df[df_df.MiscVal == 0])\n    dd(df_df[df_df.MiscFeature == \"NA\"])\n    return df\n    \ndf_misc = miscFeatureWrangle(df_mas.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51deb1480b03d1e420e7d7823a562750bf424fd2"
      },
      "cell_type": "markdown",
      "source": "* When MiscVal == 0 ; MiscFeature is mostly NA (None). Note that it can be Shed or Other too. Will park it for next level fine tuning.\n* High Values are dedicated to 'Gar2'. Therefore, testing set missing value is gar2 for sure."
    },
    {
      "metadata": {
        "_uuid": "8980852de2a7528231e9ff46005d05c850df9c12"
      },
      "cell_type": "markdown",
      "source": "### 10. PoolQC"
    },
    {
      "metadata": {
        "_uuid": "b59cdac91e5bd9224c30c061d4be36a961c85254",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def poolWrangling(df):\n\n    print(\"Count of Samples which have PoolQC valid values : \", df[df.PoolQC != -1].shape[0] )\n    print(\"Count of missing PoolQC : \",df[df.PoolQC == -1].shape[0])\n    \n    dd(\"Pattern of missing PoolQC vs Overall Cond n Qual\", df[df.PoolQC == -1][['PoolArea', 'PoolQC','OverallCond','OverallQual']].head())\n    dd(\"Pattern of PoolQC when PoolArea = 0\",df[df.PoolArea == 0][['PoolArea', 'PoolQC','OverallCond','OverallQual']]['PoolQC'].unique())\n    dd(\"Dataset samples with PoolArea missing \",df[df.PoolArea.isna()])\n    \n    dd(\"PoolQC vs Overall Cond and Qual : \" , df.groupby(['OverallCond','OverallQual','PoolQC']).agg('count').reset_index()[['OverallCond','OverallQual','PoolQC','SalePrice']])\n    \n    df.loc[(df['PoolArea'] == 0) & (df['PoolQC'] == -1), 'PoolQC'] = 0\n    df.loc[(df['PoolArea'] > 0) & (df['PoolQC'] == -1), 'PoolQC'] = 2\n    \n    dd(df[df.PoolQC == -1][['PoolArea', 'PoolQC','OverallCond','OverallQual']])\n    dd(Counter(df.PoolQC))\n    \n    return df\n    \ndf_pool = poolWrangling(df_misc.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "172e20e2b8b7baa640b0c1a1a6271c1d0f8f2af0"
      },
      "cell_type": "markdown",
      "source": "##### When PoolArea is 0 PoolQC will be NA (no pool). When PoolArea > 0 it appears to be good candidates for \"Fa\"."
    },
    {
      "metadata": {
        "_uuid": "5f976fa4ac24e012934e6dff019d317325f55833"
      },
      "cell_type": "markdown",
      "source": "* when PoolQC should be \"NA\" when PoolArea = 0 [Thumb rule / Common sense]\n* Missing values have characterstics matching with that of \"Fair\" condition. It may be \"TA\" but we dont have enough evidence or rather no evidence for that."
    },
    {
      "metadata": {
        "_uuid": "8537fd4e4d98b0a4619eb86938947fd087d6a7b4"
      },
      "cell_type": "markdown",
      "source": "### 11. SaleType"
    },
    {
      "metadata": {
        "_uuid": "38bbafafa5d0c15aa4051c7078fa30173ebdb75d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def saleTypeWrangling(df):\n\n    dd(df[df.SaleType.isna()][['MSSubClass', 'MSZoning', 'SaleCondition','SaleType']])\n    df1 = df.groupby(['MSSubClass', 'MSZoning', 'SaleCondition','SaleType']).\\\n    agg('count').reset_index().sort_values('SalePrice',ascending=False)[['MSSubClass', 'MSZoning', 'SaleCondition','SaleType', 'SalePrice']]\n    dd(df1[df1.MSSubClass == '20'])\n    \n    def popSaleType(MSSubClass, MSZoning, SaleCondition):\n        return df1[(df1.MSSubClass == MSSubClass) &\n                   (df1.MSZoning == MSZoning) &\n                   (df1.SaleCondition == SaleCondition)\n                  ]['SaleType'].tolist()[0]\n    \n    df['SaleType'] = df.SaleType.fillna(\"NA\")\n    df['SaleType'] = df.apply(lambda x: popSaleType(x['MSSubClass'], x['MSZoning'], x['SaleCondition']) \n                              if x['SaleType'] == \"NA\" else x['SaleType']\n                              ,\n                             axis = 1)\n    dd(df[df.index == 2489][['MSSubClass', 'MSZoning', 'SaleCondition','SaleType']])\n    \n    return df\n    \ndf_sale = saleTypeWrangling(df_pool.copy())    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e0b115ed3874bf8919048cfc674be8758d3433b"
      },
      "cell_type": "markdown",
      "source": "### 12. Utilities"
    },
    {
      "metadata": {
        "_uuid": "2ecef306f895d6d7ee0a3f42a41792fd5d943a3c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def utilWrangling(df):\n    dd(\"Missing entries for Utilities : \",df[df.Utilities == -1][['Neighborhood','Utilities']])\n    \n    df1 = df.groupby(['Neighborhood','Utilities']).agg('count').reset_index().\\\n    sort_values('SalePrice',ascending=False)[['Neighborhood','Utilities','SalePrice']]\n    dd(\"Relation between Neighborhood and Utilities : \",df1)\n    \n    def returnUtil(Neighborhood):\n        return df1[(df1.Neighborhood == Neighborhood)][\"Utilities\"].tolist()[0]\n    \n    \n    df['Utilities'] = df.apply(lambda x: returnUtil(x['Neighborhood']) if x['Utilities'] == -1 else x[\"Utilities\"],axis=1)\n    \n    dd(\"Post imputation :\", df[df.index.isin([1915,1945])][['Neighborhood','Utilities']])\n    return df\n        \ndf_util = utilWrangling(df_sale.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9aaa68406e5357925e5feeef6a96b25721a19a34"
      },
      "cell_type": "markdown",
      "source": "### 13. KitchenQual"
    },
    {
      "metadata": {
        "_uuid": "f6f4a73a24abc1d41e649f77b4b1271440f67371",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def kitchenQual(df):\n    \n    dd(\"Missing samples for KitchenQual\", df[df.KitchenQual == -1][['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']])\n    \n    df1 = df.groupby(['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']).agg('count').\\\n       reset_index().sort_values('SalePrice', ascending= False)[['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']]\n    dd(\"Kitchen Table : \", df1)\n    def returnkqual(OverallCond, OverallQual, KitchenAbvGr):\n        return df1[(df1.OverallCond == OverallCond)&\n                  (df1.OverallQual == OverallQual) &\n                   (df1.KitchenAbvGr == KitchenAbvGr)\n                  ]['KitchenQual'].tolist()[0]\n    \n    df['KitchenQual'] = df.apply(lambda x: returnkqual(x['OverallCond'], x['OverallQual'],x['KitchenAbvGr'])\n                                 if x['KitchenQual'] == -1 else x['KitchenQual']\n                                ,axis=1)\n    dd(df[df.index == 1555][['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']])\n    return df\n\ndf_kitchen = kitchenQual(df_util.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3dd8863db77b0edd58cc8c049b116f711341989b"
      },
      "cell_type": "markdown",
      "source": "### Checkpoint - 1"
    },
    {
      "metadata": {
        "_uuid": "8b1ffb75eada2a615de32fbfd9eb625b9cb9e58d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "plotNAs(df_kitchen.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bd94ce29be7b1f0809fc883ec993e8b2d8b0101"
      },
      "cell_type": "code",
      "source": "plotMinusOnes(df_kitchen.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b005167f421464346e3d2a571ce8d30690d5222",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def bsmtInterpolate(df):\n    bsmt_column = ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']\n    print(bsmt_column)\n    \n    bsmt_column2 = [c for c in list(df) if 'Bsmt' in c]\n    \n    dd(\"BsmtCond missing entries : \", df[df.BsmtCond == -1][bsmt_column])\n    bsmt_index = []\n    bsmt_index += list(df[df.BsmtCond == -1].index)\n    \n    dd(\"BsmtExposure missing entries : \", df[df.BsmtExposure == -1][bsmt_column])\n    bsmt_index += list(df[df.BsmtExposure == -1].index)\n    \n    dd(\"BsmtFinType1 missing entries :\",df[df.BsmtFinType1 == -1][bsmt_column])\n    bsmt_index += list(df[df.BsmtFinType1 == -1].index)\n    \n    dd(\"BsmtFinType2 missing entries :\",df[df.BsmtFinType2 == -1][bsmt_column])\n    bsmt_index += list(df[df.BsmtFinType2 == -1].index)\n    \n    dd(\"BsmtQual missing entries : \", df[df.BsmtQual == -1][bsmt_column])\n    bsmt_index += list(df[df.BsmtQual == -1].index)\n    \n    df1 = df.groupby(bsmt_column).agg('count').reset_index().sort_values('SalePrice', ascending=False)[(bsmt_column + ['SalePrice'])]\n    df1 = df1.sort_values('SalePrice',ascending=False)\n    #dd(df1)\n    \n    def getBsmtCond(BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtCond'].tolist()[0]\n    \n    def getBsmtExposure(BsmtCond, BsmtFinType1, BsmtFinType2, BsmtQual):\n        return df1[(df1.BsmtCond == BsmtCond)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtExposure'].tolist()[0]\n    \n    def getBsmtFinType1(BsmtExposure, BsmtCond, BsmtFinType2, BsmtQual):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtCond == BsmtCond)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtFinType1'].tolist()[0]\n    \n    def getBsmtFinType2(BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtCond == BsmtCond)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtFinType2'].tolist()[0]\n    \n    def getBsmtQual(BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtCond):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtCond == BsmtCond)                   \n                  ]['BsmtQual'].tolist()[0]\n    \n    #['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']\n    df['BsmtCond'] = df.apply(lambda x: getBsmtCond(x['BsmtExposure'], x['BsmtFinType1'], x['BsmtFinType2'], x['BsmtQual'])\n                              if x['BsmtCond'] == -1 else x['BsmtCond']\n                             , axis = 1)\n    \n    df['BsmtExposure'] = df.apply(lambda x: getBsmtExposure(x['BsmtCond'], x['BsmtFinType1'], x['BsmtFinType2'], x['BsmtQual'])\n                              if x['BsmtExposure'] == -1 else x['BsmtExposure']\n                             , axis = 1)\n    \n    df['BsmtFinType1'] = df.apply(lambda x: getBsmtFinType1(x['BsmtExposure'], x['BsmtCond'], x['BsmtFinType2'], x['BsmtQual'])\n                              if x['BsmtFinType1'] == -1 else x['BsmtFinType1']\n                             , axis = 1)\n\n    df['BsmtFinType2'] = df.apply(lambda x: getBsmtFinType2(x['BsmtExposure'], x['BsmtFinType1'], x['BsmtCond'], x['BsmtQual'])\n                              if x['BsmtFinType2'] == -1 else x['BsmtFinType2']\n                             , axis = 1)\n    \n    df['BsmtQual'] = df.apply(lambda x: getBsmtQual(x['BsmtExposure'], x['BsmtFinType1'], x['BsmtFinType2'], x['BsmtCond'])\n                              if x['BsmtQual'] == -1 else x['BsmtQual']\n                             , axis = 1)\n    \n    dd(\"Post imputation :\", df[df.index.isin(bsmt_index)][bsmt_column])\n    \n    return df\ndf_bsmt_final = bsmtInterpolate(df_kitchen.copy())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13713fa809f4a16512052a073d0aac6e3ae3192b"
      },
      "cell_type": "markdown",
      "source": "### Garage*"
    },
    {
      "metadata": {
        "_uuid": "63d136a9701f81b0265ab0f6943a3c5c1f799a7a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def garageWrangling(df):\n    garage_columns = [c for c in df.select_dtypes(exclude=np.number) if \"Garage\" in c]\n    print(garage_columns)\n    garage_col2 = [c for c in list(df) if \"Garage\" in c]\n    grp_by = ['GarageType','MiscFeature','OverallQual','OverallCond','GarageFinish','GarageQual','GarageCond',\\\n              'GarageArea','GarageCars']\n    \n    new_col = list(set(garage_col2 + grp_by + ['MiscFeature']))\n    \n    miss_index = []\n    for c in garage_col2:\n        df_temp = df[df[c] == -1][new_col]\n        dd(\"Missing Entries for \"+c+ \" : \",df_temp)\n        miss_index += list(df_temp.index)\n    \n    \n    df1 = df.groupby(grp_by).agg('count').reset_index().sort_values('SalePrice',ascending=False)[grp_by + ['SalePrice']]\n    dd(\"when garage type == Detchd : \", df1[df1.GarageType == 'Detchd'])\n    \n    def getGarageArea( GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageArea'].tolist()[0]\n    \n    def getGarageCars( GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageCars'].tolist()[0]\n    \n    def getGarageCond(GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageCond'].tolist()[0]\n    \n    def getGarageFinish(GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageFinish'].tolist()[0]\n    \n    def getGarageQual(GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageQual'].tolist()[0]\n    \n    df['GarageArea'] = df['GarageArea'].fillna(\"NA\")\n    df['GarageArea'] = df.apply(lambda x: getGarageArea( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageArea'] ==\"NA\" else x['GarageArea']\n                                ,axis =1)\n    \n    df['GarageCars'] = df['GarageCars'].fillna(\"NA\")\n    df['GarageCars'] = df.apply(lambda x: getGarageCars( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageCars'] ==\"NA\" else x['GarageCars']\n                                ,axis =1)\n    \n    df['GarageCond'] = df.apply(lambda x: getGarageCond( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageCond'] == -1 else x['GarageCond']\n                                ,axis =1)\n    \n    df['GarageFinish'] = df.apply(lambda x: getGarageFinish( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageFinish'] == -1 else x['GarageFinish']\n                                ,axis =1)\n    \n    df['GarageQual'] = df.apply(lambda x: getGarageQual( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageQual'] == -1 else x['GarageQual']\n                                ,axis =1)\n    \n    dd(df[df.index.isin(miss_index)][new_col])\n    return df\ndf_garage_final = garageWrangling(df_bsmt_final.copy())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19f2a5108c043fc0cccee18640efa1796d4e9000"
      },
      "cell_type": "markdown",
      "source": "### Checkpoint 2"
    },
    {
      "metadata": {
        "_uuid": "85bc6e9c6dd857613a47454812d53723b6f864b9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "plotNAs(df_garage_final.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8266b63921a561be1d77f1b4ec4fea742d7e3f06"
      },
      "cell_type": "code",
      "source": "plotMinusOnes(df_garage_final.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c6a8e936951200b769135225fdc446b2f721fb04",
        "trusted": true
      },
      "cell_type": "code",
      "source": "did_we_miss_them = ['MSSubClass', 'OverallQual', 'OverallCond', 'FireplaceQu', ]\nfor c in did_we_miss_them:\n    dd(c, df_garage_final[c].unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8815a4053c5e543a182d20944f3313372059b81c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def FireplaceWrangling(df_temp):\n    df = pd.read_csv(train_File)\n    print(\"Count of missing FireplaceQu : \",df[df.FireplaceQu.isna()].shape[0])\n    dd(\"Count of Fireplaces == 0\",df[['Fireplaces', 'FireplaceQu']][df.Fireplaces == 0].shape)\n    dd(df[['Fireplaces', 'FireplaceQu']][df.FireplaceQu.isna()].head())\n    dd(\"When FireplaceQu == NA, Fireplaces:\", df[['Fireplaces', 'FireplaceQu']][df.FireplaceQu.isna()]['Fireplaces'].unique())\n    dd(\"When Fireplaces == 0, FireplaceQu:\",df[['Fireplaces', 'FireplaceQu']][df.Fireplaces == 0]['FireplaceQu'].unique())\n    \n    #now we have NA as nan in our wrangled dataset; ideally this should not be problem but let us be consistent.\n    df = df_temp.copy()\n    \n    df['FireplaceQu'] = df.apply(lambda x: 0 if x['Fireplaces'] == 0 else x['FireplaceQu'], axis=1 )\n    \n    dd(\"Post imputing :\",df.FireplaceQu.unique())\n    \n    return df\n    \ndf_fire = FireplaceWrangling(df_garage_final.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8401311eadea9f1633c1a94743772e57c94fc27"
      },
      "cell_type": "markdown",
      "source": "##### FireplaceQu will be mapped to NA (no fireplace) whenever Fireplaces = 0"
    },
    {
      "metadata": {
        "_uuid": "07789ba999bac49f46678d2bac9fd670498e740e"
      },
      "cell_type": "markdown",
      "source": "##### TotalBsmtSF = 0 indicates there is no basement. \n* Therefore, BsmtQual = BsmtCond = BsmtExposure = BsmtFinType1 = BsmtFinType2 = \"NA\"; when TotalBsmtSF = 0 \n\n* Outlier: (df.BsmtFinSF1 > 2000) & (df.SalePrice < 200000) (2 of them) are outlier because it not only brings the co relation down but also there are enough samples for outlier's overall condition and quality samples."
    },
    {
      "metadata": {
        "_uuid": "1d794a5a7d4a632ddb32f32ded1284994b11e06d"
      },
      "cell_type": "markdown",
      "source": "##### Let us check the datatype if it is corrupted due to our imputate operation"
    },
    {
      "metadata": {
        "_uuid": "44c51ce915e9a59ee5e84e7d96342973a54d9340",
        "trusted": true
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.object))\ncols_need_change = [c for c in list(df_before_clean.select_dtypes(include=np.number)) if c in post_imputing_cols]\n\n#dd(df_before_clean[df_before_clean.MasVnrArea == 'BrkFace'])\n\n'''for c in post_imputing_cols:\n    if c in cols_need_change:\n        print(c)\n        df_fire[c] = df_fire[c].astype(np.float64)'''\nprint()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d5c3eea379e76f4dcc48d151964d3ef6661537f5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.number))\n[c for c in list(df_before_clean.select_dtypes(include=np.number)) if c not in post_imputing_cols]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e37476763572dd5dd6b69b2232c9f2f5cf02fa32",
        "trusted": true
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.object))\n[c for c in list(df_before_clean.select_dtypes(include=np.object)) if c not in post_imputing_cols]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c191d8dbbf0c0bf38c32cfbd2371309c4b4430e8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.number))\n\n_= [dd(c,df_fire[c].unique()) for c in list(df_fire.select_dtypes(exclude=np.number))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3319a4189a423e54260dc913cbca4248029cc1a1"
      },
      "cell_type": "code",
      "source": "print(\"Are there any missing entries for numerical variables ?: \", len([dd(df_fire[c].unique() ) for c in post_imputing_cols if -1 in df_fire[c].unique()]) > 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c7e8b9e778ff6318aebcb8e06810f3942b9ef41c"
      },
      "cell_type": "markdown",
      "source": "### Outliers"
    },
    {
      "metadata": {
        "_uuid": "06fa29b21e977e3c04e775651c4a764bc56fb3e9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Shape of the imputed dataset : \", df_fire.shape)\ndf_out = df_fire[df_fire.SalePrice > 0].copy()\nprint(\"Shape of the Outlier Analysis dataset : \", df_out.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b0b590952c97238bc603c029c3e3956bb97d1f0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def computeCorrCols(df):\n    df_corr = df.corr()\n    \n    df1 = df_corr.stack().reset_index().rename(columns={'level_0': \"C1\", \"level_1\": \"C2\", 0 : \"Corr_val\"})\n    \n    df1['Corr_val']= df1['Corr_val'].abs()\n    df1 = df1[df1['Corr_val'] < 1].sort_values('Corr_val',ascending=False)\n    df1 = df1.drop_duplicates('Corr_val').reset_index(drop=True)\n    dd(df1)\n    \n    return df1\ndf_corr = computeCorrCols(df_out.copy())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd47cc743a96c14c811dd8994d07ced06d6e754e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def checkCorr(df_old):\n    df = pd.get_dummies(df_old)\n    corr_target = []\n    df_corr = df.corr()\n    for c in list(df_corr):\n        corr_target.append((c, np.abs(df_corr.loc[c,'SalePrice'])))\n\n    dd(\"Top 15 Numerical Variables with High Corr value for SalePrice :\",\n        [x for x in sorted(corr_target,key=lambda x: x[1], reverse=True) if '_' not in x[0]][1:15])\n    \n    dd(\"Top 15 Variables (All Types) with High Corr value for SalePrice :\",\n        [x for x in sorted(corr_target,key=lambda x: x[1], reverse=True)][1:15])\n\ncheckCorr(df_out.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5cdc7898e4f3674a1761b654f1a8c5e3e844cb16"
      },
      "cell_type": "markdown",
      "source": "##### There may be so many outliers but let us target the ones which are not only numerical but also impact the target variables. Let us target top 5 variables for outlier removal. i.e. OverallQual, GrLivArea, ExterQual, KitchenQual, GarageCars & TotalBsmtSF.\n\n* Note that I am not selecte GarageArea for outlier detection following reasons:\n   *  it has high co relation with GarageCars\n   * GarageCars and GarageArea both gets imputed in our last steps but the error margin is less for GarageCars when compared to GarageArea\n   "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44dbad07edf6254fadc55f682a8fd92a3ba72ff0"
      },
      "cell_type": "code",
      "source": "df_out.GarageCars.unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ccd670ab12050347b0d53488330e09b521155ad1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def topCorrWithOthers(df):\n    top_5_corr = ['OverallQual', 'GrLivArea', 'ExterQual', 'KitchenQual', 'GarageCars', 'TotalBsmtSF']\n    print(\"Co relation of the top 5 columns with others : \")\n    for c in top_5_corr:\n        dd(c, df_corr[(df_corr.C1 == c) | (df_corr.C2 == c)].head())\n        \ntopCorrWithOthers(df_fire.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a784ebbca29b46f1197da9d44a4e648dd09f187",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#df_out['SalePrice'] = df_out['SalePrice'] / df_out.GrLivArea",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cc96f8eb7f98a256000773b3c364672bdee27226"
      },
      "cell_type": "markdown",
      "source": "### 1. OverallQual Outlier Check"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4165d576c0340c1a465394af89aaa87539d1d1cc"
      },
      "cell_type": "code",
      "source": "def overallQualOutlier(df):\n    df.plot.scatter(\"OverallQual\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['OverallQual','SalePrice'])   \n    df1 = df[(df.OverallQual != 10) | (df.SalePrice >200000 )]\n    df1 = df1[(df1.OverallQual != 4) |  (df1.SalePrice <200000 )]\n    df1 = df1[(df1.SalePrice >125000 ) | (df1.OverallQual != 7)]\n    df1 = df1[(df1.OverallQual != 8) | (df1.SalePrice >125000 )]\n    df1 = df1[(df1.OverallQual != 7) | (df1.SalePrice <360000)]\n    df1 = df1[(df1.OverallQual != 8) | (df1.SalePrice < 460000)]\n   #.plot.scatter(\"OverallQual\", \"SalePrice\", title=\"Zoomed view on a outlier\")\n    \n    print(\"Outlier Count: \", df.shape[0]-df1.shape[0])\n    df1.plot.scatter(\"OverallQual\", \"SalePrice\", title=\"After Removal of Outlier\")\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['OverallQual','SalePrice'])\n    return df1\ndf_overallqual = overallQualOutlier(df_out.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a14b476281e0a2c6e7875bf95ff979aa393d2e25"
      },
      "cell_type": "markdown",
      "source": "### 2. GrLivArea Outlier Check"
    },
    {
      "metadata": {
        "_uuid": "77ebb719d748783da7503f1750667a793b116b2a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def grLivAreaOutlier(df):\n    df.plot.scatter(\"GrLivArea\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['GrLivArea','SalePrice'])\n    df1 = df[(df.GrLivArea < 4000) | (df.SalePrice >250000 )]\n    df1 = df1[(df1.GrLivArea > 2000) | (df1.SalePrice < 380000)]\n    df1 = df1[(df1.GrLivArea < 3300) | (df1.SalePrice > 220000)]\n    print(\"Outlier Count: \", df.shape[0]-df1.shape[0])\n    \n    #df1[(df1.GrLivArea > 3000)&(df.SalePrice <300000)].plot.scatter(\"GrLivArea\", \"SalePrice\", title=\"Zoomed look on a outlier\")\n    \n    df1.plot.scatter(\"GrLivArea\", \"SalePrice\", title=\"After Removal of Outlier\")\n    \n    #dd(corr(df1.GrLivArea, df1.SalePrice))\n    \n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['GrLivArea','SalePrice'])\n    \n    return df1\ndf_liv = grLivAreaOutlier(df_overallqual.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9a40a2d31539f5a7499d27edaf7dcd808519ca82"
      },
      "cell_type": "markdown",
      "source": "### 3. ExterQual Outlier Check"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89bd1f00db0b96b5f8aacb00160f6801186f7c48"
      },
      "cell_type": "code",
      "source": "def exernalQualOutlier(df):\n    df.plot.scatter(\"ExterQual\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['ExterQual','SalePrice'])\n    \n    print(df['SalePrice'][df.ExterQual == 4].min(), df['SalePrice'][df.ExterQual == 4].max())\n    #df1 = df[(df.SalePrice)]\n    df1 = df[(df.ExterQual != 4) | ((df.SalePrice !=  52000.0) & ( df.SalePrice !=  745000.0))]\n    \n    print(\"Outlier Count : \", df.shape[0]-df[(df.ExterQual != 4) | ((df.SalePrice !=  52000.0) & ( df.SalePrice !=  745000.0))].shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['ExterQual','SalePrice'])\n    df1.plot.scatter(\"ExterQual\", \"SalePrice\", title=\"After Removal of Outlier\")\n    \n    return df1\n\ndf_external_qual = exernalQualOutlier(df_liv.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d90f7f47a71b0aff09176fa5e6521f518f6723eb"
      },
      "cell_type": "markdown",
      "source": "### 4. KitchenQual Outlier Check"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10d33c63722452d60174b0cb4675fecf25d4381e"
      },
      "cell_type": "code",
      "source": "def kitchenQualOutlier(df):\n    df.plot.scatter(\"KitchenQual\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['KitchenQual','SalePrice'])\n    print(\"Max values when KitchenQual == 3 :\", df[df.KitchenQual == 3]['SalePrice'].max())\n    print(\"Max values when KitchenQual == 4 :\", df[df.KitchenQual == 4]['SalePrice'].max())\n    df1 = df[(df.KitchenQual != 3) |  ( df.SalePrice !=  359100.0)  ]\n    df1 = df1[ (df1.KitchenQual != 4) | ( df1.SalePrice !=  625000.0)]\n    print(\"Outlier Count : \", df.shape[0] - df1.shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['KitchenQual','SalePrice'])\n    df1.plot.scatter(\"KitchenQual\", \"SalePrice\", title=\"After Removal of Outlier\")\n    return df1\n    \ndf_kitchen_qual = kitchenQualOutlier(df_external_qual.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4b30a26344072e6a85977a10d80d88fd0f31f16a"
      },
      "cell_type": "markdown",
      "source": "### 5. GarageCars Outlier Check"
    },
    {
      "metadata": {
        "_uuid": "288320ce0d38599cc7d8d5d3c6570de8d6b375e3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def garageCarsOutlier(df):\n    df.plot.scatter(\"GarageCars\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['GarageCars','SalePrice'])\n    \n    df1 = df[(df.GarageCars < 4)]\n    \n    print(\"Outlier Count : \", df[(df.GarageCars == 4)].shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['GarageCars','SalePrice'])\n    df1.plot.scatter(\"GarageCars\", \"SalePrice\", title=\"After Removal of Outlier\")\n    return df1\n    \ndf_gar_car = garageCarsOutlier(df_kitchen_qual.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fbda1a6414891851c2965e72f56926ff34a2bba2"
      },
      "cell_type": "markdown",
      "source": "### 6. TotalBsmtSF Outlier Check"
    },
    {
      "metadata": {
        "_uuid": "4643985119095f8e33bbc535a3e3fec0b67617e0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def totalBsmtSFOutlier(df):\n    df.plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['TotalBsmtSF','SalePrice'])\n    df[df.TotalBsmtSF >3000].plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Zoomed look on Outlier\")\n    df[df.TotalBsmtSF >3100].plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Further Zoomed look on Outlier\")\n    df[(df.TotalBsmtSF >1000) & (df.TotalBsmtSF <1500) & (df.SalePrice <100000)].plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Further Zoomed look Outlier 2\")\n    df[(df.TotalBsmtSF >2000) & (df.TotalBsmtSF <2500) & (df.SalePrice <210000)].plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Further Zoomed look Outlier 3\")\n    \n    df1 = df[(df.TotalBsmtSF < 3000) | (df.SalePrice > 300000)]\n    df1 = df1[(df1.TotalBsmtSF < 1000) | (df1.TotalBsmtSF > 1500) | (df1.SalePrice > 65000)]\n    df1 = df1[(df1.TotalBsmtSF < 2000) | (df1.TotalBsmtSF > 2500) | (df1.SalePrice > 150000)]\n    \n    print(\"Outlier Count : \", df.shape[0] - df1.shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['TotalBsmtSF','SalePrice'])\n    df1.plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"After Removal of Outlier\")\n    \n    return df1\n\ndf_tot = totalBsmtSFOutlier(df_gar_car.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cee46947c9d42be2e314c31122f2bcdaec7943ce",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "checkCorr(df_tot.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd2d1a9027f89d1372bfce4f6ec6aa28a558b1e8"
      },
      "cell_type": "markdown",
      "source": "#### We have so far identified 31 outliers altogether from 6 variables from our input data set. These outliers have increased the co relation with target variable."
    },
    {
      "metadata": {
        "_uuid": "2bb2f8ce5f08c97196d6fc64b2e77c7e2c2efd1b"
      },
      "cell_type": "markdown",
      "source": "##### Let us check with XGBoost for the score."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "971c59e4cbc6176b03a3a89931e6e21dae7f50b2"
      },
      "cell_type": "code",
      "source": "def preProcessData(df, log=False):\n    \n    print(\"Shape of the data set before pre processing : \", df.shape )\n\n    #get dummies\n    if log:\n        print(\"Categorical columns : \", list(df.select_dtypes(exclude=np.number)))\n    df = pd.get_dummies(df, dtype=np.float64)\n    \n    print(\"\\n\\nShape of the data set after pre processing : \", df.shape )\n    \n    if log:\n        print(\"Columns in the data set are : \",list(df))\n\n    return df\n\ndf_prep = preProcessData(df_fire.copy())\ndf_prep.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a413ba0b9e3b9fd349629be63e4333d2687387ad",
        "trusted": true
      },
      "cell_type": "code",
      "source": "list(df_prep)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "533356e54ae389ce2b237f00d85d963af6ecfd10",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def removeOutlier(df):\n    \n    print(\"Shape before removing outlier : \", df.shape)\n    '''df = df[(df.GrLivArea < 4000) | (df.SalePrice >250000) | (df.SalePrice == 0) ]\n    df = df[(df.GarageCars < 4) | (df.SalePrice == 0)]\n    df = df[((df.GarageArea < 1200) | (df.SalePrice > 300000)) | (df.SalePrice ==0)]\n    df = df[((df.TotalBsmtSF < 3000) | (df.SalePrice > 300000)) | (df.SalePrice ==0)]'''\n    df = df[(df.GrLivArea < 3000)| (df.SalePrice ==0)]\n    df = df[(df.SalePrice < 200)| (df.SalePrice ==0)]\n    print(\"Shape after removing outlier : \", df.shape)\n    \n    df1 = pd.concat([df_tot,df[df.SalePrice == 0]])\n    \n    return df1\n\ndf_out_removed = removeOutlier(df_fire.copy())\ndf_out_removed.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "806a5ed17172affcfedbf50d70330b8646c7f239",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from IPython import display\ndef transformTarget(df_temp, revert=False ):\n    '''df = df_temp.copy()\n    #df['new_variable'] = df.BedroomAbvGr * .1 + df.FullBath *.25 + df.HalfBath * .5 + df.BsmtFullBath *.75 + df.BsmtHalfBath * 1\n    #df['new_variable'] = df.BedroomAbvGr * .1 + df.FullBath *.25 + df.HalfBath * .5 + df.BsmtFullBath *.75 + df.BsmtHalfBath * .1\n    df['new_variable'] = df.BedroomAbvGr.apply(lambda x: x if x > 0 else 1)\n    #print(\"new_variable calculated\")\n    #display.display(df[df.new_variable.isna()])\n    \n    if not revert:\n        df['sales_per_new'] = df['SalePrice']/ df.new_variable\n        #display.display(df[df.sales_per_new.isna()])\n        y = np.array(df['sales_per_new'].apply( lambda x: math.log(x)))\n        return y\n    \n    df['sales_per_new'] = df['SalePrice']* df.new_variable\n    #print(\"sales_per_new calculated\")\n    #display.display(df[df.sales_per_new.isna()])'''\n    #return np.log( df_temp.SalePrice / df_temp.GrLivArea)\n    return np.log(df_temp['SalePrice'])\n    #return (df_temp['SalePrice'])\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "883c06225559bfa909bc320fc88689df4bdd259e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def newBoxCoxTranformation(df_temp,target,testFile=False, log=False):\n    df = df_temp.copy()\n    #assuming that only numerical features are presented\n    if log:\n        print(\"Shape of the dataset initial : \", df.shape)\n    \n    if not testFile:\n        df =df[df.SalePrice >0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        \n        #display.display(df[df['SalePrice'].isna()])\n        y = transformTarget(df)\n        X= df.drop([target],axis = 1)\n        #X=df.drop(target, axis=1)\n        x_columns = list(X)\n        #print(x_columns)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X_testx = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X,y)\n        #X_testxx = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X,df.SalePrice)\n        #print(np.unique(X == X_testx))\n        #print(np.unique(X == X_testxx))\n        #print(np.unique(X_testx == X_testxx))\n        X = pd.DataFrame(X, columns=x_columns)\n        \n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        #X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        X = preprocessing.StandardScaler().fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape, y.shape)\n        return X,y\n    else:\n        df = df[df.SalePrice == 0.0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        X=df.drop(target,axis = 1)\n        x_columns = list(X)\n        #print(x_columns)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        \n        X = pd.DataFrame(X, columns=x_columns)\n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        \n        #X = preprocessing.power_transform( X, method='box-cox')\n        #X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        X = preprocessing.StandardScaler().fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape)\n        return X\n        \n#df_fire_b = df_fire.copy()\n#df_fire_b['SalePrice'] = df_fire_b['SalePrice'] / df_fire_b.GrLivArea\ndf_out_removed = removeOutlier(df_fire.copy())\ndf_prep = preProcessData(df_out_removed.copy())\nX = newBoxCoxTranformation(df_prep.copy(),'SalePrice',True,False)  \nX,y = newBoxCoxTranformation(df_prep.copy(),'SalePrice',False,False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4531f7f0612000650dac29fd8f5ec7dd7c06bd8d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=random.randint(1,500))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f9fcf3bfb0b8a6dc009fa8cd5f58aa149a1a0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#reg = XGBRegressor(max_depth=4, n_estimators=200)\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.0, reg_lambda=0.0, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1) #0.13073788936978095'''\n\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=800,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5) #.144'''\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.35, reg_lambda=0.35, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)#0.13515'''\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.75,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=6, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.1, reg_lambda=0.6, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1) #0.13495'''\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.1, reg_lambda=0.35, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5) #0.13486'''\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.75,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=300,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.25, reg_lambda=0.25, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)'''\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.25, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.25, reg_lambda=0.25, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5) #0.13....'''\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.25, reg_lambda=0.25, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5) #0.131...'''\nreg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.75,\n       colsample_bytree=0.25, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.0, reg_lambda=0.5, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)\nreg.fit(X_train,y_train)\nreg.score(X_test,y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eed59a169d49c2cd279cbb62e69b0f7f127f11a3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#np.sqrt(mean_squared_log_error((y_test), (reg.predict(X_test))))\nnp.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(reg.predict(X_test))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "39d3c412bd01485aecba2d159d02aa4dbf8dac67",
        "trusted": true
      },
      "cell_type": "code",
      "source": "reg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ddc77523a1218401082b46de2311c633ecebeaa1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def dummyCrossValidation(loop_count):\n    for i in range(loop_count):\n        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=random.randint(1,500))\n        #reg = XGBRegressor(max_depth=3, n_estimators=100, reg_alpha=.5, reg_lambda=.5)\n        reg.fit(X_train,y_train)\n        \n        print(\"R2 Score: \", reg.score(X_test,y_test))\n        #print(\"RMSLE Score : \", np.sqrt(mean_squared_log_error((y_test), (reg.predict(X_test)))))\n        print(\"RMSLE Score : \", np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(reg.predict(X_test)))))\ndummyCrossValidation(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c02210e9c30a1e470f3504f4b772ac308c963af",
        "trusted": true
      },
      "cell_type": "code",
      "source": "reg.fit(X,y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "deda18c8d44e83cac4810ec68326bff1628b2ce3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def checkTheTrainFile(reg,df_prep):\n    df_train = pd.read_csv(train_File)\n    #df_tra['SalePrice'] = 0.0\n    \n    #df_train =  pd.read_csv(train_File)\n    #df_concat = pd.concat([df_train,df_test])\n\n    #print(df_test[df_test.TotalBsmtSF.isna()])\n    #return\n    #df = giveMeWrangledData(df_concat,True)\n    \n    #print(df.info())\n    #df = preProcessData(df)\n    #print(df.info())\n    #dt = df_prep.copy()\n    \n    #df_prep = preProcessData(df_fire.copy())\n    #dt = dt[dt.SalePrice > 0]\n    df_prep1 = df_prep.copy()\n    df_prep1 = df_prep1[df_prep1['SalePrice'] > 0]\n    df_prep1['New_SalePrice'] = 0.0\n    \n    df_prep = df_prep[df_prep['SalePrice'] > 0]\n    df_prep['SalePrice'] = 0\n    \n    X = newBoxCoxTranformation(df_prep.copy(),'SalePrice',True)\n    #print(np.sqrt(mean_squared_log_error(y, reg.predict(X))))\n    \n    df_prep1['New_SalePrice'] = list(np.exp(reg.predict(X)))\n    #df_prep1['New_SalePrice'] = reg.predict(X)\n    #df_prep1['New_SalePrice'] = df_prep1['New_SalePrice'] * df_prep1.GrLivArea\n    #df_prep1['SalePrice'] = df_prep1['SalePrice'] * df_prep1.GrLivArea\n    \n    \n    df_train_score = df_prep1[df_prep1.SalePrice > 0]\n    \n    print(np.sqrt(mean_squared_log_error(df_train_score['SalePrice'], df_train_score['New_SalePrice'])))\n    \n    #return df_test, X, reg.predict(X)\n#df_test, X_dummy, y_dummy= checkTheTestFile(reg)\ncheckTheTrainFile(reg,df_prep.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9222b48f2887acd0e3ce113ea35af963e2d9a7fd",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def checkTheTestFile(reg):\n    df_test = pd.read_csv(test_File)\n    df_test['SalePrice'] = 0.0\n    print(df_test.shape)\n    \n    #df_train =  pd.read_csv(train_File)\n    #df_concat = pd.concat([df_train,df_test])\n\n    #print(df_test[df_test.TotalBsmtSF.isna()])\n    #return\n    #df = giveMeWrangledData(df_concat,True)\n    \n    #print(df.info())\n    #df = preProcessData(df)\n    #print(df.info())\n    X = newBoxCoxTranformation(df_prep.copy(),'SalePrice',True)\n    #print(np.sqrt(mean_squared_log_error(y, reg.predict(X))))\n    #df_test['SalePrice'] = (reg.predict(X))\n    df_test['SalePrice'] = np.exp(reg.predict(X))\n    #df_test['SalePrice'] = df_test['SalePrice'] * df_test.GrLivArea\n    \n    return df_test, X, reg.predict(X)\ndf_test, X_dummy, y_dummy= checkTheTestFile(reg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "be5f648d502133c2a482563a7f9691ba120dbd2c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']].to_csv('smubmission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a2c810e950be5a1c2abeab4c194b53a4a5f0d958",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "300dbf8ff9df501991f9c45c0046ff8b08c15228",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def forCrossValidationStratifiedShuffleSplit(df):\n    sss = StratifiedShuffleSplit(n_splits=10, test_size=.5, random_state=1986)\n    #print(\"Number of Splits configured :\", sss.get_n_splits(df, df.BldgType))\n    \n    for train_index, test_index in sss.split(df, df.BldgType):\n        yield train_index, test_index\n        \n    for train_index, test_index in sss.split(df, df.OverallQual):\n        yield train_index, test_index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d87e6c649933258dcaa6f6a269b274ef647d462",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv(train_File)\ndef crossValidationScoring(reg,X,y):\n    return -np.sqrt(mean_squared_log_error(np.exp(y), \n                                          np.exp(reg.predict(X))\n                                          ))\n    #return np.sqrt(mean_squared_log_error(np.exp(y), \n    #                                      np.exp(reg.predict(X\n    #                                                        ))\n    #                                      ))\n\n    #return np.sqrt(mean_squared_log_error((y), \n    #                                      (reg.predict(X))\n    #                                      ))\nmean_temp_rmsle = np.mean(cross_val_score(reg,X,y,cv= 5,scoring='neg_mean_squared_log_error'))\nprint(\"RMSE with without target variable transformation : \", np.sqrt(mean_temp_rmsle * -1))\n\nmean_temp_rmsle = np.mean(cross_val_score(reg, X, y,\n                                          cv= 5,\n                                          scoring=crossValidationScoring))\nprint(\"RMSE with post target variable transformation : \", mean_temp_rmsle)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "741c90110661a8e9580a2da3c87eaefbcfc19cb9",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "from sklearn.model_selection import GridSearchCV\ndef gridSearchCVImp():\n    start_time =datetime.datetime.now()\n    #reg = XGBRegressor(n_jobs =2, reg_alpha=.5, reg_lambda=.5, subsample=.5)\n    reg = XGBRegressor(n_jobs =2,subsample=.5)\n    \n    parameters = {\n        'max_depth':list(range(3,7)),\n        'colsample_bylevel' : np.arange(0.25, 1.0, 0.25),\n        'n_estimators' : list(range(100,600,100)),\n        'colsample_bytree' : np.arange(0.25, 1.0, 0.25),\n        'reg_alpha': np.arange(0.0, 1.0, 0.25),\n        'reg_lambda': np.arange(0.25, 1.0, 0.25)\n        }\n    cv= ShuffleSplit(n_splits=5, test_size=.1, random_state=1986)\n    reg_grid = GridSearchCV(reg, parameters, \n                            #cv=forCrossValidationStratifiedShuffleSplit(df_train),\n                            cv=cv,\n                            n_jobs = 2,\n                            #scoring = 'neg_median_absolute_error',\n                            #scoring = 'neg_mean_absolute_error',\n                            #scoring = 'neg_mean_squared_log_error',\n                            scoring = crossValidationScoring,\n                            verbose=1,\n                            #error_score ='raise'\n                            error_score =5\n                            #pre_dispatch = 2\n                           )\n    reg_grid.fit(X,y)\n    \n    print(\"Total time for the gridserach\", datetime.datetime.now() - start_time)\n    \n    return reg_grid\nreg_grid = gridSearchCVImp()\nprint(reg_grid.best_estimator_)\nprint(reg_grid.best_score_)"
    },
    {
      "metadata": {
        "_uuid": "e29096a5c33ace235c5911b9a905704f1a510edf"
      },
      "cell_type": "markdown",
      "source": "print(np.sqrt(reg_grid.best_score_ * -1))"
    },
    {
      "metadata": {
        "_uuid": "bc1958ca77e18178b3423a523b98a0888c006b0f"
      },
      "cell_type": "markdown",
      "source": "This statement is written just to keep the kaggle kernel up during the gridsearch.This again to keep the session live."
    },
    {
      "metadata": {
        "_uuid": "6f3ea6b56764eb1beea2b8dcd08d44c53df03b7b"
      },
      "cell_type": "markdown",
      "source": "Conservative GridSearch : testsize : .5, scoring: r2\n\nTotal time for the gridserach 0:54:19.899125\n\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.25, reg_lambda=0.25, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)\n       \n0.9153911325136409"
    },
    {
      "metadata": {
        "_uuid": "fcdf145903c3037fd624e37200c384f2171b57b4"
      },
      "cell_type": "markdown",
      "source": "Conservative GridSearch : testsize : .5, scoring: neg_msle\n\nTotal time for the gridserach 0:55:00.412282\n\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.25, reg_lambda=0.25, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)\n       \n-7.906906622834065e-05"
    },
    {
      "metadata": {
        "_uuid": "849bfc268166a0e4629c715be251642f79ad5a5e"
      },
      "cell_type": "markdown",
      "source": "Conservative GridSearch : testsize : .5, scoring: negative RMSLE\n\nTotal time for the gridserach 0:50:49.492892\n\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.25, reg_lambda=0.25, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)\n       \n-0.11425853874531103 ==> Kaggle Score gave 0.131....."
    },
    {
      "metadata": {
        "_uuid": "d7167b5a9242f805ee1608910fc3584e1c6eefaa"
      },
      "cell_type": "markdown",
      "source": "Optimistic GridSearch: testsize: .1 scoring: negative RMSLE\n\nTotal time for the gridserach 1:34:16.138500\n\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.75,\n       colsample_bytree=0.25, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.0, reg_lambda=0.5, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)\n       \n-0.1135887234582907"
    },
    {
      "metadata": {
        "_uuid": "a8defc37d56eadf22109ec85a55c302673657c71"
      },
      "cell_type": "markdown",
      "source": "1. OverallQual target transformation\n2. GrLivArea target transformation\n3. Less number of outliers removal\n4. Imputing based on high co related variables.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}