{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e0a351d9be51b1c1d9839a0c13777431bdad4125"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport datetime\n\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\nfrom scipy import stats\nimport math\nimport random\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.metrics import  mean_squared_log_error\n\nfrom xgboost import XGBRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f319576eaf2c5298673868fbe31b5f1128d1232"
      },
      "cell_type": "code",
      "source": "train_File = '../input/train.csv'\ntest_File = '../input/test.csv'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51261ef31da4070dcc29efd4c83ede22dbdedc1c"
      },
      "cell_type": "code",
      "source": "dd = display.display",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "64b6f65571a681c0fa2fdadf150ec80bbdf3b650"
      },
      "cell_type": "markdown",
      "source": "# 1. Gather"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a6076c0664a55d4d6c673ead696040f69210177"
      },
      "cell_type": "code",
      "source": "def loadData():\n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    \n    df = pd.concat([df_train, df_test], axis=0,sort=True,ignore_index=True)\n    \n    return df\n\ndf_before_clean = loadData()\ndd(df_before_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "745cc50e8d0ecfc797d4305fd168c2b5189860ca"
      },
      "cell_type": "markdown",
      "source": "# 2. Assess Data : Inspecting Data for Quality and Tidiness Issues\n#### 2.1 Quality Issues : Issues with content - missing, duplicate or incorrect data. a.k.a Dirty data \n* 2.1.a Completeness : *\"Are there any rows, columns or cells missing values?\"*\n  * 35 columns have the missing values: \n  \n  ['Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtUnfSF', 'Electrical', 'Exterior1st', 'Exterior2nd', 'Fence', 'FireplaceQu', 'Functional', 'GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageYrBlt', 'KitchenQual', 'LotFrontage', 'MSZoning', 'MasVnrArea', 'MasVnrType', 'MiscFeature', 'PoolQC', 'SalePrice', 'SaleType', 'TotalBsmtSF', 'Utilities']\n######   \n* 2.1.b Validity : *\"Does the data comply to the data schema like duplicate patient id or zip code being < 5 digits or float data type?\"*\n######   \n   * Following are Categorical Variables but currently are being considered as integer/float:\n\n        * MSSubClass\n        * OverallQual\n        * OverallCond\n        * FireplaceQu\n        * MoSold\n######   \n   * Following variables are supposed to be Integer type but Box-Cox or Scaling will anyway type cast them to float:\n\n      * LotFrontage, LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, YrSold, SalePrice\n   * GarageYrBlt will be NA when garage is not available for the house and hence this variable needs to be dropped as it does not comply to the schema.\n   \n######      \n  \n* 2.1.c Accuracy : *\"Wrong data that is valid. like hieght = 300 inches; it still complies to the standard i.e. inches but data is in accurate.\"*\n######   \n    * MSZoning has 4 missing entries. Also, we do not have data samples to Agricultre (A),  Industrial (I), Residential Low Density Park (RP). Therefore, there exist a probability that missing entries will be replaced with wrong data which are valid for the variables.\n######   \n    * Similarly, Utilites has 2 missing entries. Also, we do not have data samples for NoSewr ((Electricity, Gas, and Water (Septic Tank)) and ELO (Electricity only)\n######   \n    * Exterior1st, has no missing values in training data set. But has no samples for 'Other' and 'PreCast' Material. Testing data has one missing sample for the variable.\n######   \n    * Exterior2nd has no missing values in the training data set. But has no samples for \"PreCast\" material. Testing data has one missing sample for the variable.\n######   \n    * MasVnrType has 8 missing values in training data set. It has total of 5 valid values.But there are no samples for \"CBlock\" (Cylinder block). One in testing set also has a record with missing value for this variable.\n######   \n    * ExterQual has no samples for \"Po\" but fortunately there are no missing values for it in both training or testing data.\n######   \n    * BsmtQual has 37 missing entries. It has no samples for \"Po\" ((Poor (<70 inches)). Testing data set has 46 missing entries.\n######   \n    * BsmtCond has 37 missing entries. It has no samples for \"Ex\" (Excellent). There are 46 missing entries in the testing dataset.\n######   \n    * KitchenQual has no training samples on \"Po\" but testing sample has a missing entry.\n    * Functional has no training samples for \"Po\" but testing sample has a missing entry.\n    * PoolQC has no samples for \"Typ\" but testing sample has missing values.\n    * SaleType has no samples for \"VWD\" but testing sample has a missing value record.\n######   \n######   \n* 2.1.d Consistency : *\"Both valid and accurate but inconsistent. state = california and CA\"*\n######   \n  * BsmtExposure has training samples as NA for both No Basement and also for missing values. There are also 2 testing samples with missing values as NA.\n######   \n  * BsmtFinType2 has training samples as NA for both No Basement and also for missing values.\n######   \n  * TotalBsmtSF has both 0 and NA representing as missing basement.\n######   \n  * BsmtExposure has NA for both missing and no basement houses.\n  * BsmtFinType2 has NA for both missing and no basement for a house.\n  \n"
    },
    {
      "metadata": {
        "_uuid": "2c4dd8490b734cecc9b1183116622b54a31483ac"
      },
      "cell_type": "markdown",
      "source": "#### 2.2 Tidiness Issues: Issues with structure - untidy or messy data\n* 2.2.a Each observation is a row\n  * No Issues: Each observation is a unique house (no duplicate records)\n######   \n* 2.2.b Each variable is a column\n  * No Issues: There are no colummns with multi data or concatenated data.\n######  \n* 2.2.c Each observational unit is a table\n  * No Issues: There are no cross referring keys present in the table. Bsmt* and Garage* variables do form a logical group but there is no unique identities to the group.\n######   "
    },
    {
      "metadata": {
        "_uuid": "bfa5a3201b071e159d6f0ba1ce70ce9a54a1853e"
      },
      "cell_type": "markdown",
      "source": "##### Hypothesis 1: Bsmt__ variables are NA when TotalBsmtSF is 0\n##### Proof:\n* BsmtFinType1 is NA when TotalBsmtSF is 0\n* BsmtUnfSF is 0 whenever TotalBsmtSF is 0; Even in testing set it is NA only when TotalBsmtSF is NA\n* BsmtFullBath is 0 whenever TotalBsmtSF is 0; Even in testin set it is NA only when TotalBsmtSF is NA or 0.\n* BsmtHalfBath is 0 whenever TotalBsmtSF is 0; Even in testin set it is NA only when TotalBsmtSF is NA or 0."
    },
    {
      "metadata": {
        "_uuid": "dee7662b7b69a4d6adde71ccf2d08eb83e61f071"
      },
      "cell_type": "markdown",
      "source": "##### Key Observations:\n* Dataset has House Prices which were sold in between 2006 - 2010.\n* Surprised to see no bathroom and no bedroom but with kitchen Houses !!! Where do they sleep and shit after the heavy meal ?\n* NA value in GarageType can be easily mis interpreted as missing value. However, it is not true. NA in GarageType clearly indicates no garage because in both train and testing dataset GarageArea = 0 in all those cases. Similarly, GarageYrBlt, GarageFinish, GarageQual, GarageCond are also NA when GarageArea = 0. And GarageCars,GarageArea = 0 ==> GarageArea = 0.\n* How do we verify NA in Fence as missing entry or No Fence ??\n* How do we verify NA in MiscFeatures as None or missing value ?? Note that MiscVal is zero for NA, Othr & Shed."
    },
    {
      "metadata": {
        "_uuid": "31b5569b22ccc4a934b9c7a3de528de1be0a9b15"
      },
      "cell_type": "markdown",
      "source": "### 2.1.a Completeness : *\"Are there any rows, columns or cells missing values?\"*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20d8fa80fa5b91ff02c5cf998045da909f6cbade"
      },
      "cell_type": "code",
      "source": "def missingValueAssessment(df):\n    nan_columns = df.columns[df.isna().any()].tolist()\n    print('NaN columns :', nan_columns, \"\\n# :\", len(nan_columns))\n    \n    print(\"Duplicated rows count: \", df[df.duplicated()].shape)\n    df = df.fillna('NA')\n    print(\"Duplicated rows count: \", df[df.duplicated()].shape)\n    \nmissingValueAssessment(df_before_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c11b14cec3d14276b514b892bee289f9a3a7fe3"
      },
      "cell_type": "markdown",
      "source": "### 2.2.a Each observation is a row"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b148f64b18f8778d327cfb7efa5512eee1271676"
      },
      "cell_type": "code",
      "source": "def checkHouseIsRepeated(df):\n    df_temp = df.groupby(['SalePrice','GrLivArea','YearBuilt','YearRemodAdd']).agg('count').reset_index()[['SalePrice','GrLivArea','YearBuilt','YearRemodAdd','Id']]\n    dd(\"Samples with same 'SalePrice','GrLivArea','YearBuilt','YearRemodAdd' : \",df_temp[df_temp.Id > 1])\n    \n    \ncheckHouseIsRepeated(df_before_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e86b5ac9a7d324ab778ceeb5a92640fc7102173"
      },
      "cell_type": "markdown",
      "source": "* As there are no time series data: as in there is no variable indicating the time of the reading carried out, It is safe to assume the reading was done at one shot and there would not be any duplicate entries of a house.\n* With the above assumption, group by 'SalePrice','GrLivArea','YearBuilt','YearRemodAdd' count indicates that there are no duplicate records."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "efb54e7b72fe82b33cd27f532167bd87e77ed644"
      },
      "cell_type": "code",
      "source": "df_before_clean.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f93b84bebcd31734e295a6a8103cce6a90deba4"
      },
      "cell_type": "markdown",
      "source": "# 3.0 Cleaning Data"
    },
    {
      "metadata": {
        "_uuid": "3dfbfab28b468242244b5322d32700d5654abd9b"
      },
      "cell_type": "markdown",
      "source": "##### Let us first do the cleaning activities where we have high confidence of imputing the values as listed in the above Assessment summary."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb0cf59f1497315a4e353b9046fd567b3147dcda"
      },
      "cell_type": "code",
      "source": "def cleanStage1(df):\n    \n    #convert data type\n    #we are being little lineant to give float64 for YearBuilt, YrSold but those guys are going to be box-coxed \n    #so let them at least enjoy the bigger size for now\n    float64_variables = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', \\\n                     'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \\\n                     'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\\\n                     'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n                     'PoolArea', 'MiscVal', 'YrSold', 'SalePrice']\n    \n    for c in float64_variables:\n        df[c] = df[c].astype(np.float64)\n    \n    int_to_categorical_variables = ['MSSubClass', 'OverallQual', 'OverallCond', 'FireplaceQu', 'MoSold']\n    for c in int_to_categorical_variables:\n        df[c] = df[c].astype(str)\n        \n    \n    #Blunt Initialization\n    #df = df.fillna(\"NotAvailable\")\n    \n    #TotalBsmtSF == NotAvailable\n    df['TotalBsmtSF'] = df.TotalBsmtSF.fillna(\"NotAvailable\")\n    df['TotalBsmtSF'] = df.TotalBsmtSF.apply(lambda x: 0 if \"NotAvailable\" == x else x)\n    \n    #BsmtQual\tBsmtCond\tBsmtExposure\tBsmtFinType1\tBsmtFinType2\n    df['BsmtQual'] = df.apply(lambda x: \"NA\" if x['TotalBsmtSF'] == 0 else x['BsmtQual'], axis=1)\n    df['BsmtCond'] = df.apply(lambda x: \"NA\" if x['TotalBsmtSF'] == 0 else x['BsmtCond'], axis=1)\n    df['BsmtExposure'] = df.apply(lambda x: \"NA\" if x['TotalBsmtSF'] == 0 else x['BsmtExposure'], axis=1)\n    df['BsmtFinType1'] = df.apply(lambda x: \"NA\" if x['TotalBsmtSF'] == 0 else x['BsmtFinType1'], axis=1)\n    df['BsmtFinType2'] = df.apply(lambda x: \"NA\" if x['TotalBsmtSF'] == 0 else x['BsmtFinType2'], axis=1)\n    #BsmtFullBath\tBsmtHalfBath\n    df['BsmtFullBath'] = df.apply(lambda x: 0 if x['TotalBsmtSF'] == 0 else x['BsmtFullBath'], axis=1)\n    df['BsmtHalfBath'] = df.apply(lambda x: 0 if x['TotalBsmtSF'] == 0 else x['BsmtHalfBath'], axis=1)\n    \n    #BsmtFinSF1\tBsmtFinSF2\tBsmtUnfSF\n    df['BsmtFinSF1'] = df.apply(lambda x: 0 if x['TotalBsmtSF'] == 0 else x['BsmtFinSF1'], axis=1)\n    df['BsmtFinSF2'] = df.apply(lambda x: 0 if x['TotalBsmtSF'] == 0 else x['BsmtFinSF2'], axis=1)\n    df['BsmtUnfSF'] = df.apply(lambda x: 0 if x['TotalBsmtSF'] == 0 else x['BsmtUnfSF'], axis=1)\n    \n    #GarageYrBlt\tGarageFinish GarageQual\tGarageCond GarageCars\tGarageArea\n    #df['GarageType'] = df.GarageType.apply(lambda x: \"NA\" if \"NotAvailable\" == x else x)\n    df['GarageType'] = df.GarageType.fillna(\"NA\")\n    #df['GarageYrBlt'] = df.apply(lambda x: \"NA\" if x['GarageType'] == \"NA\" else x['GarageYrBlt'], axis=1)\n    df['GarageFinish'] = df.apply(lambda x: \"NA\" if x['GarageType'] == \"NA\" else x['GarageFinish'], axis=1)\n    \n    df['GarageQual'] = df.apply(lambda x: \"NA\" if x['GarageType'] == \"NA\" else x['GarageQual'], axis=1)\n    df['GarageCond'] = df.apply(lambda x: \"NA\" if x['GarageType'] == \"NA\" else x['GarageCond'], axis=1)\n    \n    df['GarageCars'] = df.apply(lambda x: 0 if x['GarageType'] == \"NA\" else x['GarageCars'], axis=1)\n    df['GarageArea'] = df.apply(lambda x: 0 if x['GarageType'] == \"NA\" else x['GarageArea'], axis=1)\n    \n    #drop obsolete columns\n    df = df.drop(['Id','GarageYrBlt'], axis=1)\n    df['SalePrice'] = df.SalePrice.fillna(0)\n    return df\n\ndf_stage_1 = cleanStage1(df_before_clean.copy())\ndf_stage_1.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9b61767cb21897686a70ec1b189f52d84b60d26"
      },
      "cell_type": "markdown",
      "source": "##### We had map when TotalBsmtSF =0 , there are couple of entries when it is TotalBsmtSF != 0. We may have to other Bsmt- attributes to impute the values for it but we will do such analysis after the straight forward missing value imputation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ca9707b06ea25db7234ed75abda018ff57e93ef"
      },
      "cell_type": "code",
      "source": "df_stage_1[df_stage_1.BsmtQual.isna()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f8610227232a76f9017ad2b20e4a37c4ee30208b"
      },
      "cell_type": "markdown",
      "source": "##### We have now 27 columns to look after for the first round of cleaning."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46eec8a9c36947b9c636f8d587329e4d0c01878f"
      },
      "cell_type": "code",
      "source": "len(df_stage_1.columns[df_stage_1.isna().any()].tolist()), df_stage_1.columns[df_stage_1.isna().any()].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bf8f785e6aae4e376b74aec040c78b70ef11b23"
      },
      "cell_type": "code",
      "source": "def plotNAs(df):\n    nan_columns = df.columns[df.isna().any()].tolist()\n    #nan_columns.remove('SalePrice')\n    for c in df.fillna('NotAvailable')[nan_columns]:\n        df[[c,'SalePrice']].fillna('NotAvailable').\\\n        groupby(by=c).agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+c)\n        plt.xticks(rotation=45)\n        plt.show()\nplotNAs(df_stage_1.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "195f2fc30462689c693559b6a30636ea03925b02"
      },
      "cell_type": "markdown",
      "source": "### 1. Alley"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a9bfbf210d58ff43ab6f7f65639e4521c0af1a4"
      },
      "cell_type": "code",
      "source": "def outerLandScape(df_temp):\n    df = df_temp.copy()\n    print(\"Initial Shape : \", df.shape)\n    \n    beyond_house = ['Neighborhood','Street', 'PavedDrive', 'Alley']\n    df_temp = df.groupby(beyond_house).agg('count').reset_index()[['Neighborhood','Street', 'PavedDrive', 'Alley',\"SalePrice\"]]\n    dd()\n    \n    def getAlley(Street, Neighborhood, PavedDrive):\n        try:\n            alley = df_temp[\n                (df_temp['Street'] == Street ) &\n                (df_temp['Neighborhood'] == Neighborhood ) &\n                (df_temp['PavedDrive'] == PavedDrive) \n            ]['Alley'].tolist()[0]\n        except:\n            alley = 'NA'\n        \n        return alley\n        \n    \n    df['Alley'] = df.Alley.fillna(\"NA\")\n    df[['Alley','SalePrice']].fillna(0).\\\n        groupby(by='Alley').agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+'Alley')\n    plt.xticks(rotation=45)\n    plt.show()\n    \n    na_alley_count = df[df.Alley == \"NA\"].shape[0]\n    gr_alley_count = df[df.Alley == \"Grvl\"].shape[0]\n    pa_alley_count = df[df.Alley == \"Pave\"].shape[0]\n    \n        \n    df['Alley'] = df.apply( lambda x: getAlley (x['Street']\n                                                ,x['Neighborhood']\n                                                ,x['PavedDrive']\n                                               ) if x['Alley'] == \"NA\" else x['Alley']\n        ,axis=1)\n    \n    \n    df[['Alley','SalePrice']].fillna(0).\\\n        groupby(by='Alley').agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+'Alley')\n    plt.xticks(rotation=45)\n    plt.show()\n    \n    \n    print(\"Alley Snapshot Before : NA -\", na_alley_count,\"Grvl - \", gr_alley_count, \"Pave - \", pa_alley_count)\n    print(\"Alley Snapshot After  : NA -\", df[df.Alley == \"NA\"].shape[0],\n          \"Grvl - \", df[df.Alley == \"Grvl\"].shape[0], \"Pave - \", df[df.Alley == \"Pave\"].shape[0]\n         )\n    \n    return df\n\ndf_alley = outerLandScape(df_stage_1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8e683871512fd91ba2a11afeda76bcb365ab95f"
      },
      "cell_type": "code",
      "source": "def outerLandScape2():\n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    \n    df = pd.concat([df_train, df_test], axis=0,sort=True,ignore_index=True)\n    print(\"Initial Shape : \", df.shape)\n    \n    '''beyond_house = [ 'MSZoning','Street', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood',\n                      'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'PavedDrive','SaleCondition','Fence', 'Alley']\n    df_temp = df[beyond_house].copy()\n    for c in beyond_house:\n        df_temp = df_temp[df_temp[c].notna()]#.reset_index()\n        print(c, df_temp.shape)'''\n    #display.display(df_temp)\n    \n    #beyond_house = ['Neighborhood','Street', 'PavedDrive', 'Alley']\n    #df = df.fillna('NNN')\n    beyond_house = ['Neighborhood','Street','Alley']\n    #beyond_house = ['Street', 'Alley']\n    \n    #display.display(df.groupby(beyond_house).agg('count').reset_index()[['Neighborhood','Street', 'PavedDrive', 'Alley','Id']])\n    display.display(df.groupby(beyond_house).agg('count').reset_index()[['Neighborhood','Street', 'Alley','Id']])\n    \n    \nouterLandScape2()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "413785dd0f436284aa24422d59302764ab1f8392"
      },
      "cell_type": "markdown",
      "source": "##### We can have the above table mapping missing Alley variable values. I am here assuming that Street and Alley would intersect. Therefore, there is a pattern with respect to location regarding the type of material used.\n\n* However, the variable itself might not have such significance to target variable and hence can we can drop the imputation to it. The model which uses this variable would give lower significance level during training."
    },
    {
      "metadata": {
        "_uuid": "fa0e8728b8a135cd1881c8ff3f02b0cd7b556381"
      },
      "cell_type": "markdown",
      "source": "### 2. Electrical"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac4f02ceb71fbedd5049583dddcbed4deb920a55"
      },
      "cell_type": "code",
      "source": "def electricalWrangling(df_temp):\n    df = df_temp.copy()\n    df1 = df_temp.copy()\n    print(df[df.Electrical.isna()].shape)\n    df['Electrical'] = df.Electrical.fillna('NA')\n    display.display(df[['Neighborhood','Electrical', 'SalePrice']].groupby(['Neighborhood','Electrical']).agg('count'))\n    \n    df1['Electrical'] = df1.Electrical.fillna('SBrkr')\n    display.display(df1[['Neighborhood','Electrical', 'SalePrice']].groupby(['Neighborhood','Electrical']).agg('count'))\n    return df1\n    \ndf_electrical = electricalWrangling(df_stage_1.drop('Alley',axis=1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dc2535506898ebb499932dc3cda370331decd835"
      },
      "cell_type": "markdown",
      "source": "##### At Timber, most of them have SBrkr as electrical system. So, it is a safe bet to have the missing entry replaced with 'SBrkr'"
    },
    {
      "metadata": {
        "_uuid": "0b279b86709d190a8f71211c6c8803e0fdbdb722"
      },
      "cell_type": "markdown",
      "source": "### 3. Exterior1st & Exterior2nd"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f3415f7df07a92d7cadd3b06e3eff9a624fab63"
      },
      "cell_type": "code",
      "source": "def exteriorWrangle(df_temp):\n    df = df_temp.copy()\n    dd(df.shape)\n    dd(df[df.Exterior1st == df.Exterior2nd].shape)\n    dd(df[df.Exterior1st.isna()][['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond', 'MSSubClass']])\n    dd(df[df.Exterior2nd.isna()][['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond','MSSubClass']])\n    dt = df.\\\n       groupby(['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond', 'MSSubClass']).\\\n       agg('count').reset_index().\\\n       sort_values(by=['MSSubClass','SalePrice'],ascending=False)[['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond','MSSubClass','SalePrice']]\n    #dd(dt)\n    dd(dt[(dt.MSSubClass == '30') & (dt.Neighborhood == 'Edwards')].head())\n    \n    def bestExt1(Neighborhood,ExterQual, ExterCond, MSSubClass):\n        return dt[(dt.Neighborhood == Neighborhood)&\n                  (dt.ExterQual == ExterQual)&\n                  (dt.ExterCond ==  ExterCond)&\n                  (dt.MSSubClass == MSSubClass)]['Exterior1st'].tolist()[0]\n    \n    def bestExt2(Neighborhood,ExterQual, ExterCond, MSSubClass):\n        return dt[(dt.Neighborhood == Neighborhood)&\n                  (dt.ExterQual == ExterQual)&\n                  (dt.ExterCond ==  ExterCond)&\n                  (dt.MSSubClass == MSSubClass)]['Exterior2nd'].tolist()[0]\n        \n    df['Exterior1st'] = df.Exterior1st.fillna('NA')\n    df['Exterior2nd'] = df.Exterior2nd.fillna('NA')\n    \n    df['Exterior1st'] = df.apply(lambda x: bestExt1(\n                                    x['Neighborhood'], x['ExterQual'], x['ExterCond'], x['MSSubClass']\n                                    ) if x['Exterior1st'] ==\"NA\" else x['Exterior1st'], axis=1)\n    df['Exterior2nd'] = df.apply(lambda x: bestExt2(\n                                    x['Neighborhood'], x['ExterQual'], x['ExterCond'], x['MSSubClass']\n                                    ) if x['Exterior2nd'] ==\"NA\" else x['Exterior2nd'], axis=1)\n    \n    dd(df[df.index == 2151][['Exterior1st','Exterior2nd', 'Neighborhood','ExterQual', 'ExterCond', 'MSSubClass']])\n    return df\n    \n\n    \ndf_ext = exteriorWrangle(df_electrical)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "430d3c5c1ba416f2ff4f3abcf13025b91574f5cc"
      },
      "cell_type": "markdown",
      "source": "##### it is observed that many of the houses have Exterior1st and Exterior2nd same values per neighborhood. Therefore, we will create a matrix of neighborhood and Exterior1st. We will first populate Exterior1st from neighborhood value and then we will populate Exterior2nd from Exterior1st."
    },
    {
      "metadata": {
        "_uuid": "978c99b238daf55dcfb1dcc3d62435bf563fbbfa"
      },
      "cell_type": "markdown",
      "source": "### 4. Fence"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a3b8ea3a1f5dfef6073bb6ec1aa0d3581f97cdb"
      },
      "cell_type": "code",
      "source": "def fenceWrangling(df_temp):\n    df = df_temp.copy()\n\n    print(\"Count of missing Fence : \",df[df.Fence.isna()].shape)\n    \n    df['Fence'] = df.Fence.fillna('NA')\n    \n    #dd(df.groupby(['MSSubClass', 'Neighborhood','Fence', ]).agg('count').reset_index()[['MSSubClass', 'Neighborhood','Fence', 'SalePrice']])\n    dd(df.groupby(['Neighborhood','Fence', ]).agg('count').reset_index()[['Neighborhood','Fence', 'SalePrice']])\n    return df\n    \ndf_fence = fenceWrangling(df_ext)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "730cbc18cfd3208b2c6c0e2d5b287e61c1be4836"
      },
      "cell_type": "markdown",
      "source": "##### It is too tedious to decide if missing value indicates \"NA\" - No Fence or the entry was missing. What does actually Fence depend on ?\n* For now i m going to rely on NA for the missing entry. Safest assumption."
    },
    {
      "metadata": {
        "_uuid": "be573f4333b1a21672960e42d56b45061b3e12e7"
      },
      "cell_type": "markdown",
      "source": "##### It is too risky to map from othe rvariables. Though it seems like it depends on LotArea or LotFrontage. It is not very clear if it depends solely on one of the variable or sort of combination of others. Let us keep it NA for missing values, so that it would mean no fence available."
    },
    {
      "metadata": {
        "_uuid": "1bb19a2f274f3982fc6b46d4b3f8790f4deeb3f7"
      },
      "cell_type": "markdown",
      "source": "### 5.0 Functional"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61976ce4697cd3c7daa84bdd05cba25867ef3288"
      },
      "cell_type": "code",
      "source": "def functionalWrangle(df_temp):\n    df = df_temp.copy()\n   \n    display.display(df[df.Functional.isna()].shape)\n    \n    co_qu_columns = [c for c in list(df) if (\"Co\" in c) or (\"Qu\" in c)]\n    co_qu_columns.append('SalePrice')\n    co_qu_columns.append('Functional')\n    print(co_qu_columns)\n    \n    display.display(df[df.Functional.isna()])\n    display.display(df[df.Functional.isna()][co_qu_columns])\n    \n    dd(df[(df.OverallCond == 5) & (df.OverallQual == 1)][co_qu_columns])\n    dd(df[(df.OverallCond == 1) & (df.OverallQual == 4)][co_qu_columns])\n    \n    df['Functional'] = df.Functional.fillna('Typ')\n    \n    return df\n    \ndf_funct = functionalWrangle(df_fence)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd3e5826dfeb7f45dab9c5c912e41f5995d202ef"
      },
      "cell_type": "markdown",
      "source": "##### the rule itself says, assume typical unless deductions are warranted. However, there is no entry of salvage in our data set. Though it is not mandatory to have all categorical values has to be there in the dataset but it always raises the question why not that variable ? Can *Cond and *Qu variable give us hint of not 'Sal' ?\n\n* I am actually tempted to put 'Sal' but due to lack of samples for Sal, I will be putting it as 'Typ'"
    },
    {
      "metadata": {
        "_uuid": "9efcd37b79e28599c6f8fb0f8ceb9df55f72b7fd"
      },
      "cell_type": "markdown",
      "source": "### 6. LotFrontage"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c26b9467cc19772ed497c7829ee23cd1ea7bda8f"
      },
      "cell_type": "code",
      "source": "def LotFrontagecheck(df_temp):\n    df = df_temp.copy()\n    #df = pd.read_csv('train.csv')\n    print(df[df.LotFrontage.isna()].shape)\n    df_LotFrontage = df[['Neighborhood','LotFrontage']].groupby('Neighborhood').agg(lambda x:x.value_counts().index[0]).reset_index()\n    df_dict = dict([tuple(x) for x in df_LotFrontage.values])\n    print(df_dict)\n    \n    df['LotFrontage'] =df.LotFrontage.fillna(-1)\n    df['LotFrontage'] =df.apply(lambda x: df_dict[x['Neighborhood']] if x['LotFrontage'] == -1 else x['LotFrontage'],axis=1)\n    print(df[df.LotFrontage.isna()].shape)\n    \n    '''df.plot.scatter('LotFrontage', 'SalePrice')\n    df.plot.scatter('LotArea', 'SalePrice')\n    df.plot.scatter('LotArea', 'LotFrontage')\n    \n    display.display(df[df.LotFrontage > 175])'''\n    return df\n    \ndf_lot = LotFrontagecheck(df_funct) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd967d023b7a776f2758d74275ce7343ac26271f"
      },
      "cell_type": "markdown",
      "source": "##### LotFrontage: taking neighborhood as reference most occuring distance is used for filling missing values. Inspiration: neighboring house have same distance to road /gate.\n\n* Lot area > 10000 & LotFrontage > 200  seems like outliers"
    },
    {
      "metadata": {
        "_uuid": "171d354f1715e98a01aa1bee87b36d7052608b9c"
      },
      "cell_type": "markdown",
      "source": "### 7. MSZoning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eecd29007907d674605071ecee152c12326ca47"
      },
      "cell_type": "code",
      "source": "def msZoningWrangle(df_temp):\n    df = df_temp.copy()\n    \n    zone_related = ['LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n                    'Condition2', 'BldgType', 'OverallQual', 'OverallCond', 'MSZoning'\n                   ]\n    \n    temp  = df.groupby(['Neighborhood','MSSubClass','MSZoning']).\\\n    agg('count').reset_index().\\\n    sort_values(by=['SalePrice'],ascending=False)[['Neighborhood','MSSubClass','MSZoning','SalePrice']]\n    dd(temp[(temp.Neighborhood == 'IDOTRR')  |(temp.Neighborhood == 'Mitchel')])\n    \n    def returnmsZone(Neighborhood,MSSubClass):\n        return temp[(temp.Neighborhood == Neighborhood) &\n                   (temp.MSSubClass == MSSubClass)]['MSZoning'].tolist()[0]\n    \n    dd(df[df.MSZoning.isna()][['Neighborhood','MSSubClass','MSZoning']])\n    df['MSZoning'] = df.MSZoning.fillna(\"NA\")\n    df['MSZoning'] = df.apply(lambda x: returnmsZone(x['Neighborhood'], x['MSSubClass']) \n                              if x['MSZoning'] == \"NA\" else x['MSZoning'], axis=1)\n    \n    dd(df[df.index.isin([1915,2216,2250,2904])][['Neighborhood','MSSubClass','MSZoning']])\n\n    return df\n\ndf_ms = msZoningWrangle(df_lot)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "279247013392f2109f7b362cae054707a1e8520c"
      },
      "cell_type": "markdown",
      "source": "##### MSZoning is general zoning classification. Therefore,  it must be specific to an area and hence 'Neighborhood' is the variable to our rescue. MSZoning = RL  when neighbor is 'Mitchel' and  RM when neighbor is  IDOTRR and they are is missing."
    },
    {
      "metadata": {
        "_uuid": "d61a447937627d4dbc6778f2ff890e359c29a7c9"
      },
      "cell_type": "markdown",
      "source": "### 8.0 MasVnrType & MasVnrArea"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45d033892a94cf4e22daffdea5521148899a89be"
      },
      "cell_type": "code",
      "source": "def MasVnrTypeCheck(df_temp):\n    #df = pd.read_csv('train.csv')\n    df=df_temp.copy()\n    print(df[df.MasVnrType.isna()].shape, )\n    #display.display(df[['Neighborhood','MasVnrType','Id']].groupby(['Neighborhood','MasVnrType']).agg('count').reset_index())\n    \n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrType.isna()].head())\n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrArea.isna()].head())\n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrArea == 0].head())\n    dd(df[['MasVnrType','MasVnrArea']][df.MasVnrType == 'None'].head())\n\n    df['MasVnrType'] = df.MasVnrType.fillna(\"None\")\n    df['MasVnrArea'] = df.MasVnrArea.fillna(0)\n\n    df['MasVnrArea'] = df['MasVnrArea'].astype(np.float64)\n\n    return df\ndf_mas = MasVnrTypeCheck(df_ms)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ce92aaf2df956a7379b59490823b03e0c06d5cc"
      },
      "cell_type": "markdown",
      "source": "* MasVnrArea nan count is equivalent to MasVnrType count.\n* MasVnrArea == 0 is already present \n* whenever MasVnrArea == 0 MasVnrType is also None \n* Therefore, MasVnrArea will be mapped to zero when MasVnrType = None\n\n##### Outlier: area > 1400 is only one sample which has low sale price. Its overall condition and quality is moderate and there are enough sample for those bands."
    },
    {
      "metadata": {
        "_uuid": "0da58a3a3cb01966220c12cdf8ecfa6348a6720f"
      },
      "cell_type": "markdown",
      "source": "### 9. MiscFeature"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0ae3cb5602c945d48beb29b3a5f20688b596fee"
      },
      "cell_type": "code",
      "source": "def miscFeatureWrangle(df_temp):\n    df = df_temp.copy()\n    df_df = df.fillna('NA').groupby(['MiscFeature','MiscVal']).agg('count').reset_index()[['MiscFeature','MiscVal','SalePrice']]\n    \n    dd(\"Before :\",df_df[df_df.MiscVal == 0])\n    dd(df_df[df_df.MiscFeature == \"NA\"])\n    #dd(df_df[df_df.MiscVal == \"NA\"])\n    \n    df['MiscFeature'] = df.apply(lambda x: \"NA\" if x['MiscVal'] == 0 else x['MiscFeature'],axis=1)\n    df['MiscFeature'] = df.apply(lambda x: \"Gar2\" if x['MiscVal'] == 17000.0 else x['MiscFeature'],axis=1)\n    \n    df_df = df.fillna('NA').groupby(['MiscFeature','MiscVal']).agg('count').reset_index()[['MiscFeature','MiscVal','SalePrice']]\n    \n    dd(\"After :\",df_df[df_df.MiscVal == 0])\n    dd(df_df[df_df.MiscFeature == \"NA\"])\n    return df\n    \ndf_misc = miscFeatureWrangle(df_mas)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51deb1480b03d1e420e7d7823a562750bf424fd2"
      },
      "cell_type": "markdown",
      "source": "* When MiscVal == 0 ; MiscFeature is mostly NA (None). Note that it can be Shed or Other too. Will park it for next level fine tuning.\n* High Values are dedicated to 'Gar2'. Therefore, testing set missing value is gar2 for sure."
    },
    {
      "metadata": {
        "_uuid": "8980852de2a7528231e9ff46005d05c850df9c12"
      },
      "cell_type": "markdown",
      "source": "### 10. PoolQC"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b59cdac91e5bd9224c30c061d4be36a961c85254"
      },
      "cell_type": "code",
      "source": "def poolWrangling(df_temp):\n    df=df_temp.copy()\n    #df = pd.read_csv('train.csv')\n    print(\"Count of missing PoolQC : \",df[df.PoolQC.isna()].shape)\n    \n    dd(df[df.PoolQC.isna()][['PoolArea', 'PoolQC','OverallCond','OverallQual']].head())\n    #dd(df[df.PoolQC.isna()][['PoolArea', 'PoolQC','OverallCond','OverallQual']])\n    dd(df[df.PoolArea == 0][['PoolArea', 'PoolQC','OverallCond','OverallQual']]['PoolQC'].unique())\n    dd(df[df.PoolArea.isna()])\n    \n    dd(df.groupby(['OverallCond','OverallQual','PoolQC']).agg('count').reset_index()[['OverallCond','OverallQual','PoolQC','SalePrice']])\n    df['PoolQC'] = df.PoolQC.fillna(\"NA\")\n    df['PoolQC'] = df.apply(lambda x: \"Fa\" if (x['PoolArea'] > 0) & (x['PoolQC'] == \"NA\") else x['PoolQC'],axis=1)\n    #dd(df.fillna(\"NA\").groupby(['PoolArea','PoolQC']).agg('count').reset_index()[['PoolArea','PoolQC','SalePrice']])\n    dd(df[df.PoolQC.isna()][['PoolArea', 'PoolQC','OverallCond','OverallQual']])\n    dd(Counter(df.PoolQC))\n    \n    return df\n    \ndf_pool = poolWrangling(df_misc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "172e20e2b8b7baa640b0c1a1a6271c1d0f8f2af0"
      },
      "cell_type": "markdown",
      "source": "##### When PoolArea is 0 PoolQC will be NA (no pool). When PoolArea > 0 it appears to be good candidates for \"Fa\"."
    },
    {
      "metadata": {
        "_uuid": "5f976fa4ac24e012934e6dff019d317325f55833"
      },
      "cell_type": "markdown",
      "source": "* when PoolQC should be \"NA\" when PoolArea = 0 [Thumb rule / Common sense]\n* Missing values have characterstics matching with that of \"Fair\" condition. It may be \"TA\" but we dont have enough evidence or rather no evidence for that."
    },
    {
      "metadata": {
        "_uuid": "8537fd4e4d98b0a4619eb86938947fd087d6a7b4"
      },
      "cell_type": "markdown",
      "source": "### 11. SaleType"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38bbafafa5d0c15aa4051c7078fa30173ebdb75d"
      },
      "cell_type": "code",
      "source": "def saleTypeWrangling(df):\n\n    dd(df[df.SaleType.isna()][['MSSubClass', 'MSZoning', 'SaleCondition','SaleType']])\n    df1 = df.groupby(['MSSubClass', 'MSZoning', 'SaleCondition','SaleType']).\\\n    agg('count').reset_index().sort_values('SalePrice',ascending=False)[['MSSubClass', 'MSZoning', 'SaleCondition','SaleType', 'SalePrice']]\n    dd(df1[df1.MSSubClass == '20'])\n    \n    def popSaleType(MSSubClass, MSZoning, SaleCondition):\n        return df1[(df1.MSSubClass == MSSubClass) &\n                   (df1.MSZoning == MSZoning) &\n                   (df1.SaleCondition == SaleCondition)\n                  ]['SaleType'].tolist()[0]\n    \n    df['SaleType'] = df.SaleType.fillna(\"NA\")\n    df['SaleType'] = df.apply(lambda x: popSaleType(x['MSSubClass'], x['MSZoning'], x['SaleCondition']) \n                              if x['SaleType'] == \"NA\" else x['SaleType']\n                              ,\n                             axis = 1)\n    dd(df[df.index == 2489][['MSSubClass', 'MSZoning', 'SaleCondition','SaleType']])\n    \n    return df\n    \ndf_sale = saleTypeWrangling(df_pool.copy())    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e0b115ed3874bf8919048cfc674be8758d3433b"
      },
      "cell_type": "markdown",
      "source": "### 12. Utilities"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ecef306f895d6d7ee0a3f42a41792fd5d943a3c"
      },
      "cell_type": "code",
      "source": "def utilWrangling(df):\n    dd(df[df.Utilities.isna()][['Neighborhood','Utilities']])\n    \n    df1 = df.groupby(['Neighborhood','Utilities']).agg('count').reset_index().\\\n    sort_values('SalePrice',ascending=False)[['Neighborhood','Utilities','SalePrice']]\n    dd(df1)\n    \n    def returnUtil(Neighborhood):\n        return df1[(df1.Neighborhood == Neighborhood)][\"Utilities\"].tolist()[0]\n    \n    df['Utilities'] = df.Utilities.fillna(\"NA\")\n    df['Utilities'] = df.apply(lambda x: returnUtil(x['Neighborhood']) if x['Utilities'] == \"NA\" else x[\"Utilities\"],axis=1)\n    \n    dd(df[df.index.isin([1915,1945])][['Neighborhood','Utilities']])\n    return df\n        \ndf_util = utilWrangling(df_sale.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9aaa68406e5357925e5feeef6a96b25721a19a34"
      },
      "cell_type": "markdown",
      "source": "### 13. KitchenQual"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6f4a73a24abc1d41e649f77b4b1271440f67371"
      },
      "cell_type": "code",
      "source": "def kitchenQual(df):\n    \n    dd(df[df.KitchenQual.isna()][['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']])\n    \n    df1 = df.groupby(['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']).agg('count').\\\n       reset_index()[['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']]\n    \n    def returnkqual(OverallCond, OverallQual, KitchenAbvGr):\n        return df1[(df1.OverallCond == OverallCond)&\n                  (df1.OverallQual == OverallQual) &\n                   (df1.KitchenAbvGr == KitchenAbvGr)\n                  ]['KitchenQual'].tolist()[0]\n    \n    df['KitchenQual'] = df.KitchenQual.fillna(\"NA\")\n    df['KitchenQual'] = df.apply(lambda x: returnkqual(x['OverallCond'], x['OverallQual'],x['KitchenAbvGr'])\n                                 if x['KitchenQual'] == \"NA\" else x['KitchenQual']\n                                ,axis=1)\n    dd(df[df.index == 1555][['OverallCond', 'OverallQual', 'KitchenAbvGr', 'KitchenQual']])\n    return df\n\ndf_kitchen = kitchenQual(df_util.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3dd8863db77b0edd58cc8c049b116f711341989b"
      },
      "cell_type": "markdown",
      "source": "### Checkpoint - 1"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b1ffb75eada2a615de32fbfd9eb625b9cb9e58d"
      },
      "cell_type": "code",
      "source": "plotNAs(df_kitchen.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b005167f421464346e3d2a571ce8d30690d5222"
      },
      "cell_type": "code",
      "source": "def bsmtInterpolate(df):\n    bsmt_column = ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']\n    print(bsmt_column)\n    \n    bsmt_column2 = [c for c in list(df) if 'Bsmt' in c]\n    \n    dd(df[df.BsmtCond.isna()][bsmt_column])\n    bsmt_index = []\n    bsmt_index += list(df[df.BsmtCond.isna()].index)\n    \n    dd(df[df.BsmtExposure.isna()][bsmt_column])\n    bsmt_index += list(df[df.BsmtExposure.isna()].index)\n    \n    dd(df[df.BsmtFinType2.isna()][bsmt_column])\n    bsmt_index += list(df[df.BsmtFinType2.isna()].index)\n    \n    dd(df[df.BsmtQual.isna()][bsmt_column])\n    bsmt_index += list(df[df.BsmtQual.isna()].index)\n    \n    df1 = df.groupby(bsmt_column).agg('count').reset_index()[(bsmt_column + ['SalePrice'])]\n    df1 = df1.sort_values('SalePrice',ascending=False)\n    #dd(df1)\n    \n    def getBsmtCond(BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtCond'].tolist()[0]\n    \n    def getBsmtExposure(BsmtCond, BsmtFinType1, BsmtFinType2, BsmtQual):\n        return df1[(df1.BsmtCond == BsmtCond)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtExposure'].tolist()[0]\n    \n    def getBsmtFinType1(BsmtExposure, BsmtCond, BsmtFinType2, BsmtQual):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtCond == BsmtCond)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtFinType1'].tolist()[0]\n    \n    def getBsmtFinType2(BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtCond == BsmtCond)&\n                   (df1.BsmtQual == BsmtQual)                   \n                  ]['BsmtFinType2'].tolist()[0]\n    \n    def getBsmtQual(BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtCond):\n        return df1[(df1.BsmtExposure == BsmtExposure)&\n                   (df1.BsmtFinType1 == BsmtFinType1)&\n                   (df1.BsmtFinType2 == BsmtFinType2)&\n                   (df1.BsmtCond == BsmtCond)                   \n                  ]['BsmtQual'].tolist()[0]\n    \n    #['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']\n    df['BsmtCond'] = df.BsmtCond.fillna(\"NA\")\n    df['BsmtCond'] = df.apply(lambda x: getBsmtCond(x['BsmtExposure'], x['BsmtFinType1'], x['BsmtFinType2'], x['BsmtQual'])\n                              if x['BsmtCond'] == \"NA\" else x['BsmtCond']\n                             , axis = 1)\n    \n    df['BsmtExposure'] = df.BsmtExposure.fillna(\"NA\")\n    df['BsmtExposure'] = df.apply(lambda x: getBsmtExposure(x['BsmtCond'], x['BsmtFinType1'], x['BsmtFinType2'], x['BsmtQual'])\n                              if x['BsmtExposure'] == \"NA\" else x['BsmtExposure']\n                             , axis = 1)\n    \n    df['BsmtFinType1'] = df.BsmtFinType1.fillna(\"NA\")\n    df['BsmtFinType1'] = df.apply(lambda x: getBsmtFinType1(x['BsmtExposure'], x['BsmtCond'], x['BsmtFinType2'], x['BsmtQual'])\n                              if x['BsmtFinType1'] == \"NA\" else x['BsmtFinType1']\n                             , axis = 1)\n\n    df['BsmtFinType2'] = df.BsmtFinType2.fillna(\"NA\")\n    df['BsmtFinType2'] = df.apply(lambda x: getBsmtFinType2(x['BsmtExposure'], x['BsmtFinType1'], x['BsmtCond'], x['BsmtQual'])\n                              if x['BsmtFinType2'] == \"NA\" else x['BsmtFinType2']\n                             , axis = 1)\n    \n    df['BsmtQual'] = df.BsmtQual.fillna(\"NA\")\n    df['BsmtQual'] = df.apply(lambda x: getBsmtQual(x['BsmtExposure'], x['BsmtFinType1'], x['BsmtFinType2'], x['BsmtCond'])\n                              if x['BsmtQual'] == \"NA\" else x['BsmtQual']\n                             , axis = 1)\n    \n    dd(df[df.index.isin(bsmt_index)][bsmt_column])\n    \n    return df\ndf_bsmt_final = bsmtInterpolate(df_kitchen.copy())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13713fa809f4a16512052a073d0aac6e3ae3192b"
      },
      "cell_type": "markdown",
      "source": "### Garage*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63d136a9701f81b0265ab0f6943a3c5c1f799a7a"
      },
      "cell_type": "code",
      "source": "def garageWrangling(df):\n    garage_columns = [c for c in df.select_dtypes(exclude=np.number) if \"Garage\" in c]\n    print(garage_columns)\n    garage_col2 = [c for c in list(df) if \"Garage\" in c]\n    grp_by = ['GarageType','MiscFeature','OverallQual','OverallCond','GarageFinish','GarageQual','GarageCond',\\\n              'GarageArea','GarageCars']\n    \n    new_col = list(set(garage_col2 + grp_by + ['MiscFeature']))\n    \n    miss_index = []\n    for c in garage_col2:\n        df_temp = df[df[c].isna()][new_col]\n        dd(c,df_temp)\n        miss_index += list(df_temp.index)\n    \n    \n    df1 = df.groupby(grp_by).agg('count').reset_index().sort_values('SalePrice',ascending=False)[grp_by + ['SalePrice']]\n    dd(df1[df1.GarageType == 'Detchd'])\n    \n    def getGarageArea( GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageArea'].tolist()[0]\n    \n    def getGarageCars( GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageCars'].tolist()[0]\n    \n    def getGarageCond(GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageCond'].tolist()[0]\n    \n    def getGarageFinish(GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageFinish'].tolist()[0]\n    \n    def getGarageQual(GarageType, MiscFeature, OverallQual, OverallCond):\n        return df1[\n                   (df1.GarageType == GarageType) &\n                   (df1.MiscFeature == MiscFeature) &\n                   (df1.OverallQual == OverallQual) &\n                   (df1.OverallCond == OverallCond)\n                  ]['GarageQual'].tolist()[0]\n    \n    df['GarageArea'] = df['GarageArea'].fillna(\"NA\")\n    df['GarageArea'] = df.apply(lambda x: getGarageArea( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageArea'] ==\"NA\" else x['GarageArea']\n                                ,axis =1)\n    \n    df['GarageCars'] = df['GarageCars'].fillna(\"NA\")\n    df['GarageCars'] = df.apply(lambda x: getGarageCars( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageCars'] ==\"NA\" else x['GarageCars']\n                                ,axis =1)\n    \n    df['GarageCond'] = df['GarageCond'].fillna(\"NA\")\n    df['GarageCond'] = df.apply(lambda x: getGarageCond( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageCond'] ==\"NA\" else x['GarageCond']\n                                ,axis =1)\n    \n    df['GarageFinish'] = df['GarageFinish'].fillna(\"NA\")\n    df['GarageFinish'] = df.apply(lambda x: getGarageFinish( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageFinish'] ==\"NA\" else x['GarageFinish']\n                                ,axis =1)\n    \n    df['GarageQual'] = df['GarageQual'].fillna(\"NA\")\n    df['GarageQual'] = df.apply(lambda x: getGarageQual( x['GarageType'], x['MiscFeature'], x['OverallQual'], x['OverallCond'])\n                                if x['GarageQual'] ==\"NA\" else x['GarageQual']\n                                ,axis =1)\n    \n    dd(df[df.index.isin(miss_index)][new_col])\n    return df\ndf_garage_final = garageWrangling(df_bsmt_final.copy())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19f2a5108c043fc0cccee18640efa1796d4e9000"
      },
      "cell_type": "markdown",
      "source": "### Checkpoint 2"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85bc6e9c6dd857613a47454812d53723b6f864b9"
      },
      "cell_type": "code",
      "source": "plotNAs(df_garage_final.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6a8e936951200b769135225fdc446b2f721fb04"
      },
      "cell_type": "code",
      "source": "did_we_miss_them = ['MSSubClass', 'OverallQual', 'OverallCond', 'FireplaceQu', 'MoSold']\nfor c in did_we_miss_them:\n    dd(c, df_garage_final[c].unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8815a4053c5e543a182d20944f3313372059b81c"
      },
      "cell_type": "code",
      "source": "def FireplaceWrangling(df_temp):\n    df = pd.read_csv(train_File)\n    print(\"Count of missing FireplaceQu : \",df[df.FireplaceQu.isna()].shape[0])\n    dd(\"Count of Fireplaces == 0\",df[['Fireplaces', 'FireplaceQu']][df.Fireplaces == 0].shape)\n    dd(df[['Fireplaces', 'FireplaceQu']][df.FireplaceQu.isna()].head())\n    dd(\"When FireplaceQu == NA, Fireplaces:\", df[['Fireplaces', 'FireplaceQu']][df.FireplaceQu.isna()]['Fireplaces'].unique())\n    dd(\"When Fireplaces == 0, FireplaceQu:\",df[['Fireplaces', 'FireplaceQu']][df.Fireplaces == 0]['FireplaceQu'].unique())\n    \n    #now we have NA as nan in our wrangled dataset; ideally this should not be problem but let us be consistent.\n    df = df_temp.copy()\n    \n    df['FireplaceQu'] = df.apply(lambda x: \"NA\" if x['Fireplaces'] == 0 else x['FireplaceQu'], axis=1 )\n    \n    dd(\"Post imputing :\",df.FireplaceQu.unique())\n    \n    return df\n    \ndf_fire = FireplaceWrangling(df_garage_final.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8401311eadea9f1633c1a94743772e57c94fc27"
      },
      "cell_type": "markdown",
      "source": "##### FireplaceQu will be mapped to NA (no fireplace) whenever Fireplaces = 0"
    },
    {
      "metadata": {
        "_uuid": "07789ba999bac49f46678d2bac9fd670498e740e"
      },
      "cell_type": "markdown",
      "source": "##### TotalBsmtSF = 0 indicates there is no basement. \n* Therefore, BsmtQual = BsmtCond = BsmtExposure = BsmtFinType1 = BsmtFinType2 = \"NA\"; when TotalBsmtSF = 0 \n\n* Outlier: (df.BsmtFinSF1 > 2000) & (df.SalePrice < 200000) (2 of them) are outlier because it not only brings the co relation down but also there are enough samples for outlier's overall condition and quality samples."
    },
    {
      "metadata": {
        "_uuid": "1d794a5a7d4a632ddb32f32ded1284994b11e06d"
      },
      "cell_type": "markdown",
      "source": "##### Let us check the datatype if it is corrupted due to our imputate operation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44c51ce915e9a59ee5e84e7d96342973a54d9340"
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.object))\ncols_need_change = [c for c in list(df_before_clean.select_dtypes(include=np.number)) if c in post_imputing_cols]\n\n#dd(df_before_clean[df_before_clean.MasVnrArea == 'BrkFace'])\n\n'''for c in post_imputing_cols:\n    if c in cols_need_change:\n        print(c)\n        df_fire[c] = df_fire[c].astype(np.float64)'''\nprint()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5c3eea379e76f4dcc48d151964d3ef6661537f5"
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.number))\n[c for c in list(df_before_clean.select_dtypes(include=np.number)) if c not in post_imputing_cols]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e37476763572dd5dd6b69b2232c9f2f5cf02fa32"
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.object))\n[c for c in list(df_before_clean.select_dtypes(include=np.object)) if c not in post_imputing_cols]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c191d8dbbf0c0bf38c32cfbd2371309c4b4430e8"
      },
      "cell_type": "code",
      "source": "post_imputing_cols = list(df_fire.select_dtypes(include=np.number))\n\n#for c in post_imputing_cols:\n_ = [dd(c,df_fire[c].unique()) for c in list(df_fire.select_dtypes(exclude=np.number))]\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c7e8b9e778ff6318aebcb8e06810f3942b9ef41c"
      },
      "cell_type": "markdown",
      "source": "### Outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06fa29b21e977e3c04e775651c4a764bc56fb3e9"
      },
      "cell_type": "code",
      "source": "print(\"Shape of the imputed dataset : \", df_fire.shape)\ndf_out = df_fire[df_fire.SalePrice > 0].copy()\nprint(\"Shape of the Outlier Analysis dataset : \", df_out.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b0b590952c97238bc603c029c3e3956bb97d1f0"
      },
      "cell_type": "code",
      "source": "def computeCorrCols(df):\n    df_corr = df.corr()\n    \n    df1 = df_corr.stack().reset_index().rename(columns={'level_0': \"C1\", \"level_1\": \"C2\", 0 : \"Corr_val\"})\n    \n    df1['Corr_val']= df1['Corr_val'].abs()\n    df1 = df1[df1['Corr_val'] < 1].sort_values('Corr_val',ascending=False)\n    df1 = df1.drop_duplicates('Corr_val').reset_index(drop=True)\n    dd(df1)\n    \n    return df1\ndf_corr = computeCorrCols(df_out.copy())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "_uuid": "dd47cc743a96c14c811dd8994d07ced06d6e754e"
      },
      "cell_type": "markdown",
      "source": "def checkCorr(df_old):\n    df = pd.get_dummies(df_old)\n    corr_target = []\n    df_corr = df.corr()\n    for c in list(df_corr):\n        corr_target.append((c, np.abs(df_corr.loc[c,'SalePrice'])))\n\n    dd(\"Top 15 Numerical Variables with High Corr value for SalePrice :\",\n        [x for x in sorted(corr_target,key=lambda x: x[1], reverse=True) if '_' not in x[0]][1:15])\n    \n    dd(\"Top 15 Variables (All Types) with High Corr value for SalePrice :\",\n        [x for x in sorted(corr_target,key=lambda x: x[1], reverse=True)][1:15])\n\ncheckCorr(df_out.copy())"
    },
    {
      "metadata": {
        "_uuid": "5cdc7898e4f3674a1761b654f1a8c5e3e844cb16"
      },
      "cell_type": "markdown",
      "source": "##### There may be so many outliers but let us target the ones which are not only numerical but also impact the target variables. Let us target top 5 variables for outlier removal. i.e. GrLivArea, GarageCars, GarageArea, TotalBsmtSF & 1stFlrSF"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ccd670ab12050347b0d53488330e09b521155ad1"
      },
      "cell_type": "code",
      "source": "def topCorrWithOthers(df):\n    top_5_corr = ['GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF']\n    print(\"Co relation of the top 5 columns with others : \")\n    for c in top_5_corr:\n        dd(df_corr[(df_corr.C1 == c) | (df_corr.C2 == c)].head())\n        \ntopCorrWithOthers(df_fire.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a784ebbca29b46f1197da9d44a4e648dd09f187"
      },
      "cell_type": "code",
      "source": "df_out['SalePrice'] = df_out['SalePrice'] / df_out.GrLivArea",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a14b476281e0a2c6e7875bf95ff979aa393d2e25"
      },
      "cell_type": "markdown",
      "source": "### 1. GrLivArea"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77ebb719d748783da7503f1750667a793b116b2a"
      },
      "cell_type": "code",
      "source": "def grLivAreaOutlier(df):\n    df.plot.scatter(\"GrLivArea\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['GrLivArea','SalePrice'])\n    df1 = df[(df.GrLivArea < 4000) | (df.SalePrice >250000 )]\n    print(\"Outlier Count: \", df.shape[0]-df1.shape[0])\n    df1.plot.scatter(\"GrLivArea\", \"SalePrice\", title=\"After Removal of Outlier\")\n    \n    #dd(corr(df1.GrLivArea, df1.SalePrice))\n    \n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['GrLivArea','SalePrice'])\n    \n    return df1\ndf_liv = grLivAreaOutlier(df_out.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d90f7f47a71b0aff09176fa5e6521f518f6723eb"
      },
      "cell_type": "markdown",
      "source": "### 2. GarageCars"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "288320ce0d38599cc7d8d5d3c6570de8d6b375e3"
      },
      "cell_type": "code",
      "source": "def garageCarsOutlier(df):\n    df.plot.scatter(\"GarageCars\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['GarageCars','SalePrice'])\n    \n    df1 = df[(df.GarageCars < 4)]\n    \n    print(\"Outlier Count : \", df[(df.GarageCars == 4)].shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['GarageCars','SalePrice'])\n    df1.plot.scatter(\"GarageCars\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    return df1\n    \ndf_gar_car = garageCarsOutlier(df_liv.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c01d37074160fae50d1f4d55cc8c901fc03ea78c"
      },
      "cell_type": "markdown",
      "source": "### 3.GarageArea"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95e7da0b5651e7973745abc1b4f21bae34ee6046"
      },
      "cell_type": "code",
      "source": "def garageAreaOutlier(df):\n    df.plot.scatter(\"GarageArea\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['GarageArea','SalePrice'])\n    \n    df1 = df[(df.GarageArea < 1200) | (df.SalePrice > 300000)]\n    \n    print(\"Outlier Count : \", df[(df.GarageArea > 1200) & (df.SalePrice < 300000)].shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['GarageArea','SalePrice'])\n    df1.plot.scatter(\"GarageArea\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    \n    return df1\n\ndf_gar_area = garageAreaOutlier(df_gar_car.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fbda1a6414891851c2965e72f56926ff34a2bba2"
      },
      "cell_type": "markdown",
      "source": "### 4. TotalBsmtSF"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4643985119095f8e33bbc535a3e3fec0b67617e0"
      },
      "cell_type": "code",
      "source": "def totalBsmtSFOutlier(df):\n    df.plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['TotalBsmtSF','SalePrice'])\n    df[df.TotalBsmtSF >3000].plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Zoomed look on Outlier\")\n    df[df.TotalBsmtSF >3100].plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Further Zoomed look on Outlier\")\n    \n    df1 = df[(df.TotalBsmtSF < 3000) | (df.SalePrice > 300000)]\n    \n    print(\"Outlier Count : \", df[(df.TotalBsmtSF > 3000) & (df.SalePrice < 300000)].shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['TotalBsmtSF','SalePrice'])\n    df1.plot.scatter(\"TotalBsmtSF\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    \n    return df1\n\ndf_tot = totalBsmtSFOutlier(df_gar_area.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36d1aa2dcad576197b28b6f9b6a03c42929e7d98"
      },
      "cell_type": "markdown",
      "source": "### 5. 1stFlrSF"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8080add18719fcc66e4a75f8e63d642c5bbb72f"
      },
      "cell_type": "code",
      "source": "def firststFlrSFOutlierCheck(df):\n    df.plot.scatter(\"1stFlrSF\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    df_corr = df.corr()\n    print(\"Co relation before removing the outlier : \",df_corr.loc['1stFlrSF','SalePrice'])\n    df[df['1stFlrSF'] >2500].plot.scatter(\"1stFlrSF\", \"SalePrice\", title=\"Zoomed look on Outlier\")\n    #df[df.TotalBsmtSF >3100].plot.scatter(\"1stFlrSF\", \"SalePrice\", title=\"Further Zoomed look on Outlier\")\n    \n    df1 = df[(df['1stFlrSF'] < 3000) ]\n    \n    print(\"Outlier Count : \", df[(df['1stFlrSF'] > 3000)].shape[0])\n    df_corr = df1.corr()\n    print(\"Co relation After removing the outlier : \",df_corr.loc['1stFlrSF','SalePrice'])\n    df1.plot.scatter(\"1stFlrSF\", \"SalePrice\", title=\"Before Removal of Outlier\")\n    \n    return df1\n    \ndf_1stFlrSF = firststFlrSFOutlierCheck(df_tot.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "cee46947c9d42be2e314c31122f2bcdaec7943ce"
      },
      "cell_type": "code",
      "source": "#checkCorr(df_tot.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "1ef748b09935cd0b7392af55fc1e98f8476e5bb3"
      },
      "cell_type": "code",
      "source": "#checkCorr(df_1stFlrSF.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd2d1a9027f89d1372bfce4f6ec6aa28a558b1e8"
      },
      "cell_type": "markdown",
      "source": "#### On comparing the results from the co relation from the above 2 cells, it is clear that we [*should not*] go for the 1stFlrSF outlier removal."
    },
    {
      "metadata": {
        "_uuid": "be6014c0fae1fcfd30d07e6ee15c2618e1bf2c00"
      },
      "cell_type": "markdown",
      "source": "#### 2.2 Wrangling\n* should we try to see all the outlier or only the ones which have high co relation with target variable ?\n* have both test and train data for data missing and imputation activities and NOT for outlier removal.\n* is there any way we can identify the outlier in categorical values ?"
    },
    {
      "metadata": {
        "_uuid": "1ca8ef26d661674e80035c955b2b71af8a8b35fe"
      },
      "cell_type": "markdown",
      "source": "def interpolateAtLast(df):\n   \n    df[['Alley','SalePrice']].fillna('NotAvailable').\\\n    groupby(by='Alley').agg('count').\\\n    plot.bar(legend=None, title=\"Frequency Plot for Alley\")\n    plt.xticks(rotation=45)\n    plt.show()\n    df = df.interpolate()\n    \n    df[['Alley','SalePrice']].fillna('NotAvailable').\\\n    groupby(by='Alley').agg('count').\\\n    plot.bar(legend=None, title=\"Frequency Plot for Alley\")\n    plt.xticks(rotation=45)\n    plt.show()\n    \ninterpolateAtLast(df_fire.copy())"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9ae25d1ff8f852025c5b3adcc8c36cd1e14680d"
      },
      "cell_type": "code",
      "source": "def interpolateCheck():\n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    \n    df = pd.concat([df_train, df_test], axis=0,sort=True,ignore_index=True)\n    print(\"Initial Shape : \", df.shape)\n    \n    nan_columns_before = df.columns[df.isna().any()].tolist()\n    \n    bsmt_columns = [c for c in list(df) if 'Bsmt' in c]\n    gar_columns = [c for c in list(df) if 'Garage' in c]\n    \n    df_inter = df.interpolate()\n    nan_columns = df_inter.columns[df_inter.isna().any()].tolist()\n    \n    interpolated_columns = [c for c in nan_columns_before if c not in nan_columns]\n    \n    display_columns = list(set(bsmt_columns + interpolated_columns + gar_columns))\n    \n    print(\"These are interpolated : \", interpolated_columns)\n    \n    for c in interpolated_columns:\n        na_indices = df[df[c].isna()].index.tolist()\n        print(c,na_indices)\n        \n        if 'Bsmt' in c:\n            display.display(df[df.index.isin(na_indices)][bsmt_columns])\n            display.display(df_inter[df_inter.index.isin(na_indices)][bsmt_columns])\n        else: \n            display.display(df[df.index.isin(na_indices)][gar_columns])\n            display.display(df_inter[df_inter.index.isin(na_indices)][gar_columns])\n        #break\n    \n    '''for c in df.fillna('NotAvailable')[nan_columns]:\n        df[[c,'SalePrice']].fillna('NotAvailable').\\\n        groupby(by=c).agg('count').\\\n        plot.bar(legend=None, title=\"Frequency Plot for \"+c)\n        plt.xticks(rotation=45)\n        plt.show()'''\ninterpolateCheck()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "592c64a71d867b87e14666fa410fdf3ec411cd46"
      },
      "cell_type": "markdown",
      "source": "##### We should not blindly depend on the pandas interpolate outcome. I guess we should use the method parameters that interpolate uses and then use it with a understanding. e.g.: ‘krogh’, ‘piecewise_polynomial’, ‘spline’, ‘pchip’ and ‘akima’ methods  in scipy [and thus in panda].\n\n* TotalBsmtSF being null indicate no basement and should not impute values based on neighbor (i guess)\n* basement full and half bathroom being .5 makes no sense.\n* garage year built in decimals does not makes sence."
    },
    {
      "metadata": {
        "_uuid": "2bb2f8ce5f08c97196d6fc64b2e77c7e2c2efd1b"
      },
      "cell_type": "markdown",
      "source": "##### Let us check with XGBoost for the score."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63f50265933e749b984ec5375c746769ae72d5e5"
      },
      "cell_type": "code",
      "source": "def preProcessData(df, log=False):\n    \n    print(\"Shape of the data set before pre processing : \", df.shape )\n\n    #get dummies\n    if log:\n        print(\"Categorical columns : \", list(df.select_dtypes(exclude=np.number)))\n    df = pd.get_dummies(df, dtype=np.float64)\n    \n    print(\"\\n\\nShape of the data set after pre processing : \", df.shape )\n    \n    if log:\n        print(\"Columns in the data set are : \",list(df))\n\n    return df\n\ndf_prep = preProcessData(df_fire.copy())\ndf_prep.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "533356e54ae389ce2b237f00d85d963af6ecfd10"
      },
      "cell_type": "code",
      "source": "def getOutlierIndices(df):\n    \n    print(\"Shape before removing outlier : \", df.shape)\n    '''df = df[(df.GrLivArea < 4000) | (df.SalePrice >250000) | (df.SalePrice == 0) ]\n    df = df[(df.GarageCars < 4) | (df.SalePrice == 0)]\n    df = df[((df.GarageArea < 1200) | (df.SalePrice > 300000)) | (df.SalePrice ==0)]\n    df = df[((df.TotalBsmtSF < 3000) | (df.SalePrice > 300000)) | (df.SalePrice ==0)]'''\n    df = df[(df.GrLivArea < 3000)| (df.SalePrice ==0)]\n    df = df[(df.SalePrice < 200)| (df.SalePrice ==0)]\n    print(\"Shape after removing outlier : \", df.shape)\n    \n    return df\n\ndf_out_removed = getOutlierIndices(df_fire.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dec3eb0b1a03e74b969d66f79428ff1cadcbdc67"
      },
      "cell_type": "code",
      "source": "def newBoxCoxTranformation1(df,target,testFile=False, log=False):\n    \n    #assuming that only numerical features are presented\n    print(\"Shape of the dataset initial : \", df.shape)\n    \n    if not testFile:\n        #outlier_indices = getOutlierIndices(df)\n        df =df[df.SalePrice >0]\n        \n        #let us remove outliers\n        '''df = grLivAreaOutlier(df)\n        df = garageCarsOutlier(df)\n        df = garageAreaOutlier(df)\n        df = totalBsmtSFOutlier(df)'''\n        \n        #df = preProcessData(df.copy())\n\n        \n        print(\"Shape of the dataset before transformation : \", df.shape)\n        y = np.array(df[target].apply( lambda x: math.log(x)))\n        X= df.drop(target,axis = 1)\n        x_columns = list(X)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        X = pd.DataFrame(X, columns=x_columns)\n        \n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        #X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        X = preprocessing.StandardScaler().fit_transform(X)\n        #X = X.values\n        print(\"Shape of the dataset after transformation : \", X.shape, y.shape)\n        return X,y\n    else:\n        df = df[df.SalePrice == 0.0]\n        #df = preProcessData(df.copy())\n        print(\"Shape of the dataset before transformation : \", df.shape)\n        X=df.drop(target,axis = 1)\n        x_columns = list(X)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        \n        X = pd.DataFrame(X, columns=x_columns)\n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        \n        #X = preprocessing.power_transform( X, method='box-cox')\n        #X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        X = preprocessing.StandardScaler().fit_transform(X)\n        #X = X.values\n        print(\"Shape of the dataset after transformation : \", X.shape)\n        return X\n\ndf_fire_b = df_fire.copy()\ndf_fire_b['SalePrice'] = df_fire_b['SalePrice'] / df_fire_b.GrLivArea\ndf_out_removed = getOutlierIndices(df_fire_b.copy())\ndf_prep = preProcessData(df_out_removed.copy())\nX = newBoxCoxTranformation1(df_prep.copy(),'SalePrice',True,False)  \nX,y = newBoxCoxTranformation1(df_prep.copy(),'SalePrice',False,False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "806a5ed17172affcfedbf50d70330b8646c7f239"
      },
      "cell_type": "code",
      "source": "from IPython import display\ndef transformTarget(df_temp, revert=False ):\n    '''df = df_temp.copy()\n    #df['new_variable'] = df.BedroomAbvGr * .1 + df.FullBath *.25 + df.HalfBath * .5 + df.BsmtFullBath *.75 + df.BsmtHalfBath * 1\n    #df['new_variable'] = df.BedroomAbvGr * .1 + df.FullBath *.25 + df.HalfBath * .5 + df.BsmtFullBath *.75 + df.BsmtHalfBath * .1\n    df['new_variable'] = df.BedroomAbvGr.apply(lambda x: x if x > 0 else 1)\n    #print(\"new_variable calculated\")\n    #display.display(df[df.new_variable.isna()])\n    \n    if not revert:\n        df['sales_per_new'] = df['SalePrice']/ df.new_variable\n        #display.display(df[df.sales_per_new.isna()])\n        y = np.array(df['sales_per_new'].apply( lambda x: math.log(x)))\n        return y\n    \n    df['sales_per_new'] = df['SalePrice']* df.new_variable\n    #print(\"sales_per_new calculated\")\n    #display.display(df[df.sales_per_new.isna()])'''\n    #return np.log( df_temp.SalePrice / df_temp.GrLivArea)\n    #return np.log(df_temp['SalePrice'])\n    return (df_temp['SalePrice'])\n    ",
      "execution_count": 124,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "883c06225559bfa909bc320fc88689df4bdd259e"
      },
      "cell_type": "code",
      "source": "def newBoxCoxTranformation(df_temp,target,testFile=False, log=False):\n    df = df_temp.copy()\n    #assuming that only numerical features are presented\n    if log:\n        print(\"Shape of the dataset initial : \", df.shape)\n    \n    if not testFile:\n        df =df[df.SalePrice >0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        \n        #display.display(df[df['SalePrice'].isna()])\n        y = transformTarget(df)\n        X= df.drop([target],axis = 1)\n        #X=df.drop(target, axis=1)\n        x_columns = list(X)\n        #print(x_columns)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X_testx = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X,y)\n        #X_testxx = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X,df.SalePrice)\n        #print(np.unique(X == X_testx))\n        #print(np.unique(X == X_testxx))\n        #print(np.unique(X_testx == X_testxx))\n        X = pd.DataFrame(X, columns=x_columns)\n        \n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape, y.shape)\n        return X,y\n    else:\n        df = df[df.SalePrice == 0.0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        X=df.drop(target,axis = 1)\n        x_columns = list(X)\n        #print(x_columns)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        \n        X = pd.DataFrame(X, columns=x_columns)\n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        \n        #X = preprocessing.power_transform( X, method='box-cox')\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape)\n        return X\n        \ndf_fire_b = df_fire.copy()\ndf_fire_b['SalePrice'] = df_fire_b['SalePrice'] / df_fire_b.GrLivArea\ndf_out_removed = getOutlierIndices(df_fire_b.copy())\ndf_prep = preProcessData(df_out_removed.copy())\nX = newBoxCoxTranformation(df_prep.copy(),'SalePrice',True,False)  \nX,y = newBoxCoxTranformation(df_prep.copy(),'SalePrice',False,False)",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Shape before removing outlier :  (2919, 78)\nShape after removing outlier :  (2882, 78)\nShape of the data set before pre processing :  (2882, 78)\n\n\nShape of the data set after pre processing :  (2882, 341)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4531f7f0612000650dac29fd8f5ec7dd7c06bd8d"
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=random.randint(1,500))",
      "execution_count": 126,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f9fcf3bfb0b8a6dc009fa8cd5f58aa149a1a0e5"
      },
      "cell_type": "code",
      "source": "#reg = XGBRegressor(max_depth=4, n_estimators=200)\n'''reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.25,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.0, reg_lambda=0.0, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1) #0.13073788936978095'''\n\nreg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=800,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)\n\nreg.fit(X_train,y_train)\nreg.score(X_test,y_test)",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 127,
          "data": {
            "text/plain": "0.7900031561961954"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eed59a169d49c2cd279cbb62e69b0f7f127f11a3"
      },
      "cell_type": "code",
      "source": "np.sqrt(mean_squared_log_error((y_test), (reg.predict(X_test))))\n#np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(reg.predict(X_test))))",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 129,
          "data": {
            "text/plain": "0.12427355827503385"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39d3c412bd01485aecba2d159d02aa4dbf8dac67"
      },
      "cell_type": "code",
      "source": "reg",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 130,
          "data": {
            "text/plain": "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=800,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddc77523a1218401082b46de2311c633ecebeaa1"
      },
      "cell_type": "code",
      "source": "def dummyCrossValidation(loop_count):\n    for i in range(loop_count):\n        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=random.randint(1,500))\n        reg = XGBRegressor(max_depth=3, n_estimators=100, reg_alpha=.5, reg_lambda=.5)\n        reg.fit(X_train,y_train)\n        \n        print(\"R2 Score: \", reg.score(X_test,y_test))\n        print(\"RMSLE Score : \", np.sqrt(mean_squared_log_error((y_test), (reg.predict(X_test)))))\n        #print(\"RMSLE Score : \", np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(reg.predict(X_test)))))\ndummyCrossValidation(10)",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": "R2 Score:  0.7411838063406728\nRMSLE Score :  0.13914474577656455\nR2 Score:  0.7705217854286269\nRMSLE Score :  0.128712985235048\nR2 Score:  0.7659611731347808\nRMSLE Score :  0.12708034580087846\nR2 Score:  0.7399736620743083\nRMSLE Score :  0.13700615329255592\nR2 Score:  0.7469803417123881\nRMSLE Score :  0.12838130514005583\nR2 Score:  0.7296315714648065\nRMSLE Score :  0.1374840966074223\nR2 Score:  0.7597053047495568\nRMSLE Score :  0.13633871273268358\nR2 Score:  0.7755820591234065\nRMSLE Score :  0.12845408728968877\nR2 Score:  0.7802512465245441\nRMSLE Score :  0.12594878460041573\nR2 Score:  0.7487091405924081\nRMSLE Score :  0.13357726690260732\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c02210e9c30a1e470f3504f4b772ac308c963af"
      },
      "cell_type": "code",
      "source": "reg.fit(X,y)",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 133,
          "data": {
            "text/plain": "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=800,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "deda18c8d44e83cac4810ec68326bff1628b2ce3"
      },
      "cell_type": "code",
      "source": "def checkTheTrainFile(reg,df_prep):\n    df_train = pd.read_csv(train_File)\n    #df_tra['SalePrice'] = 0.0\n    \n    #df_train =  pd.read_csv(train_File)\n    #df_concat = pd.concat([df_train,df_test])\n\n    #print(df_test[df_test.TotalBsmtSF.isna()])\n    #return\n    #df = giveMeWrangledData(df_concat,True)\n    \n    #print(df.info())\n    #df = preProcessData(df)\n    #print(df.info())\n    #dt = df_prep.copy()\n    \n    #df_prep = preProcessData(df_fire.copy())\n    #dt = dt[dt.SalePrice > 0]\n    df_prep1 = df_prep.copy()\n    df_prep1 = df_prep1[df_prep1['SalePrice'] > 0]\n    df_prep1['New_SalePrice'] = 0.0\n    \n    df_prep = df_prep[df_prep['SalePrice'] > 0]\n    df_prep['SalePrice'] = 0\n    \n    X = newBoxCoxTranformation(df_prep.copy(),'SalePrice',True)\n    #print(np.sqrt(mean_squared_log_error(y, reg.predict(X))))\n    \n    #df_prep1['New_SalePrice'] = list(np.exp(reg.predict(X)))\n    df_prep1['New_SalePrice'] = reg.predict(X)\n    df_prep1['New_SalePrice'] = df_prep1['New_SalePrice'] * df_prep1.GrLivArea\n    df_prep1['SalePrice'] = df_prep1['SalePrice'] * df_prep1.GrLivArea\n    \n    \n    df_train_score = df_prep1[df_prep1.SalePrice > 0]\n    \n    print(np.sqrt(mean_squared_log_error(df_train_score['SalePrice'], df_train_score['New_SalePrice'])))\n    \n    #return df_test, X, reg.predict(X)\n#df_test, X_dummy, y_dummy= checkTheTestFile(reg)\ncheckTheTrainFile(reg,df_prep.copy())",
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.031629483392693634\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "9222b48f2887acd0e3ce113ea35af963e2d9a7fd"
      },
      "cell_type": "code",
      "source": "def checkTheTestFile(reg):\n    df_test = pd.read_csv(test_File)\n    df_test['SalePrice'] = 0.0\n    print(df_test.shape)\n    \n    #df_train =  pd.read_csv(train_File)\n    #df_concat = pd.concat([df_train,df_test])\n\n    #print(df_test[df_test.TotalBsmtSF.isna()])\n    #return\n    #df = giveMeWrangledData(df_concat,True)\n    \n    #print(df.info())\n    #df = preProcessData(df)\n    #print(df.info())\n    X = newBoxCoxTranformation(df_prep.copy(),'SalePrice',True)\n    #print(np.sqrt(mean_squared_log_error(y, reg.predict(X))))\n    df_test['SalePrice'] = (reg.predict(X))\n    #df_test['SalePrice'] = np.exp(reg.predict(X))\n    df_test['SalePrice'] = df_test['SalePrice'] * df_test.GrLivArea\n    \n    return df_test, X, reg.predict(X)\ndf_test, X_dummy, y_dummy= checkTheTestFile(reg)",
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(1459, 81)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be5f648d502133c2a482563a7f9691ba120dbd2c"
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']].to_csv('submission.csv',index=False)",
      "execution_count": 137,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2c810e950be5a1c2abeab4c194b53a4a5f0d958"
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']]",
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 138,
          "data": {
            "text/plain": "        Id      SalePrice\n0     1461  136423.464844\n1     1462  174365.282639\n2     1463  203167.291168\n3     1464  200823.291565\n4     1465  190085.859375\n5     1466  188352.843857\n6     1467  173680.381607\n7     1468  179601.184998\n8     1469  204438.965378\n9     1470  132822.651215\n10    1471  207509.291458\n11    1472  112803.620956\n12    1473  103375.622314\n13    1474  158200.000854\n14    1475  114878.449646\n15    1476  376371.567535\n16    1477  256446.535400\n17    1478  301344.113892\n18    1479  263418.980621\n19    1480  474275.954468\n20    1481  314260.997772\n21    1482  212589.064331\n22    1483  179489.982483\n23    1484  158826.137466\n24    1485  191233.582489\n25    1486  207506.560402\n26    1487  323418.668747\n27    1488  250572.660065\n28    1489  192191.501404\n29    1490  252008.398926\n...    ...            ...\n1429  2890   81926.511642\n1430  2891  151775.636353\n1431  2892   29737.581997\n1432  2893   63313.758102\n1433  2894   38976.199860\n1434  2895  300873.786163\n1435  2896  265810.690460\n1436  2897  217392.498016\n1437  2898  133663.245117\n1438  2899  215144.479294\n1439  2900  174616.075592\n1440  2901  224318.023682\n1441  2902  199363.075943\n1442  2903  321269.110107\n1443  2904  333237.959534\n1444  2905   99113.122559\n1445  2906  193253.128418\n1446  2907  114724.412170\n1447  2908  142727.189972\n1448  2909  151370.247314\n1449  2910   81224.559174\n1450  2911   76404.167084\n1451  2912  158166.779785\n1452  2913   87098.547180\n1453  2914   81043.050934\n1454  2915   86993.431183\n1455  2916   79557.955261\n1456  2917  173335.758179\n1457  2918  127077.888947\n1458  2919  203733.932495\n\n[1459 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1461</td>\n      <td>136423.464844</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1462</td>\n      <td>174365.282639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1463</td>\n      <td>203167.291168</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1464</td>\n      <td>200823.291565</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1465</td>\n      <td>190085.859375</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1466</td>\n      <td>188352.843857</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1467</td>\n      <td>173680.381607</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1468</td>\n      <td>179601.184998</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1469</td>\n      <td>204438.965378</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1470</td>\n      <td>132822.651215</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1471</td>\n      <td>207509.291458</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1472</td>\n      <td>112803.620956</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1473</td>\n      <td>103375.622314</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1474</td>\n      <td>158200.000854</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1475</td>\n      <td>114878.449646</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1476</td>\n      <td>376371.567535</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1477</td>\n      <td>256446.535400</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1478</td>\n      <td>301344.113892</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1479</td>\n      <td>263418.980621</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1480</td>\n      <td>474275.954468</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1481</td>\n      <td>314260.997772</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1482</td>\n      <td>212589.064331</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1483</td>\n      <td>179489.982483</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1484</td>\n      <td>158826.137466</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1485</td>\n      <td>191233.582489</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1486</td>\n      <td>207506.560402</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1487</td>\n      <td>323418.668747</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1488</td>\n      <td>250572.660065</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1489</td>\n      <td>192191.501404</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1490</td>\n      <td>252008.398926</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1429</th>\n      <td>2890</td>\n      <td>81926.511642</td>\n    </tr>\n    <tr>\n      <th>1430</th>\n      <td>2891</td>\n      <td>151775.636353</td>\n    </tr>\n    <tr>\n      <th>1431</th>\n      <td>2892</td>\n      <td>29737.581997</td>\n    </tr>\n    <tr>\n      <th>1432</th>\n      <td>2893</td>\n      <td>63313.758102</td>\n    </tr>\n    <tr>\n      <th>1433</th>\n      <td>2894</td>\n      <td>38976.199860</td>\n    </tr>\n    <tr>\n      <th>1434</th>\n      <td>2895</td>\n      <td>300873.786163</td>\n    </tr>\n    <tr>\n      <th>1435</th>\n      <td>2896</td>\n      <td>265810.690460</td>\n    </tr>\n    <tr>\n      <th>1436</th>\n      <td>2897</td>\n      <td>217392.498016</td>\n    </tr>\n    <tr>\n      <th>1437</th>\n      <td>2898</td>\n      <td>133663.245117</td>\n    </tr>\n    <tr>\n      <th>1438</th>\n      <td>2899</td>\n      <td>215144.479294</td>\n    </tr>\n    <tr>\n      <th>1439</th>\n      <td>2900</td>\n      <td>174616.075592</td>\n    </tr>\n    <tr>\n      <th>1440</th>\n      <td>2901</td>\n      <td>224318.023682</td>\n    </tr>\n    <tr>\n      <th>1441</th>\n      <td>2902</td>\n      <td>199363.075943</td>\n    </tr>\n    <tr>\n      <th>1442</th>\n      <td>2903</td>\n      <td>321269.110107</td>\n    </tr>\n    <tr>\n      <th>1443</th>\n      <td>2904</td>\n      <td>333237.959534</td>\n    </tr>\n    <tr>\n      <th>1444</th>\n      <td>2905</td>\n      <td>99113.122559</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>2906</td>\n      <td>193253.128418</td>\n    </tr>\n    <tr>\n      <th>1446</th>\n      <td>2907</td>\n      <td>114724.412170</td>\n    </tr>\n    <tr>\n      <th>1447</th>\n      <td>2908</td>\n      <td>142727.189972</td>\n    </tr>\n    <tr>\n      <th>1448</th>\n      <td>2909</td>\n      <td>151370.247314</td>\n    </tr>\n    <tr>\n      <th>1449</th>\n      <td>2910</td>\n      <td>81224.559174</td>\n    </tr>\n    <tr>\n      <th>1450</th>\n      <td>2911</td>\n      <td>76404.167084</td>\n    </tr>\n    <tr>\n      <th>1451</th>\n      <td>2912</td>\n      <td>158166.779785</td>\n    </tr>\n    <tr>\n      <th>1452</th>\n      <td>2913</td>\n      <td>87098.547180</td>\n    </tr>\n    <tr>\n      <th>1453</th>\n      <td>2914</td>\n      <td>81043.050934</td>\n    </tr>\n    <tr>\n      <th>1454</th>\n      <td>2915</td>\n      <td>86993.431183</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>2916</td>\n      <td>79557.955261</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>2917</td>\n      <td>173335.758179</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>2918</td>\n      <td>127077.888947</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>2919</td>\n      <td>203733.932495</td>\n    </tr>\n  </tbody>\n</table>\n<p>1459 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "300dbf8ff9df501991f9c45c0046ff8b08c15228"
      },
      "cell_type": "code",
      "source": "def forCrossValidationStratifiedShuffleSplit(df):\n    sss = StratifiedShuffleSplit(n_splits=10, test_size=.5, random_state=1986)\n    #print(\"Number of Splits configured :\", sss.get_n_splits(df, df.BldgType))\n    \n    for train_index, test_index in sss.split(df, df.BldgType):\n        yield train_index, test_index\n        \n    for train_index, test_index in sss.split(df, df.OverallQual):\n        yield train_index, test_index",
      "execution_count": 139,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d87e6c649933258dcaa6f6a269b274ef647d462"
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv(train_File)\ndef crossValidationScoring(reg,X,y):\n    return -np.sqrt(mean_squared_log_error(np.exp(y), \n                                          np.exp(reg.predict(X))\n                                          ))\n    #return np.sqrt(mean_squared_log_error(np.exp(y), \n    #                                      np.exp(reg.predict(X\n    #                                                        ))\n    #                                      ))\n\n    #return np.sqrt(mean_squared_log_error((y), \n    #                                      (reg.predict(X))\n    #                                      ))\nmean_temp_rmsle = np.mean(cross_val_score(reg,X,y,cv= 5,scoring='neg_mean_squared_log_error'))\nprint(\"RMSE with without target variable transformation : \", np.sqrt(mean_temp_rmsle * -1))\n\nmean_temp_rmsle = np.mean(cross_val_score(reg, X, y,\n                                          cv= 5,\n                                          scoring=crossValidationScoring))\nprint(\"RMSE with post target variable transformation : \", mean_temp_rmsle)",
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": "RMSE with without target variable transformation :  0.11843599777284641\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n  after removing the cwd from sys.path.\n",
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-140-d1b95b48273a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m mean_temp_rmsle = np.mean(cross_val_score(reg, X, y,\n\u001b[1;32m     18\u001b[0m                                           \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                           scoring=crossValidationScoring))\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE with post target variable transformation : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_temp_rmsle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \"\"\"\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-140-d1b95b48273a>\u001b[0m in \u001b[0;36mcrossValidationScoring\u001b[0;34m(reg, X, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrossValidationScoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     return -np.sqrt(mean_squared_log_error(np.exp(y), \n\u001b[0;32m----> 4\u001b[0;31m                                           \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                           ))\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#return np.sqrt(mean_squared_log_error(np.exp(y),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 310\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "741c90110661a8e9580a2da3c87eaefbcfc19cb9"
      },
      "cell_type": "markdown",
      "source": "from sklearn.model_selection import GridSearchCV\ndef gridSearchCVImp():\n    start_time =datetime.datetime.now()\n    reg = XGBRegressor(n_jobs =2, reg_alpha=.5, reg_lambda=.5, subsample=1)\n    \n    parameters = {\n        'max_depth':list(range(3,4)),\n        'colsample_bylevel' : np.arange(0.0, 1.0, 0.25),\n        'n_estimators' : list(range(500,1500,100)),\n        'colsample_bytree' : np.arange(0.0, 1.0, 0.25),\n        #'reg_alpha': np.arange(0.0, 1.0, 0.25),\n        #'reg_lambda': np.arange(0.0, 1.0, 0.25)\n        }\n    cv= ShuffleSplit(n_splits=5, test_size=.5, random_state=1986)\n    reg_grid = GridSearchCV(reg, parameters, \n                            #cv=forCrossValidationStratifiedShuffleSplit(df_train),\n                            cv=cv,\n                            n_jobs = 2,\n                            #scoring = 'neg_median_absolute_error',\n                            #scoring = 'neg_mean_absolute_error',\n                            #scoring = 'neg_mean_squared_log_error',\n                            #scoring = crossValidationScoring,\n                            verbose=1,\n                            #error_score ='raise'\n                            error_score =5\n                            #pre_dispatch = 2\n                           )\n    reg_grid.fit(X,y)\n    \n    print(\"Total time for the gridserach\", datetime.datetime.now() - start_time)\n    \n    return reg_grid\nreg_grid = gridSearchCVImp()\nprint(reg_grid.best_estimator_)\nprint(reg_grid.best_score_)"
    },
    {
      "metadata": {
        "_uuid": "755bde2b47f64465a570754700fa9b17bb5108e2"
      },
      "cell_type": "markdown",
      "source": "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=2, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.5)\n0.8998707938479593"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a621ec768591cd8f8f1e9db44267e959c41c4d64"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}