{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2af6c1bf47b5300fddfd754fe24eff638d46d9e"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport datetime\n\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nimport math\nimport random\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.metrics import  mean_squared_log_error\n\nfrom xgboost import XGBRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4909622d44d7e20201b25b6407af57e7a035498"
      },
      "cell_type": "code",
      "source": "#Global Variables for grid search\nn_splits = 10\nn_jobs = 2\n\nmax_depth_min = 3\nmax_depth_max = 11\nn_estimator_min = 100\nn_estimator_max = 200\n\ntest_size =0.5\nrandom_state = 1986",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c5318c5aa322350262771276ec2e9011d6ace91"
      },
      "cell_type": "code",
      "source": "train_File = '../input/train.csv'\ntest_File = '../input/test.csv'\n\ndf_train = pd.read_csv(train_File)\ndf_test = pd.read_csv(test_File)\ndf_test['SalePrice'] = 0\ndf_concat = pd.concat([df_train,df_test])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7273d0af0a883ac7cafdc44cf7b67cbb1019b039"
      },
      "cell_type": "code",
      "source": "df_concat.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c17981d35937f82123173ebd02604c0bb0ea2c06"
      },
      "cell_type": "code",
      "source": "def giveMeWrangledData(df, testFile=False, log=False):\n    \n    \n    df = df.drop(['Id', 'GarageYrBlt','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'],axis=1)\n    \n    df['LotFrontage'] =df.LotFrontage.fillna(df.LotFrontage.mode()[0])\n    df['MasVnrArea']=df.MasVnrArea.fillna(0.0)\n    df['TotalBsmtSF'] = df.TotalBsmtSF.fillna(0)\n    df['BsmtFullBath'] = df.BsmtFullBath.fillna(0)\n    df['BsmtHalfBath'] = df.BsmtHalfBath.fillna(0)\n    df['GarageCars'] = df.GarageCars.fillna(0)\n    df['GarageArea'] = df.GarageArea.fillna(0)\n    \n    #convert data type\n    #we are being little lineant to give int64 for YearBuilt, YrSold but those guys are going to be box-coxed \n    #so let them at least enjoy the bigger size for now\n    int64_variables = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', \\\n                     'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \\\n                     'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\\\n                     'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n                     'PoolArea', 'MiscVal', 'YrSold', 'SalePrice']\n    \n    #if testFile:\n    #    int64_variables.remove('SalePrice')\n\n        \n        \n        \n    \n    for c in int64_variables:\n        if log:\n            print(\"Changing the data type for :\", c)\n        #df[c] = df[c].astype(np.int64)\n        df[c] = df[c].astype(np.float64)\n        \n    int_to_categorical_variables = ['MSSubClass', 'OverallQual', 'OverallCond', 'FireplaceQu', 'MoSold']\n    for c in int_to_categorical_variables:\n        df[c] = df[c].astype(str)\n        \n    df = df.fillna('NotAvailable')\n    return df\ndf = giveMeWrangledData(df_concat)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfc70ea0613ac359fc7fcf69f4fe5138c2d8159b"
      },
      "cell_type": "code",
      "source": "df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6ca7e49c5dc6c21600fd68ddd89f3a84e39515d"
      },
      "cell_type": "code",
      "source": "def preProcessData(df, log=False):\n    \n    \n\n    #get dummies\n    if log:\n        print(\"Shape of the data set before pre processing : \", df.shape )\n        print(\"Categorical columns : \", list(df.select_dtypes(exclude=np.number)))\n    df = pd.get_dummies(df, dtype=np.float64)\n    \n    \n    \n    if log:\n        print(\"\\n\\nShape of the data set after pre processing : \", df.shape )\n        print(\"Columns in the data set are : \",list(df))\n\n    return df\n\ndf_prep = preProcessData(df)\ndf_prep.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1a805a9ca49b054c0eb6e50905effac11fc6eff6"
      },
      "cell_type": "markdown",
      "source": "https://stats.stackexchange.com/questions/130262/why-not-log-transform-all-variables-that-are-not-of-main-interest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4755fe3335b956ba6d54bb079b984368cc3a6f6c"
      },
      "cell_type": "code",
      "source": "def newBoxCoxTranformation(df,target,testFile=False, log=False):\n    \n    #assuming that only numerical features are presented\n    if log:\n        print(\"Shape of the dataset initial : \", df.shape)\n    \n    if not testFile:\n        df =df[df.SalePrice >0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        y = np.array(df[target].apply( lambda x: math.log(x)))\n        X= df.drop(target,axis = 1)\n        x_columns = list(X)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        X = pd.DataFrame(X, columns=x_columns)\n        \n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape, y.shape)\n        return X,y\n    else:\n        df = df[df.SalePrice == 0.0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        X=df.drop(target,axis = 1)\n        x_columns = list(X)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        \n        X = pd.DataFrame(X, columns=x_columns)\n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        \n        #X = preprocessing.power_transform( X, method='box-cox')\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape)\n        return X\n        \n    \n\nX = newBoxCoxTranformation(df_prep,'SalePrice',True,False)  \nX,y = newBoxCoxTranformation(df_prep,'SalePrice',False,False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5fade6388778969408a988fae1972df3b453a876"
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=random.randint(1,500))#, stratify=df.BldgType)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c481d90f4fe0e0193c6640c0c08ded36ea776920"
      },
      "cell_type": "code",
      "source": "reg = XGBRegressor(max_depth=6,n_estimator=100, \n                   colsample_bytree=.25,\n                   colsample_bylevel=1,\n                   reg_alpha =1,\n                   reg_lambda=0, \n                   learning_rate=0.1,                   \n                  )\nreg.fit(X_train,y_train)\nreg.score(X_test,y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8da4789c20d447c150607bebd18b21ed73e8bdc"
      },
      "cell_type": "code",
      "source": "np.sqrt(mean_squared_log_error(y_test, reg.predict(X_test)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "184a80c1ae678e85eda56d4faa5047ffbaae5321"
      },
      "cell_type": "code",
      "source": "np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(reg.predict(X_test))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de6dfc7cc02217e7ffdbf9d73da406fe993798aa"
      },
      "cell_type": "code",
      "source": "reg.fit(X,y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba35c7645edf1637935e3e900f8570fa6e850049"
      },
      "cell_type": "code",
      "source": "reg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7edf959b8aae25f97d9e71f36c400f7a8465f1cf"
      },
      "cell_type": "markdown",
      "source": "##### We need to have different pre-processing logic to test data. We will come back to it little later."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5262ac7909e28889a0aaba1564ee310caa43488"
      },
      "cell_type": "code",
      "source": "def checkTheTestFile(reg):\n    df_test = pd.read_csv(test_File)\n    df_test['SalePrice'] = 0.0\n    \n    df_train =  pd.read_csv(train_File)\n    df_concat = pd.concat([df_train,df_test])\n\n    #print(df_test[df_test.TotalBsmtSF.isna()])\n    #return\n    df = giveMeWrangledData(df_concat,True)\n    \n    #print(df.info())\n    df = preProcessData(df)\n    #print(df.info())\n    X = newBoxCoxTranformation(df,'SalePrice',True)\n    #print(np.sqrt(mean_squared_log_error(y, reg.predict(X))))\n    \n    df_test['SalePrice'] = np.exp(reg.predict(X))\n    \n    \n    return df_test, X, reg.predict(X)\ndf_test, X_dummy, y_dummy= checkTheTestFile(reg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4b1a60a7d572f5520b9d5a15d6cad6705c2fb3b"
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "add99d35892adf15e435ee2289eeae6309ecbb39"
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']].to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9fc8f97f810bde8b8156ec7d805ff4bc164ceb1d"
      },
      "cell_type": "markdown",
      "source": "##### I got Kaggle Rank of 2539/4463 with RMSLE =0.14357\n##### As on 1/17/2019 : 9:06PM IST\n* 0.13501 ==> 2040 \n* 0.13252 ==> 1865\n* 0.13002 ==> 1704\n* 0.12658 ==> 1500\n* 0.12351 ==> 1250\n* 0.12081 ==> 1000\n* 0.11572 ==> 500\n* 0.11475 ==> 250\n* 0.11310 ==> 100\n* 0.10985 ==> 50\n* 0.10973 ==> 25\n* 0.10845 ==> 10\n* 0.08021 ==> 5\n* 0.00000 ==> 1"
    },
    {
      "metadata": {
        "_uuid": "239afef2151c918dba6e55e8953533963f4b794c"
      },
      "cell_type": "markdown",
      "source": "##### Now that I know around what score gets what rank; can we have a function which would what would be testing score ?"
    },
    {
      "metadata": {
        "_uuid": "f1cf3ab0ab8f51dbea7eb474a40fa6e5166f7101"
      },
      "cell_type": "markdown",
      "source": "##### Logic is to predict first the testing samples. Later use that for training and predict the initial training data set. We would then have actual and predicted SalePrices with which we can calculated the RMSLE."
    },
    {
      "metadata": {
        "_uuid": "04a364da82d203e9ab6e14363bdcc3ef5db2d66d"
      },
      "cell_type": "markdown",
      "source": "##### Would this logic work ? let us try for our case now and compare that with Kaggle result....Finger crossed :)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd7f6c4a4c973389d46564509706a61e435dccc8"
      },
      "cell_type": "code",
      "source": "def forCrossValidationStratifiedShuffleSplit(df):\n    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n    #print(\"Number of Splits configured :\", sss.get_n_splits(df, df.BldgType))\n    \n    for train_index, test_index in sss.split(df, df.BldgType):\n        yield train_index, test_index\n        \n    for train_index, test_index in sss.split(df, df.OverallQual):\n        yield train_index, test_index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed74572f3252cae125da18849bbc4dcc56c58f09"
      },
      "cell_type": "code",
      "source": "def crossValidationScoring(reg,X,y):\n    return np.sqrt(mean_squared_log_error(np.exp(y), \n                                          np.exp(reg.predict(X))\n                                          ))\nmean_temp_rmsle = np.mean(cross_val_score(reg,X,y,cv= 5,scoring='neg_mean_squared_log_error'))\nprint(\"RMSE with without target variable transformation : \", np.sqrt(mean_temp_rmsle * -1))\n\nmean_temp_rmsle = np.mean(cross_val_score(reg, X, y,\n                                          cv= forCrossValidationStratifiedShuffleSplit(df_train),\n                                          scoring=crossValidationScoring))\nprint(\"RMSE with post target variable transformation : \", mean_temp_rmsle)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "a048507811ab76194c514b637c8b0b2308a97e5f"
      },
      "cell_type": "markdown",
      "source": "def doGridSearch():\n    \n    start_time = datetime.datetime.now()\n    \n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    df_test['SalePrice'] = 0\n    df_concat = pd.concat([df_train,df_test])\n    \n    df = giveMeWrangledData(df_concat)\n    df_prep = preProcessData(df)\n    \n    X,y = newBoxCoxTranformation(df_prep,'SalePrice',False,False)\n    \n    score_list = []\n    '''for i in range(max_depth_min,max_depth_max):\n        for j in range(n_estimator_min,n_estimator_max,100):\n            loop_start = datetime.datetime.now()\n            for bytree in [0.25, 0.5, 0.75,1]:\n                for bylevel in [0.25, 0.5, 0.75,1]:'''\n    for reg_alpha in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1]:\n        #loop_start = datetime.datetime.now()\n        for reg_lambda in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1]:\n\n        #for lrate in [.01, .025, 0.05, .075, .1, .25, .5]:\n            #j=100\n\n\n            #reg = XGBRegressor(max_depth=i, n_estimators=j)\n            '''reg=XGBRegressor(max_depth=i, \n                 n_estimator=j,\n                 colsample_bylevel=bylevel,\n                 colsample_bytree=bytree,\n                 learning_rate=.1,\n                 reg_alpha =1,\n                 reg_lambda=1 ,\n                 n_jobs=n_jobs\n                )'''\n\n            i=6\n            j=100\n            bylevel= 1\n            bytree = .25\n            reg=XGBRegressor(max_depth=6, \n                 n_estimator=100,\n                 colsample_bylevel=bylevel,\n                 colsample_bytree=bytree,\n                 learning_rate=.1,\n                 reg_alpha =reg_alpha,\n                 reg_lambda=reg_lambda ,\n                 n_jobs=n_jobs\n                )\n\n            #cv = ShuffleSplit(n_splits=20, test_size=random.randint(7,9)/10, random_state=random.randint(1,1000))\n            #cv = ShuffleSplit(n_splits=20, test_size=.5, random_state=1986)\n            #print(datetime.datetime())\n            cross_cv = cross_val_score(reg, X, y,\\\n                                       cv=forCrossValidationStratifiedShuffleSplit(df_train), \\\n                                       #cv=cv, \\\n                                       scoring=crossValidationScoring,n_jobs=n_jobs)\n            #print(\" Validat Median Score : \", np.sqrt(np.median(cross_cv) * -1), \\\n            #      \"Average Score : \", np.sqrt(np.average(cross_cv) * -1) )\n\n            reg.fit(X,y)\n            training_score = np.sqrt(mean_squared_log_error(np.exp(y), np.exp(reg.predict(X))))\n            #print(\"Training Score :\", training_score)\n\n            df_test_new, X_test, y_test = checkTheTestFile(reg)\n            reg.fit(X_test,y_test)\n            testing_score = np.sqrt(mean_squared_log_error(np.exp(y), np.exp(reg.predict(X))))\n            #print(\"Testing Score :\", testing_score)\n            print(\"Scores\",(training_score, np.average(cross_cv), testing_score,i,j,bytree, bylevel, reg_alpha, reg_lambda))\n            score_list.append((training_score, np.average(cross_cv), testing_score,i,j,bytree, bylevel,reg_alpha, reg_lambda))\n\n        #print(\"Time for max_depth -\",i,\"n_estimator -\",j,\" is : \", datetime.datetime.now() - loop_start)\n    \n    print(\"Total time for GridSearch : \", datetime.datetime.now() - start_time)\n    return score_list\n\nscore_list = doGridSearch()\n#sorted(score_list,key= lambda x:x[0])"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b985a529e359db621577c792daa6e417b611d1aa"
      },
      "cell_type": "code",
      "source": "temp_df = pd.DataFrame(score_list,columns=[\"training_score\",\n                                           \"validation_score\",\n                                           \"testing_score\", \n                                           \"max_depth\",\n                                           \"n_estimator\",\n                                           \"bytree\",\n                                           \"bylevel\",\n                                           \"reg_alpha\",\n                                           \"reg_lambda\"\n                                          ])\n\ntemp_df.to_csv(\"GridSearchResults-\"+str(datetime.datetime.now().date()))\ntemp_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b0f6d66aecf5602b56c980160d0ccb5b3023929"
      },
      "cell_type": "code",
      "source": "temp_df[temp_df.testing_score == temp_df.testing_score.min()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "452401a7e157947052e0f283ba81bd3d0fa289af",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "temp_df[temp_df.validation_score == temp_df.validation_score.min()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "4e4ebf0d6215466b1111e8006308637f726c79d6"
      },
      "cell_type": "code",
      "source": "def plottheTemp(df,title):\n    #for bytree in [.25,.5,.75,1]:\n    #for n in temp_df.n_estimator.unique():\n        fig, ax = plt.subplots()\n        plt.title(title)\n        ax = df.plot.line('max_depth','training_score',ax =ax,xticks= list(range(3,11)))\n        ax = df.plot.line('max_depth','validation_score',ax =ax,xticks= list(range(3,11)))\n        ax = df.plot.line('max_depth','testing_score',ax =ax,xticks= list(range(3,11)))\n        #x_point = temp_df[temp_df.validation_score == temp_df.validation_score.min()]['max_depth'].tolist()[0]\n        #ax= ax.vlines(x_point,0,.2)\n        #x_point1 = df[df.testing_score == temp_df.testing_score.min()]['max_depth'].tolist()[0]\n        #ax= ax.vlines(x_point1,0,.2)\n        \n        plt.show()\n#,'validation_score')\n\nfor bytree in temp_df.bytree.unique():\n    plottheTemp(temp_df[temp_df.bytree == bytree], \"bytree - \"+str(bytree))\n    \nfor bylevel in temp_df.bylevel.unique():\n    plottheTemp(temp_df[temp_df.bylevel == bylevel], \"bylevel - \"+str(bylevel))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28a19e9ea5f2cf089bcf2471d23151cf49feb240"
      },
      "cell_type": "code",
      "source": "temp_df[[\"reg_alpha\",\"validation_score\"]].groupby(by=\"reg_alpha\").agg('mean').reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "000adeebc5beef1cdef75cf77759259d33141f47"
      },
      "cell_type": "code",
      "source": "temp_df[[\"reg_lambda\",\"validation_score\"]].groupby(by=\"reg_lambda\").agg('median').reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1687d221c88c9bd7e54ae0c15a8c22eff5b646cd"
      },
      "cell_type": "markdown",
      "source": "##### Kaggle score remained unchanged, this proves that our logic of test score calculation fails and there must be some value prediction which is huge which brings the average error value up\n"
    },
    {
      "metadata": {
        "_uuid": "86ec97e2b5317e69a5742374760e089a89ffa358"
      },
      "cell_type": "markdown",
      "source": "##### Next Steps:\n* re run the grid search and note training score, validation score and testing score. This should not only double check on test score calculation but also gives us the right hyper parameter from the training and validation perspective.\n* address the runtime error during scaling or power transform. If boxcox fails attempt a log transformation at least.\n* stratify fold testing to check if the training score & validation in the previous exercise still holds goods.\n* hyper parameter research for XGBoost\n* target variable transformation\n* best of best stack approach\n* team work stack approach\n* XGBoost as the final assesser in best of best stack approach\n* XGBoost as the final assesser in the team work starck approach\n* 3 layers in stack approach: best of best candidates in the order of their accuracy feeding on input in each case.\n* re-assess the dataset"
    },
    {
      "metadata": {
        "_uuid": "fd61af6bd1d247a69e846af22625a265b4f6b2f2"
      },
      "cell_type": "markdown",
      "source": "##### The grid search in the kaggle resulted in the different hyper parameter for lowest validation score. Not sure why is that ? validation is through the shuffle split. isnt 3 cross validation set sufficient ?\n\n##### Or it is a game of kfold shuffle split and stratify ?"
    },
    {
      "metadata": {
        "_uuid": "55a0a09f1fd87fe69a94859187b6ccc2f9387c8a"
      },
      "cell_type": "markdown",
      "source": "##### It only makes sense to move on for stacked approach and other hyper parameter tuning if we sort out the cross validation consistency issue. Otherwise, we cannot have the confidence of impact of changes in stacked approaches."
    },
    {
      "metadata": {
        "_uuid": "5cd73aadf72033198ff29708d8389915afa21bd8"
      },
      "cell_type": "markdown",
      "source": "##### The full grid search for split count = 10 is as below. It is evident that \n* testing training score is of no use. We can ignore it in the future grid searches. it would save couple minutes from one grid search.\n\n##### Following are the observation yet to be confirmed:\n* Does the testing score confidence is high; post custom stratified split ? We will have predict with the best testing score and see the rank in Kaggle. 6,800 = > 0.15404  & 3,800 ==> 0.16021\n* Does we have relation between validation score and testing score ?\n* Can we first focus on training score to be 0 with other hyper parameters ? would that be overfitting ? Should we need still learning curve of validation vs testing score.?\n* Can we ignore n_estimator and see if the validation score converges with testing score only with max_depth and other hyper parameters?"
    },
    {
      "metadata": {
        "_uuid": "5b47e9763bd59669f61c104944f46de2161ce0d4"
      },
      "cell_type": "markdown",
      "source": "##### Let us talk about the hyper parameter till we get the gridsearch result"
    },
    {
      "metadata": {
        "_uuid": "8db131a039893493f30424aa9a2532389594614b"
      },
      "cell_type": "markdown",
      "source": "* https://xgboost.readthedocs.io/en/latest/parameter.html\n* https://www.kaggle.com/dansbecker/xgboost\n* https://www.datacamp.com/community/tutorials/xgboost-in-python\n\n* 1 being the max value, let us have half as the value for {'colsample_bytree':0.5, 'colsample_bylevel':0.5}\n* learning_rate = 0.05 because we have already using 0.1 so far. It is suggested in the kaggle blog.\n* n_jobs= 2 /4 based on the cpu. I guess Kaggle provides 4 cpu machine. I saw the max cpu spike as 400%\n* Surprisingly and unnoticed so far that it does the cross validation by itself. so we can leverage the n_estimator to be 1000 and use early_stopping_rounds for our quick turnaround. So our 2 for loops reduced to one :)\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4c3e26e2e455e9f089320c3af74fb7543ade251"
      },
      "cell_type": "code",
      "source": "reg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fac7047a1da3551cf8ccc791a2915d37408df48"
      },
      "cell_type": "code",
      "source": "reg = XGBRegressor(max_depth=4, n_estimators=1000, learning_rate = 0.05, n_jobs = 2, colsample_bylevel = 0.5, colsample_bytree = 0.5)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=random.randint(1,500))\nreg.fit(X_train,y_train, early_stopping_rounds=250, eval_metric='rmse',eval_set=[(X_test,y_test)], verbose=100,)\n\n'''my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)'''\nprint()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "38f4d0e988044c5d6df7100d966c8101289ec893"
      },
      "cell_type": "markdown",
      "source": "* 15 ==> validation_0-rmse:0.145474\n* 25 ==> [ 527 ]\tvalidation_0-rmse:0.14391\n* 100 ==> [567]\tvalidation_0-rmse:0.143869\n* 250 ==> [567]\tvalidation_0-rmse:0.143869\n\n\n* for max_depth = 4\n  * [273]\tvalidation_0-rmse:0.148329\n  * [729]\tvalidation_0-rmse:0.12575\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96eba0a45dfa1e1709bcf1d15e5b0001cdb7945b"
      },
      "cell_type": "code",
      "source": "reg2 = XGBRegressor()\nreg2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fe03586407468ebf629ffc1e0897a8d852aae688"
      },
      "cell_type": "markdown",
      "source": "##### n_estimator we will choose it at the end again through Stratified"
    },
    {
      "metadata": {
        "_uuid": "cdb259d1e30e2fd868aab2baef2cb985f7fe62d9"
      },
      "cell_type": "markdown",
      "source": "##### With hyper parameter set to above the grid search is as below:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c34b14e4a2b64d21b3ca57cd1be6d69e5258fbd7"
      },
      "cell_type": "code",
      "source": "    ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}