{
  "cells": [
    {
      "metadata": {
        "_uuid": "f2af6c1bf47b5300fddfd754fe24eff638d46d9e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport datetime\n\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nimport math\nimport random\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.metrics import  mean_squared_log_error\n\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f4909622d44d7e20201b25b6407af57e7a035498",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Global Variables for grid search\nn_splits = 10\nn_jobs = 2\n\nmax_depth_min = 3\nmax_depth_max = 11\nn_estimator_min = 100\nn_estimator_max = 200\n\ntest_size =0.5\nrandom_state = 1986",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c5318c5aa322350262771276ec2e9011d6ace91",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_File = '../input/train.csv'\ntest_File = '../input/test.csv'\n\ndf_train = pd.read_csv(train_File)\ndf_test = pd.read_csv(test_File)\ndf_test['SalePrice'] = 0\ndf_concat = pd.concat([df_train,df_test])\n#df_concat.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7273d0af0a883ac7cafdc44cf7b67cbb1019b039",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_concat.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c17981d35937f82123173ebd02604c0bb0ea2c06",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def giveMeWrangledData(df, testFile=False, log=False):\n    \n    \n    df = df.drop(['Id', 'GarageYrBlt','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'],axis=1)\n    \n    df['LotFrontage'] =df.LotFrontage.fillna(df.LotFrontage.mode()[0])\n    df['MasVnrArea']=df.MasVnrArea.fillna(0.0)\n    df['TotalBsmtSF'] = df.TotalBsmtSF.fillna(0)\n    df['BsmtFullBath'] = df.BsmtFullBath.fillna(0)\n    df['BsmtHalfBath'] = df.BsmtHalfBath.fillna(0)\n    df['GarageCars'] = df.GarageCars.fillna(0)\n    df['GarageArea'] = df.GarageArea.fillna(0)\n    \n    #convert data type\n    #we are being little lineant to give int64 for YearBuilt, YrSold but those guys are going to be box-coxed \n    #so let them at least enjoy the bigger size for now\n    int64_variables = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', \\\n                     'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \\\n                     'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\\\n                     'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n                     'PoolArea', 'MiscVal', 'YrSold', 'SalePrice']\n    \n    #if testFile:\n    #    int64_variables.remove('SalePrice')\n\n        \n        \n        \n    \n    for c in int64_variables:\n        if log:\n            print(\"Changing the data type for :\", c)\n        #df[c] = df[c].astype(np.int64)\n        df[c] = df[c].astype(np.float64)\n        \n    int_to_categorical_variables = ['MSSubClass', 'OverallQual', 'OverallCond', 'FireplaceQu', 'MoSold']\n    for c in int_to_categorical_variables:\n        df[c] = df[c].astype(str)\n        \n    df = df.fillna('NotAvailable')\n    return df\ndf = giveMeWrangledData(df_concat)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cfc70ea0613ac359fc7fcf69f4fe5138c2d8159b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b6ca7e49c5dc6c21600fd68ddd89f3a84e39515d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def preProcessData(df, log=False):\n    \n    \n\n    #get dummies\n    if log:\n        print(\"Shape of the data set before pre processing : \", df.shape )\n        print(\"Categorical columns : \", list(df.select_dtypes(exclude=np.number)))\n    df = pd.get_dummies(df, dtype=np.float64)\n    \n    \n    \n    if log:\n        print(\"\\n\\nShape of the data set after pre processing : \", df.shape )\n        print(\"Columns in the data set are : \",list(df))\n\n    return df\n\ndf_prep = preProcessData(df)\ndf_prep.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1a805a9ca49b054c0eb6e50905effac11fc6eff6"
      },
      "cell_type": "markdown",
      "source": "https://stats.stackexchange.com/questions/130262/why-not-log-transform-all-variables-that-are-not-of-main-interest"
    },
    {
      "metadata": {
        "_uuid": "ffb972e06356fa72461f0c193b82b7d44a5d7c91",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from IPython import display\ndef transformTarget(df_temp, revert=False ):\n    '''df = df_temp.copy()\n    #df['new_variable'] = df.BedroomAbvGr * .1 + df.FullBath *.25 + df.HalfBath * .5 + df.BsmtFullBath *.75 + df.BsmtHalfBath * 1\n    #df['new_variable'] = df.BedroomAbvGr * .1 + df.FullBath *.25 + df.HalfBath * .5 + df.BsmtFullBath *.75 + df.BsmtHalfBath * .1\n    df['new_variable'] = df.BedroomAbvGr.apply(lambda x: x if x > 0 else 1)\n    #print(\"new_variable calculated\")\n    #display.display(df[df.new_variable.isna()])\n    \n    if not revert:\n        df['sales_per_new'] = df['SalePrice']/ df.new_variable\n        #display.display(df[df.sales_per_new.isna()])\n        y = np.array(df['sales_per_new'].apply( lambda x: math.log(x)))\n        return y\n    \n    df['sales_per_new'] = df['SalePrice']* df.new_variable\n    #print(\"sales_per_new calculated\")\n    #display.display(df[df.sales_per_new.isna()])'''\n    return np.log( df_temp.SalePrice / df_temp.GrLivArea)\n    #return np.log(df_temp['SalePrice'])\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4755fe3335b956ba6d54bb079b984368cc3a6f6c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def newBoxCoxTranformation(df_temp,target,testFile=False, log=False):\n    df = df_temp.copy()\n    #assuming that only numerical features are presented\n    if log:\n        print(\"Shape of the dataset initial : \", df.shape)\n    \n    if not testFile:\n        df =df[df.SalePrice >0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        \n        display.display(df[df['SalePrice'].isna()])\n        y = transformTarget(df)\n        X= df.drop([target],axis = 1)\n        #X=df.drop(target, axis=1)\n        x_columns = list(X)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        X_testx = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X,y)\n        X_testxx = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X,df.SalePrice)\n        print(np.unique(X == X_testx))\n        print(np.unique(X == X_testxx))\n        print(np.unique(X_testx == X_testxx))\n        X = pd.DataFrame(X, columns=x_columns)\n        \n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape, y.shape)\n        return X,y\n    else:\n        df = df[df.SalePrice == 0.0]\n        if log:\n            print(\"Shape of the dataset before transformation : \", df.shape)\n        X=df.drop(target,axis = 1)\n        x_columns = list(X)\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        \n        X = pd.DataFrame(X, columns=x_columns)\n        for c in list(X):\n            if len(X[c].unique()) in  [1,2]:\n                if log:\n                    print(\"Skipping Transformation for \", c, \"because unique values are :\",X[c].unique())\n            else:\n                if log:\n                    print(\"Boxcoxing : \", c)\n                X[c] = stats.boxcox(X[c])[0]\n        \n        \n        #X = preprocessing.power_transform( X, method='box-cox')\n        X = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit_transform(X)\n        #X = X.values\n        if log:\n            print(\"Shape of the dataset after transformation : \", X.shape)\n        return X\n        \n    \n\nX = newBoxCoxTranformation(df_prep,'SalePrice',True,False)  \nX,y = newBoxCoxTranformation(df_prep,'SalePrice',False,False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5fade6388778969408a988fae1972df3b453a876",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=random.randint(1,500))#, stratify=df.BldgType)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c481d90f4fe0e0193c6640c0c08ded36ea776920",
        "trusted": true
      },
      "cell_type": "code",
      "source": "reg = XGBRegressor( colsample_bylevel=0.5,colsample_bytree=0.5, \n                   learning_rate=0.1, max_depth=3, \n       n_estimators=100, n_jobs=2, reg_alpha=0.5, reg_lambda=0.5)\nreg.fit(X_train,y_train)\nreg.score(X_test,y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a8da4789c20d447c150607bebd18b21ed73e8bdc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.sqrt(mean_squared_log_error(y_test, reg.predict(X_test)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "328b54a68971fc179d7d782bfa9c5ee9f63de2cd"
      },
      "cell_type": "code",
      "source": "np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(reg.predict(X_test))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de6dfc7cc02217e7ffdbf9d73da406fe993798aa",
        "trusted": true
      },
      "cell_type": "code",
      "source": "reg.fit(X,y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba35c7645edf1637935e3e900f8570fa6e850049",
        "trusted": true
      },
      "cell_type": "code",
      "source": "reg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7edf959b8aae25f97d9e71f36c400f7a8465f1cf"
      },
      "cell_type": "markdown",
      "source": "##### We need to have different pre-processing logic to test data. We will come back to it little later."
    },
    {
      "metadata": {
        "_uuid": "a5262ac7909e28889a0aaba1564ee310caa43488",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def checkTheTestFile(reg, auto_ml=False):\n    df_test = pd.read_csv(test_File)\n    df_test['SalePrice'] = 0.0\n    \n    \n    \n    df_train =  pd.read_csv(train_File)\n    df_concat = pd.concat([df_train,df_test])\n\n    #print(df_test[df_test.TotalBsmtSF.isna()])\n    #return\n    df = giveMeWrangledData(df_concat,True)\n    \n    #print(df.info())\n    df = preProcessData(df)\n    X_columns = list(df.columns)\n    X_columns.remove('SalePrice')\n    #df_test_dummy = df.copy()\n    #df_test_dummy = df_test_dummy[df_test_dummy.SalePrice == 0]\n    #print(df.info())\n    X = newBoxCoxTranformation(df,'SalePrice',True)\n    \n    if auto_ml:\n        X = pd.DataFrame(X, columns= X_columns)\n    #print(np.sqrt(mean_squared_log_error(y, reg.predict(X))))\n    \n    #df_test_dummy['SalePrice'] = np.exp(reg.predict(X))\n    #display.display(df_test_dummy[df_test_dummy['SalePrice'].isna()])\n    \n    \n    #df_test['SalePrice'] = np.exp(reg.predict(X))\n    df_test['SalePrice_predicted'] = np.exp(reg.predict(X))\n    df_test['SalePrice'] = df_test['SalePrice_predicted'] * df_test['GrLivArea']\n    \n    \n    \n    #display.display(df_test[df_test['SalePrice'].isna()])\n    \n    return df_test, X, reg.predict(X)\ndf_test, X_dummy, y_dummy= checkTheTestFile(reg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "97bcc0b7e677d6d0bb1216006058a45837eacfa6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_test.SalePrice.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1f6e94c180a6911a28cda0d3cd6d19a8f1be9691",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_test[df_test.SalePrice.isna()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3e685e60b2685c047be65fc8cd4533e1a977a672",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_test[df_test.SalePrice == 0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4b1a60a7d572f5520b9d5a15d6cad6705c2fb3b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "add99d35892adf15e435ee2289eeae6309ecbb39",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_test[['Id','SalePrice']].to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9fc8f97f810bde8b8156ec7d805ff4bc164ceb1d"
      },
      "cell_type": "markdown",
      "source": "##### I got Kaggle Rank of 2539/4463 with RMSLE =0.14357\n##### As on 1/17/2019 : 9:06PM IST\n* 0.13501 ==> 2040 \n* 0.13252 ==> 1865\n* 0.13002 ==> 1704\n* 0.12658 ==> 1500\n* 0.12351 ==> 1250\n* 0.12081 ==> 1000\n* 0.11572 ==> 500\n* 0.11475 ==> 250\n* 0.11310 ==> 100\n* 0.10985 ==> 50\n* 0.10973 ==> 25\n* 0.10845 ==> 10\n* 0.08021 ==> 5\n* 0.00000 ==> 1"
    },
    {
      "metadata": {
        "_uuid": "dd7f6c4a4c973389d46564509706a61e435dccc8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def forCrossValidationStratifiedShuffleSplit(df):\n    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n    #print(\"Number of Splits configured :\", sss.get_n_splits(df, df.BldgType))\n    \n    for train_index, test_index in sss.split(df, df.BldgType):\n        yield train_index, test_index\n        \n    for train_index, test_index in sss.split(df, df.OverallQual):\n        yield train_index, test_index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed74572f3252cae125da18849bbc4dcc56c58f09",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def crossValidationScoring(reg,X,y):\n    \n    return -np.sqrt(mean_squared_log_error(np.exp(y), \n                                          np.exp(reg.predict(X))\n                                          ))\n\n    #return np.sqrt(mean_squared_log_error((y), \n    #                                      (reg.predict(X))\n    #                                      ))\nmean_temp_rmsle = np.mean(cross_val_score(reg,X,y,cv= 5,scoring='neg_mean_squared_log_error'))\nprint(\"RMSE with without target variable transformation : \", np.sqrt(mean_temp_rmsle * -1))\n\nmean_temp_rmsle = np.mean(cross_val_score(reg, X, y,\n                                          cv= forCrossValidationStratifiedShuffleSplit(df_train),\n                                          scoring=crossValidationScoring))\nprint(\"RMSE with post target variable transformation : \", mean_temp_rmsle)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "405b65a8f9199d66ec74571c23aed2b89c892b99"
      },
      "cell_type": "markdown",
      "source": "##### Below are the best scores I have run so far (Kaggle Scores)\nXGBRegressor(max_depth=6,n_estimator=100, \n                   colsample_bytree=.25,\n                   colsample_bylevel=1,\n                   reg_alpha =.5,\n                   reg_lambda=.5, \n                   learning_rate=0.1,                   \n                  )\n\t\t\t\t  \nreg = XGBRegressor(max_depth=6,n_estimator=100, \n                   colsample_bytree=.25,\n                   colsample_bylevel=1,\n                   reg_alpha =0.5,\n                   reg_lambda=0, \n                   learning_rate=0.1,                   \n                  )\n\t\t\t\t  \n\t\t\t\t  \nreg = XGBRegressor(max_depth=6,n_estimator=100, \n                   colsample_bytree=.25,\n                   colsample_bylevel=1,\n                   reg_alpha =1,\n                   reg_lambda=0, \n                   learning_rate=0.1,                   \n                  )\n\t\t\t\t  \nreg = XGBRegressor(max_depth=5,n_estimator=100, \n                   colsample_bylevel=.25,\n                   colsample_bytree=0.5,\n                   learning_rate=0.1,)\n\t\t\t\t   \nreg =  XGBRegressor(max_depth=4,n_estimator=200)"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "ff5213ed488bfdbbc40c5c733cf273376de50b8b"
      },
      "cell_type": "code",
      "source": "def checkTheNewBestOfBest():\n    \n    start_time = datetime.datetime.now()\n    \n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    df_test['SalePrice'] = 0\n    df_concat = pd.concat([df_train,df_test])\n    \n    df = giveMeWrangledData(df_concat)\n    df_prep = preProcessData(df)\n    \n    X,y = newBoxCoxTranformation(df_prep,'SalePrice',False,False)\n    \n    def rmsle_cv(model, X,y):\n        cv = ShuffleSplit(n_splits=3, test_size=random.randint(7,9)/10, random_state=random.randint(1,1000))\n        cross_cv = cross_val_score(model, X, y, cv=cv, scoring='r2', n_jobs=3)\n        return(cross_cv)\n\n    xgb1 = XGBRegressor(max_depth=4,n_estimator=200)\n    xgb2 = XGBRegressor(max_depth=5,n_estimator=100, \n                   colsample_bylevel=.25,\n                   colsample_bytree=0.5,\n                   learning_rate=0.1,)\n    xgb3 = XGBRegressor(max_depth=6,n_estimator=100, \n                   colsample_bytree=.25,\n                   colsample_bylevel=1,\n                   reg_alpha =0.5,\n                   reg_lambda=0, \n                   learning_rate=0.1,                   \n                  )\n    xgb4 = XGBRegressor(max_depth=6,n_estimator=100, \n                   colsample_bytree=.25,\n                   colsample_bylevel=1,\n                   reg_alpha =1,\n                   reg_lambda=0, \n                   learning_rate=0.1,                   \n                  )\n\n    xgb5 = XGBRegressor(max_depth=6,n_estimator=100, \n                   colsample_bytree=.25,\n                   colsample_bylevel=1,\n                   reg_alpha =.5,\n                   reg_lambda=.5, \n                   learning_rate=0.1,                   \n                  )\n    lr = LinearRegression()\n    \n    class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n        def __init__(self, base_models, meta_model, n_folds=5):\n            self.base_models = base_models\n            self.meta_model = meta_model\n            self.n_folds = n_folds\n\n        # We again fit the data on clones of the original models\n        def fit(self, X, y):\n\n            self.base_models_ = [list() for x in self.base_models]\n            self.meta_model_ = clone(self.meta_model)\n            kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=1986)\n\n            # Train cloned base models then create out-of-fold predictions\n            # that are needed to train the cloned meta-model\n            out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n            for i, model in enumerate(self.base_models):\n                for train_index, holdout_index in kfold.split(X, y):\n                    instance = clone(model)\n                    self.base_models_[i].append(instance)\n                    instance.fit(X[train_index], y[train_index])\n                    y_pred = instance.predict(X[holdout_index])\n                    out_of_fold_predictions[holdout_index, i] = y_pred\n\n            # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n            self.meta_model_.fit(out_of_fold_predictions, y)\n            return self\n\n        #Do the predictions of all base models on the test data and use the averaged predictions as \n        #meta-features for the final prediction which is done by the meta-model\n        def predict(self, X):\n            meta_features = np.column_stack([\n                np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n                for base_models in self.base_models_ ])\n            return self.meta_model_.predict(meta_features)\n\n\n    stacked_averaged_models = StackingAveragedModels(base_models = (xgb1, xgb2, xgb3, xgb4, xgb5),\n                                                     meta_model = lr)\n    def rmsle(y, y_pred):\n        return np.sqrt(mean_squared_error(y, y_pred))\n\n    score = rmsle_cv(stacked_averaged_models, X, y)\n    print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n    stacked_averaged_models.fit(X, y)\n    df_test, X_dummy, y_dummy= checkTheTestFile(stacked_averaged_models)\n    #df_test['pred_rent_per_bed'] = np.exp(stacked_averaged_models.predict(test))\n    #df_test['pred_rent'] = df_test['pred_rent_per_bed'] * df_test['bed']\n\n    #print(\"Test Score r2 (post inverse function) : \", r2_score(df_test.rent, df_test.pred_rent))\n    \n    #printStats(stacked_test_pred, y_test)\n    #print(\"\\n\\nAs per Competetion Evaluation Metrics :\")\n    #printStats(df_test.pred_rent, df_test.rent)\n    \n    \n    print(\"\\n\\nTotal time taken by the stack approach : \",datetime.datetime.now() - start_time )\n\n    return stacked_averaged_models, df_test\n\nstacked_xgboost_reg_new, df_test_new = checkTheNewBestOfBest()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3358637cf82296a93c7b731f846f3b7b951e55f"
      },
      "cell_type": "markdown",
      "source": "from sklearn.model_selection import GridSearchCV\ndef gridSearchCVImp():\n    start_time =datetime.datetime.now()\n    reg = XGBRegressor(reg_alpha =.5, reg_lambda=.5, n_jobs =2)\n    \n    parameters = {\n        'max-dpth':list(range(3,11)),\n        'colsample_bylevel' : np.arange(0.0, 1.0, 0.25),\n        'colsample_bytree' : np.arange(0.0, 1.0, 0.25)\n        }\n    #cv= ShuffleSplit(n_splits=20, test_size=.5, random_state=1986)\n    reg_grid = GridSearchCV(reg, parameters, \n                            cv=forCrossValidationStratifiedShuffleSplit(df_train),\n                            n_jobs = 2,\n                            #scoring = 'neg_mean_absolute_error',\n                            #scoring = 'neg_mean_squared_log_error',\n                            scoring = crossValidationScoring,\n                            verbose=1\n                            #pre_dispatch = 2\n                           )\n    reg_grid.fit(X,y)\n    \n    print(\"Total time for the gridserach\", datetime.datetime.now() - start_time)\n    \n    return reg_grid\nreg_grid = gridSearchCVImp()\nprint(reg_grid.best_estimator_)\nprint(reg_grid.best_score_)"
    },
    {
      "metadata": {
        "_uuid": "d709399b40ae71da21662f31f50951e2b3297d95"
      },
      "cell_type": "markdown",
      "source": "##### when GridSearchCV replaces exactly the code return in manually\n\n* Total time for the gridserach 0:03:56.721226\n\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max-dpth=3,\n       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n       n_estimators=100, n_jobs=2, nthread=None, objective='reg:linear',\n       random_state=0, reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1,\n       seed=None, silent=True, subsample=1)\n       \n-0.14418238953245838\n\n##### run 2\n* Total time for the gridserach 0:03:48.821054\n\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max-dpth=3,\n       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n       n_estimators=100, n_jobs=2, nthread=None, objective='reg:linear',\n       random_state=0, reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1,\n       seed=None, silent=True, subsample=1)\n       \n-0.14418238953245838\n\n##### run 3\n* Total time for the gridserach 0:03:49.627057\n\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max-dpth=3,\n       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n       n_estimators=100, n_jobs=2, nthread=None, objective='reg:linear',\n       random_state=0, reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1,\n       seed=None, silent=True, subsample=1)\n       \n-0.14418238953245838"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5669b226e4b4d4d5499befd2b160a11cbb39d9d"
      },
      "cell_type": "code",
      "source": "print(reg_grid.best_score_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b877613e878504658a0b3e8beebb39527217b15"
      },
      "cell_type": "markdown",
      "source": "#for log target/livarea\n\n#for target / livarea\ndef doGridSearch():\n    \n    start_time = datetime.datetime.now()\n    \n    df_train = pd.read_csv(train_File)\n    df_test = pd.read_csv(test_File)\n    df_test['SalePrice'] = 0\n    df_concat = pd.concat([df_train,df_test])\n    \n    df = giveMeWrangledData(df_concat)\n    df_prep = preProcessData(df)\n    \n    X,y = newBoxCoxTranformation(df_prep,'SalePrice',False,False)\n    \n    score_list = []\n    for i in range(max_depth_min,max_depth_max):\n        for j in range(n_estimator_min,n_estimator_max,100):\n            loop_start = datetime.datetime.now()\n            for bytree in [0.25, 0.5, 0.75,1]:\n                for bylevel in [0.25, 0.5, 0.75,1]:\n\n                    #reg = XGBRegressor(max_depth=i, n_estimators=j)\n                    reg=XGBRegressor(max_depth=i, \n                         n_estimator=j,\n                         colsample_bylevel=bylevel,\n                         colsample_bytree=bytree,\n                         learning_rate=.1,\n                         reg_alpha =.5,\n                         reg_lambda=.5,\n                         n_jobs=n_jobs\n                        )\n\n                    #cv = ShuffleSplit(n_splits=20, test_size=random.randint(7,9)/10, random_state=random.randint(1,1000))\n                    #cv = ShuffleSplit(n_splits=20, test_size=.5, random_state=1986)\n                    #print(datetime.datetime())\n                    cross_cv = cross_val_score(reg, X, y,\\\n                                               cv=forCrossValidationStratifiedShuffleSplit(df_train), \\\n                                               #cv=cv, \\\n                                               scoring=crossValidationScoring,n_jobs=n_jobs)\n                    #print(\" Validat Median Score : \", np.sqrt(np.median(cross_cv) * -1), \\\n                    #      \"Average Score : \", np.sqrt(np.average(cross_cv) * -1) )\n\n                    reg.fit(X,y)\n                    training_score = np.sqrt(mean_squared_log_error(np.exp(y), np.exp(reg.predict(X))))\n                    #print(\"Training Score :\", training_score)\n\n                    df_test_new, X_test, y_test = checkTheTestFile(reg)\n                    reg.fit(X_test,y_test)\n                    testing_score = np.sqrt(mean_squared_log_error(np.exp(y), np.exp(reg.predict(X))))\n                    #print(\"Testing Score :\", testing_score)\n                    print(\"Scores\",(training_score, np.average(cross_cv), testing_score,i,j,bytree, bylevel))\n                    score_list.append((training_score, np.average(cross_cv), testing_score,i,j,bytree, bylevel))\n\n        #print(\"Time for max_depth -\",i,\"n_estimator -\",j,\" is : \", datetime.datetime.now() - loop_start)\n    \n    print(\"Total time for GridSearch : \", datetime.datetime.now() - start_time)\n    return score_list\n\nscore_list = doGridSearch()\n#sorted(score_list,key= lambda x:x[0])"
    },
    {
      "metadata": {
        "_uuid": "b985a529e359db621577c792daa6e417b611d1aa",
        "trusted": true
      },
      "cell_type": "code",
      "source": "temp_df = pd.DataFrame(score_list,columns=[\"training_score\",\n                                           \"validation_score\",\n                                           \"testing_score\", \n                                           \"max_depth\",\n                                           \"n_estimator\",\n                                           \"bytree\",\n                                           \"bylevel\",\n                                           #\"reg_alpha\",\n                                           #\"reg_lambda\"\n                                          ])\n\ntemp_df.to_csv(\"GridSearchResults-\"+str(datetime.datetime.now().date()))\ntemp_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b0f6d66aecf5602b56c980160d0ccb5b3023929",
        "trusted": true
      },
      "cell_type": "code",
      "source": "temp_df[temp_df.testing_score == temp_df.testing_score.min()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "452401a7e157947052e0f283ba81bd3d0fa289af",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "temp_df[temp_df.validation_score == temp_df.validation_score.min()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28a19e9ea5f2cf089bcf2471d23151cf49feb240",
        "trusted": true
      },
      "cell_type": "code",
      "source": "temp_df[[\"reg_alpha\",\"validation_score\"]].groupby(by=\"reg_alpha\").agg('mean').reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "000adeebc5beef1cdef75cf77759259d33141f47",
        "trusted": true
      },
      "cell_type": "code",
      "source": "temp_df[[\"reg_lambda\",\"validation_score\"]].groupby(by=\"reg_lambda\").agg('median').reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "86ec97e2b5317e69a5742374760e089a89ffa358"
      },
      "cell_type": "markdown",
      "source": "##### Next Steps:\n* re run the grid search and note training score, validation score and testing score. This should not only double check on test score calculation but also gives us the right hyper parameter from the training and validation perspective.\n* address the runtime error during scaling or power transform. If boxcox fails attempt a log transformation at least.\n* stratify fold testing to check if the training score & validation in the previous exercise still holds goods.\n* hyper parameter research for XGBoost\n* target variable transformation\n* best of best stack approach\n* team work stack approach\n* XGBoost as the final assesser in best of best stack approach\n* XGBoost as the final assesser in the team work starck approach\n* 3 layers in stack approach: best of best candidates in the order of their accuracy feeding on input in each case.\n* re-assess the dataset"
    },
    {
      "metadata": {
        "_uuid": "5b47e9763bd59669f61c104944f46de2161ce0d4"
      },
      "cell_type": "markdown",
      "source": "##### Let us talk about the hyper parameter till we get the gridsearch result"
    },
    {
      "metadata": {
        "_uuid": "8db131a039893493f30424aa9a2532389594614b"
      },
      "cell_type": "markdown",
      "source": "* https://xgboost.readthedocs.io/en/latest/parameter.html\n* https://www.kaggle.com/dansbecker/xgboost\n* https://www.datacamp.com/community/tutorials/xgboost-in-python\n\n* 1 being the max value, let us have half as the value for {'colsample_bytree':0.5, 'colsample_bylevel':0.5}\n* learning_rate = 0.05 because we have already using 0.1 so far. It is suggested in the kaggle blog.\n* n_jobs= 2 /4 based on the cpu. I guess Kaggle provides 4 cpu machine. I saw the max cpu spike as 400%\n* Surprisingly and unnoticed so far that it does the cross validation by itself. so we can leverage the n_estimator to be 1000 and use early_stopping_rounds for our quick turnaround. So our 2 for loops reduced to one :)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}